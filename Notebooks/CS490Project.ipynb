{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS490Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNzQmST9lnUHGP19sXsX/v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Johoodcoder/CS490Project/blob/hood/Notebooks/CS490Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTZbJ1SJ53XO"
      },
      "source": [
        "Non-preinstalled module installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm5_ujD458U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63ef279-27c1-4159-c8e3-163b8c399db1"
      },
      "source": [
        "!pip install pytorch-pretrained-bert\r\n",
        "# !pip install pytorch-nlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.17.30)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.0+cu101)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.4)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.30 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.20.30)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.30->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.30->boto3->pytorch-pretrained-bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw-rU23d7kyr"
      },
      "source": [
        "Import Dataset used in https://towardsdatascience.com/fake-news-classification-with-bert-afbeee601f41"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PN2iZvR7xuA"
      },
      "source": [
        "# from google.colab import files\r\n",
        "\r\n",
        "# uploaded = files.upload()\r\n",
        "# fileName = ''\r\n",
        "\r\n",
        "# for fn in uploaded.keys():\r\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\r\n",
        "#       name=fn, length=len(uploaded[fn])))\r\n",
        "#   fileName = fn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vvyvLZY3Lnl"
      },
      "source": [
        "The base code from https://github.com/spierre91/medium_code/blob/master/fake_news_classifcation.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbufH4ZX8X5N"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "import torch\n",
        "# from torchnlp.datasets import imdb_dataset\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a-5pyC08dzO",
        "outputId": "24d6e8b1-2ffe-42b5-aede-6740281e508c"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "# train_data, test_data = imdb_dataset(train=True, test=True)\n",
        "df = pd.read_csv(\"condensed_fake_real_news_lowercase.csv\")\n",
        "df = df[['text', 'type']]\n",
        "print(len(df))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7MfTPO18kw8",
        "outputId": "f506c122-8cb6-4d43-e30c-1d65eddb1e4a"
      },
      "source": [
        "from collections import Counter \n",
        "\n",
        "print(Counter(df['type'].values))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'fake': 4000, 'real': 4000})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HHnQVRq8z0n",
        "outputId": "2dff00fe-fb94-4ae3-94ff-666d949565e2"
      },
      "source": [
        "df = df[df['type'].isin(['fake', 'real'])]\n",
        "df.dropna(inplace = True)\n",
        "df = df.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "\n",
        "print(Counter(df['type'].values))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'fake': 4000, 'real': 4000})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wOwpOde8_WC",
        "outputId": "0ba8e4cf-6fed-4b1d-d556-4511866ca2f6"
      },
      "source": [
        "train_data_df = df.head(640)\n",
        "test_data_df = df.tail(160)\n",
        "print(train_data_df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  text  type\n",
            "0    donald trump s most recent secretive actions t...  fake\n",
            "1    washington (reuters) - u.s. president donald t...  real\n",
            "2    washington (reuters) - u.s. president donald t...  real\n",
            "3    washington (reuters) - congressional leaders a...  real\n",
            "4    one of trump s biggest campaign promises was t...  fake\n",
            "..                                                 ...   ...\n",
            "635  president barack obama gave an amazing farewel...  fake\n",
            "636  when donald trump kicked off  made in america ...  fake\n",
            "637  with donald trump winning the election, albeit...  fake\n",
            "638  washington (reuters) - the u.s. house of repre...  real\n",
            "639  (reuters) - the republican party will resume f...  real\n",
            "\n",
            "[640 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bborPzYM9CUR"
      },
      "source": [
        "train_data = []\n",
        "for index, row in train_data_df.iterrows():\n",
        "    train_data.append({'text': row['text'], 'type': row['type']})\n",
        "\n",
        "test_data = []\n",
        "for index, row in test_data_df.iterrows():\n",
        "    test_data.append({'text': row['text'], 'type': row['type']})"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G7zoLaF9gNf"
      },
      "source": [
        "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['type']), train_data)))\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['type']), test_data)))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KNrngW99ooH"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\n",
        "\n",
        "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
        "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
        "\n",
        "\n",
        "\n",
        "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAKhviVo9ujZ"
      },
      "source": [
        "train_y = np.array(train_labels) == 'fake'\n",
        "test_y = np.array(test_labels) == 'fake'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKVuGoMm9wXe"
      },
      "source": [
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens, masks=None):\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjP7PqON92FH"
      },
      "source": [
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1poQHOBe-DM6"
      },
      "source": [
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "train_masks_tensor = train_masks_tensor.to('cuda')\n",
        "test_masks_tensor = test_masks_tensor.to('cuda')\n",
        "\n",
        "train_tokens_tensor = train_tokens_tensor.to('cuda')\n",
        "test_tokens_tensor = test_tokens_tensor.to('cuda')\n",
        "train_y_tensor = train_y_tensor.to('cuda')\n",
        "test_y_tensor = test_y_tensor.to('cuda')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kHKfGwF-DZp"
      },
      "source": [
        "BATCH_SIZE = 12\n",
        "EPOCHS = 3\n",
        "\n",
        "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
        "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
        "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kdg4sqN-DkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef58c121-5213-40ed-fb39-e08cfb2ccebc"
      },
      "source": [
        "bert_clf = BertBinaryClassifier()\n",
        "bert_clf.to('cuda')\n",
        "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
        "\n",
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "        probas = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        batch_loss = loss_func(probas, labels)\n",
        "        train_loss += batch_loss.item()\n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "0/53.333333333333336 loss: 0.6675477027893066 \n",
            "Epoch:  1\n",
            "1/53.333333333333336 loss: 0.6754174828529358 \n",
            "Epoch:  1\n",
            "2/53.333333333333336 loss: 0.6807045539220175 \n",
            "Epoch:  1\n",
            "3/53.333333333333336 loss: 0.6759223341941833 \n",
            "Epoch:  1\n",
            "4/53.333333333333336 loss: 0.6721952676773071 \n",
            "Epoch:  1\n",
            "5/53.333333333333336 loss: 0.6722391148408254 \n",
            "Epoch:  1\n",
            "6/53.333333333333336 loss: 0.6729859624590192 \n",
            "Epoch:  1\n",
            "7/53.333333333333336 loss: 0.6698206439614296 \n",
            "Epoch:  1\n",
            "8/53.333333333333336 loss: 0.6684075395266215 \n",
            "Epoch:  1\n",
            "9/53.333333333333336 loss: 0.664815878868103 \n",
            "Epoch:  1\n",
            "10/53.333333333333336 loss: 0.658666502345692 \n",
            "Epoch:  1\n",
            "11/53.333333333333336 loss: 0.6542098522186279 \n",
            "Epoch:  1\n",
            "12/53.333333333333336 loss: 0.6505814974124615 \n",
            "Epoch:  1\n",
            "13/53.333333333333336 loss: 0.6502741532666343 \n",
            "Epoch:  1\n",
            "14/53.333333333333336 loss: 0.6499961813290914 \n",
            "Epoch:  1\n",
            "15/53.333333333333336 loss: 0.6507352814078331 \n",
            "Epoch:  1\n",
            "16/53.333333333333336 loss: 0.6486241782412809 \n",
            "Epoch:  1\n",
            "17/53.333333333333336 loss: 0.6478509406248728 \n",
            "Epoch:  1\n",
            "18/53.333333333333336 loss: 0.6460221033347281 \n",
            "Epoch:  1\n",
            "19/53.333333333333336 loss: 0.6433867275714874 \n",
            "Epoch:  1\n",
            "20/53.333333333333336 loss: 0.6413248436791557 \n",
            "Epoch:  1\n",
            "21/53.333333333333336 loss: 0.6400781815702264 \n",
            "Epoch:  1\n",
            "22/53.333333333333336 loss: 0.6388210628343665 \n",
            "Epoch:  1\n",
            "23/53.333333333333336 loss: 0.6364935785531998 \n",
            "Epoch:  1\n",
            "24/53.333333333333336 loss: 0.6346671795845031 \n",
            "Epoch:  1\n",
            "25/53.333333333333336 loss: 0.6345454867069538 \n",
            "Epoch:  1\n",
            "26/53.333333333333336 loss: 0.6328685636873599 \n",
            "Epoch:  1\n",
            "27/53.333333333333336 loss: 0.6342130814279828 \n",
            "Epoch:  1\n",
            "28/53.333333333333336 loss: 0.6307440811190111 \n",
            "Epoch:  1\n",
            "29/53.333333333333336 loss: 0.6274569054444631 \n",
            "Epoch:  1\n",
            "30/53.333333333333336 loss: 0.6242773782822394 \n",
            "Epoch:  1\n",
            "31/53.333333333333336 loss: 0.6204119194298983 \n",
            "Epoch:  1\n",
            "32/53.333333333333336 loss: 0.6178160002737334 \n",
            "Epoch:  1\n",
            "33/53.333333333333336 loss: 0.6165899423991933 \n",
            "Epoch:  1\n",
            "34/53.333333333333336 loss: 0.6140122703143529 \n",
            "Epoch:  1\n",
            "35/53.333333333333336 loss: 0.6100180463658439 \n",
            "Epoch:  1\n",
            "36/53.333333333333336 loss: 0.6062279488589313 \n",
            "Epoch:  1\n",
            "37/53.333333333333336 loss: 0.6035516701246563 \n",
            "Epoch:  1\n",
            "38/53.333333333333336 loss: 0.6021742209410056 \n",
            "Epoch:  1\n",
            "39/53.333333333333336 loss: 0.5994457036256791 \n",
            "Epoch:  1\n",
            "40/53.333333333333336 loss: 0.5956403235109841 \n",
            "Epoch:  1\n",
            "41/53.333333333333336 loss: 0.5920463665610268 \n",
            "Epoch:  1\n",
            "42/53.333333333333336 loss: 0.5885400002778962 \n",
            "Epoch:  1\n",
            "43/53.333333333333336 loss: 0.5848928534171798 \n",
            "Epoch:  1\n",
            "44/53.333333333333336 loss: 0.5812992884053124 \n",
            "Epoch:  1\n",
            "45/53.333333333333336 loss: 0.5784556204858033 \n",
            "Epoch:  1\n",
            "46/53.333333333333336 loss: 0.5753925417331939 \n",
            "Epoch:  1\n",
            "47/53.333333333333336 loss: 0.5716342013329268 \n",
            "Epoch:  1\n",
            "48/53.333333333333336 loss: 0.567770646542919 \n",
            "Epoch:  1\n",
            "49/53.333333333333336 loss: 0.5644134354591369 \n",
            "Epoch:  1\n",
            "50/53.333333333333336 loss: 0.5609479055685156 \n",
            "Epoch:  1\n",
            "51/53.333333333333336 loss: 0.5572159020946577 \n",
            "Epoch:  1\n",
            "52/53.333333333333336 loss: 0.5543775069263747 \n",
            "Epoch:  1\n",
            "53/53.333333333333336 loss: 0.5503828795971694 \n",
            "Epoch:  2\n",
            "0/53.333333333333336 loss: 0.3510616719722748 \n",
            "Epoch:  2\n",
            "1/53.333333333333336 loss: 0.3507005125284195 \n",
            "Epoch:  2\n",
            "2/53.333333333333336 loss: 0.36155879497528076 \n",
            "Epoch:  2\n",
            "3/53.333333333333336 loss: 0.3568923771381378 \n",
            "Epoch:  2\n",
            "4/53.333333333333336 loss: 0.35551615953445437 \n",
            "Epoch:  2\n",
            "5/53.333333333333336 loss: 0.35453402002652484 \n",
            "Epoch:  2\n",
            "6/53.333333333333336 loss: 0.34037908911705017 \n",
            "Epoch:  2\n",
            "7/53.333333333333336 loss: 0.33107735216617584 \n",
            "Epoch:  2\n",
            "8/53.333333333333336 loss: 0.32213857107692295 \n",
            "Epoch:  2\n",
            "9/53.333333333333336 loss: 0.3188127905130386 \n",
            "Epoch:  2\n",
            "10/53.333333333333336 loss: 0.31599310582334345 \n",
            "Epoch:  2\n",
            "11/53.333333333333336 loss: 0.31141358613967896 \n",
            "Epoch:  2\n",
            "12/53.333333333333336 loss: 0.3073155742425185 \n",
            "Epoch:  2\n",
            "13/53.333333333333336 loss: 0.3105821652071817 \n",
            "Epoch:  2\n",
            "14/53.333333333333336 loss: 0.3082923889160156 \n",
            "Epoch:  2\n",
            "15/53.333333333333336 loss: 0.304969510063529 \n",
            "Epoch:  2\n",
            "16/53.333333333333336 loss: 0.31038877893896666 \n",
            "Epoch:  2\n",
            "17/53.333333333333336 loss: 0.31237541966968113 \n",
            "Epoch:  2\n",
            "18/53.333333333333336 loss: 0.30736929178237915 \n",
            "Epoch:  2\n",
            "19/53.333333333333336 loss: 0.30450116842985153 \n",
            "Epoch:  2\n",
            "20/53.333333333333336 loss: 0.2997917852231434 \n",
            "Epoch:  2\n",
            "21/53.333333333333336 loss: 0.29818702895532956 \n",
            "Epoch:  2\n",
            "22/53.333333333333336 loss: 0.29518535668435303 \n",
            "Epoch:  2\n",
            "23/53.333333333333336 loss: 0.29360158555209637 \n",
            "Epoch:  2\n",
            "24/53.333333333333336 loss: 0.2947305029630661 \n",
            "Epoch:  2\n",
            "25/53.333333333333336 loss: 0.2941568006689732 \n",
            "Epoch:  2\n",
            "26/53.333333333333336 loss: 0.2909236975290157 \n",
            "Epoch:  2\n",
            "27/53.333333333333336 loss: 0.2939621662454946 \n",
            "Epoch:  2\n",
            "28/53.333333333333336 loss: 0.2907674811009703 \n",
            "Epoch:  2\n",
            "29/53.333333333333336 loss: 0.28794681231180824 \n",
            "Epoch:  2\n",
            "30/53.333333333333336 loss: 0.28454835088022296 \n",
            "Epoch:  2\n",
            "31/53.333333333333336 loss: 0.28243983956053853 \n",
            "Epoch:  2\n",
            "32/53.333333333333336 loss: 0.27977767315777863 \n",
            "Epoch:  2\n",
            "33/53.333333333333336 loss: 0.2819512311150046 \n",
            "Epoch:  2\n",
            "34/53.333333333333336 loss: 0.27892073690891267 \n",
            "Epoch:  2\n",
            "35/53.333333333333336 loss: 0.27964532665080494 \n",
            "Epoch:  2\n",
            "36/53.333333333333336 loss: 0.2779943705410571 \n",
            "Epoch:  2\n",
            "37/53.333333333333336 loss: 0.2751656758942102 \n",
            "Epoch:  2\n",
            "38/53.333333333333336 loss: 0.2719753212653674 \n",
            "Epoch:  2\n",
            "39/53.333333333333336 loss: 0.269556038454175 \n",
            "Epoch:  2\n",
            "40/53.333333333333336 loss: 0.2673869463728695 \n",
            "Epoch:  2\n",
            "41/53.333333333333336 loss: 0.267324145705927 \n",
            "Epoch:  2\n",
            "42/53.333333333333336 loss: 0.26444818876510445 \n",
            "Epoch:  2\n",
            "43/53.333333333333336 loss: 0.26164672963998536 \n",
            "Epoch:  2\n",
            "44/53.333333333333336 loss: 0.26234053472677865 \n",
            "Epoch:  2\n",
            "45/53.333333333333336 loss: 0.2597043841429379 \n",
            "Epoch:  2\n",
            "46/53.333333333333336 loss: 0.257328600008437 \n",
            "Epoch:  2\n",
            "47/53.333333333333336 loss: 0.2552875072384874 \n",
            "Epoch:  2\n",
            "48/53.333333333333336 loss: 0.2528490475854095 \n",
            "Epoch:  2\n",
            "49/53.333333333333336 loss: 0.25055240660905836 \n",
            "Epoch:  2\n",
            "50/53.333333333333336 loss: 0.24825996160507202 \n",
            "Epoch:  2\n",
            "51/53.333333333333336 loss: 0.24620103262937987 \n",
            "Epoch:  2\n",
            "52/53.333333333333336 loss: 0.24408181096022985 \n",
            "Epoch:  2\n",
            "53/53.333333333333336 loss: 0.2505687066802272 \n",
            "Epoch:  3\n",
            "0/53.333333333333336 loss: 0.12045004218816757 \n",
            "Epoch:  3\n",
            "1/53.333333333333336 loss: 0.11169087141752243 \n",
            "Epoch:  3\n",
            "2/53.333333333333336 loss: 0.12255777418613434 \n",
            "Epoch:  3\n",
            "3/53.333333333333336 loss: 0.12327870354056358 \n",
            "Epoch:  3\n",
            "4/53.333333333333336 loss: 0.11921709924936294 \n",
            "Epoch:  3\n",
            "5/53.333333333333336 loss: 0.11908566951751709 \n",
            "Epoch:  3\n",
            "6/53.333333333333336 loss: 0.11827837995120458 \n",
            "Epoch:  3\n",
            "7/53.333333333333336 loss: 0.11751149129122496 \n",
            "Epoch:  3\n",
            "8/53.333333333333336 loss: 0.11738112817207973 \n",
            "Epoch:  3\n",
            "9/53.333333333333336 loss: 0.11608112305402755 \n",
            "Epoch:  3\n",
            "10/53.333333333333336 loss: 0.11597347124056383 \n",
            "Epoch:  3\n",
            "11/53.333333333333336 loss: 0.11486412957310677 \n",
            "Epoch:  3\n",
            "12/53.333333333333336 loss: 0.12486921594693111 \n",
            "Epoch:  3\n",
            "13/53.333333333333336 loss: 0.1228765994310379 \n",
            "Epoch:  3\n",
            "14/53.333333333333336 loss: 0.12147095203399658 \n",
            "Epoch:  3\n",
            "15/53.333333333333336 loss: 0.1342179849743843 \n",
            "Epoch:  3\n",
            "16/53.333333333333336 loss: 0.14069068694815917 \n",
            "Epoch:  3\n",
            "17/53.333333333333336 loss: 0.13858314727743468 \n",
            "Epoch:  3\n",
            "18/53.333333333333336 loss: 0.136183568129414 \n",
            "Epoch:  3\n",
            "19/53.333333333333336 loss: 0.13440721817314624 \n",
            "Epoch:  3\n",
            "20/53.333333333333336 loss: 0.1330411434173584 \n",
            "Epoch:  3\n",
            "21/53.333333333333336 loss: 0.13082239505919543 \n",
            "Epoch:  3\n",
            "22/53.333333333333336 loss: 0.12893153208753336 \n",
            "Epoch:  3\n",
            "23/53.333333333333336 loss: 0.13128184899687767 \n",
            "Epoch:  3\n",
            "24/53.333333333333336 loss: 0.13344304919242858 \n",
            "Epoch:  3\n",
            "25/53.333333333333336 loss: 0.132038847471659 \n",
            "Epoch:  3\n",
            "26/53.333333333333336 loss: 0.13097114971390478 \n",
            "Epoch:  3\n",
            "27/53.333333333333336 loss: 0.13293508599911416 \n",
            "Epoch:  3\n",
            "28/53.333333333333336 loss: 0.13699010888050342 \n",
            "Epoch:  3\n",
            "29/53.333333333333336 loss: 0.1352340340614319 \n",
            "Epoch:  3\n",
            "30/53.333333333333336 loss: 0.13451246436565154 \n",
            "Epoch:  3\n",
            "31/53.333333333333336 loss: 0.13309586816467345 \n",
            "Epoch:  3\n",
            "32/53.333333333333336 loss: 0.1319028732903076 \n",
            "Epoch:  3\n",
            "33/53.333333333333336 loss: 0.13049732806051478 \n",
            "Epoch:  3\n",
            "34/53.333333333333336 loss: 0.12896313624722616 \n",
            "Epoch:  3\n",
            "35/53.333333333333336 loss: 0.12867271982961231 \n",
            "Epoch:  3\n",
            "36/53.333333333333336 loss: 0.12805878612640742 \n",
            "Epoch:  3\n",
            "37/53.333333333333336 loss: 0.1266815246720063 \n",
            "Epoch:  3\n",
            "38/53.333333333333336 loss: 0.12531407406696907 \n",
            "Epoch:  3\n",
            "39/53.333333333333336 loss: 0.12411090843379498 \n",
            "Epoch:  3\n",
            "40/53.333333333333336 loss: 0.12273312959729171 \n",
            "Epoch:  3\n",
            "41/53.333333333333336 loss: 0.1215351317964849 \n",
            "Epoch:  3\n",
            "42/53.333333333333336 loss: 0.12140083174372829 \n",
            "Epoch:  3\n",
            "43/53.333333333333336 loss: 0.12018399634821848 \n",
            "Epoch:  3\n",
            "44/53.333333333333336 loss: 0.11894479476743275 \n",
            "Epoch:  3\n",
            "45/53.333333333333336 loss: 0.1181029743798401 \n",
            "Epoch:  3\n",
            "46/53.333333333333336 loss: 0.1169756960044516 \n",
            "Epoch:  3\n",
            "47/53.333333333333336 loss: 0.11590740503743291 \n",
            "Epoch:  3\n",
            "48/53.333333333333336 loss: 0.11494854959298154 \n",
            "Epoch:  3\n",
            "49/53.333333333333336 loss: 0.11402297988533974 \n",
            "Epoch:  3\n",
            "50/53.333333333333336 loss: 0.11301177542875795 \n",
            "Epoch:  3\n",
            "51/53.333333333333336 loss: 0.11207264797905317 \n",
            "Epoch:  3\n",
            "52/53.333333333333336 loss: 0.11108058430957345 \n",
            "Epoch:  3\n",
            "53/53.333333333333336 loss: 0.11015537353577437 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZffdI9Wn-dT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d74eb1-3791-4aa5-9401-6ccb873e99f6"
      },
      "source": [
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        loss = loss_func(logits, labels)\n",
        "        numpy_logits = logits.cpu().detach().numpy()\n",
        "        \n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "        all_logits += list(numpy_logits[:, 0])\n",
        "        \n",
        "print(classification_report(test_y, bert_predicted))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       1.00      0.98      0.99        85\n",
            "        True       0.97      1.00      0.99        75\n",
            "\n",
            "    accuracy                           0.99       160\n",
            "   macro avg       0.99      0.99      0.99       160\n",
            "weighted avg       0.99      0.99      0.99       160\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}