{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS490Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdo3ybTets5sxFfmY4YHqz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Johoodcoder/CS490Project/blob/hood/Notebooks/CS490ProjectSequenceImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTZbJ1SJ53XO"
      },
      "source": [
        "Non-preinstalled module installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm5_ujD458U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b7c784-76d1-4ebb-8001-3b16d748241b"
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.17.33)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.0+cu101)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.33 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.20.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.33->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.33->boto3->pytorch-pretrained-bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbufH4ZX8X5N"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
        "import torch\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a-5pyC08dzO",
        "outputId": "3905d5ed-f793-4e4f-9cbf-d58968cb3a00"
      },
      "source": [
        "# Load dataset\n",
        "# df = pd.read_csv(\"condensed_fake_real_news_SANITIZED.csv\")\n",
        "df = pd.read_csv(\"LIARPLUSTrainSanitized.csv\")\n",
        "df = df[['text', 'type']]\n",
        "print(len(df))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HHnQVRq8z0n",
        "outputId": "52dd44e2-d881-4cb5-f3e8-3fe77eebf2ec"
      },
      "source": [
        "df = df[df['type'].isin(['fake', 'real'])]\n",
        "# Scramble data indexes from dataset. Random_state is a seed.\n",
        "df = df.dropna()\n",
        "df = df.sample(frac=1, random_state = 23).reset_index(drop=True)\n",
        "\n",
        "print(Counter(df['type'].values))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'real': 7347, 'fake': 2803})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wOwpOde8_WC",
        "outputId": "9a94d5e0-a620-42a3-be6e-9ed319bc20b7"
      },
      "source": [
        "train_data_df = df.head(800)\n",
        "test_data_df = df.tail(100)\n",
        "print(train_data_df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  text  type\n",
            "0    Bike Austin said: \"Bike lanes and sidewalks ha...  real\n",
            "1    But his full-bodied explanation at the debate ...  fake\n",
            "2    Alee Lockman, Bruuns spokeswoman, says the con...  real\n",
            "3    Clinton said, \"the U. S. military footprint in...  real\n",
            "4    The Florida Democratic Party said, \"Marco Rubi...  fake\n",
            "..                                                 ...   ...\n",
            "795  But that isnt Scotts statement. In the face of...  fake\n",
            "796  According to the study, 62. 7 percent of minim...  real\n",
            "797  Ayotte said that mental health provisions rela...  real\n",
            "798  Our rating Trump called the United States \"one...  real\n",
            "799  (We used eight budgets for an apples-to-apples...  fake\n",
            "\n",
            "[800 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bborPzYM9CUR"
      },
      "source": [
        "train_data = []\n",
        "for index, row in train_data_df.iterrows():\n",
        "    train_data.append({'text': row['text'], 'type': row['type']})\n",
        "\n",
        "test_data = []\n",
        "for index, row in test_data_df.iterrows():\n",
        "    test_data.append({'text': row['text'], 'type': row['type']})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G7zoLaF9gNf"
      },
      "source": [
        "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['type']), train_data)))\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['type']), test_data)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KNrngW99ooH"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\n",
        "\n",
        "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
        "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
        "\n",
        "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=128, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=128, truncating=\"post\", padding=\"post\", dtype=\"int\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAKhviVo9ujZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035f99f0-a053-4d31-e152-a66721845be0"
      },
      "source": [
        "# If value == fake then make it true. Otherwise false.\n",
        "train_y = np.array(train_labels) == 'fake'\n",
        "test_y = np.array(test_labels) == 'fake'\n",
        "\n",
        "train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((800,), (100,), 0.2925, 0.34)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjP7PqON92FH"
      },
      "source": [
        "# Input masks differentiate padding tokens from legitimate data token. 1 == data, 0 == padding\n",
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1poQHOBe-DM6"
      },
      "source": [
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "# Testing datatypes for speed and compatability\n",
        "# ----------------------------------------- CUDA -----------------------------------------------------\n",
        "# train_masks_tensor = train_masks_tensor.to('cuda')\n",
        "# test_masks_tensor = test_masks_tensor.to('cuda')\n",
        "\n",
        "# train_tokens_tensor = train_tokens_tensor.to('cuda')\n",
        "# test_tokens_tensor = test_tokens_tensor.to('cuda')\n",
        "\n",
        "train_y_tensor = train_y_tensor.to('cuda')\n",
        "test_y_tensor = test_y_tensor.to('cuda')\n",
        "\n",
        "# ----------------------------------------- CUDA LONG -----------------------------------------------------\n",
        "cuda = torch.device('cuda')\n",
        "train_masks_tensor = train_masks_tensor.to(cuda, dtype = torch.long)\n",
        "test_masks_tensor = test_masks_tensor.to(cuda, dtype = torch.long)\n",
        "\n",
        "train_tokens_tensor = train_tokens_tensor.to(cuda, dtype = torch.long)\n",
        "test_tokens_tensor = test_tokens_tensor.to(cuda, dtype = torch.long)\n",
        "\n",
        "# train_y_tensor = train_y_tensor.to(cuda, dtype = torch.long)\n",
        "# test_y_tensor = test_y_tensor.to(cuda, dtype = torch.long)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kHKfGwF-DZp"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "\n",
        "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
        "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
        "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "num_labels = 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kdg4sqN-DkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9666a2-9818-49bb-d2ac-4bc8b25de18a"
      },
      "source": [
        "bert_clf = BertForSequenceClassification(config, num_labels)\n",
        "bert_clf.to('cuda')\n",
        "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
        "\n",
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "        \n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        # The new model has slightly different outputs. A sigmoid() is applied to probas to bound between 0 and 1\n",
        "        batch_loss = loss_func(logits.sigmoid(), labels)\n",
        "        train_loss += batch_loss.item()\n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "0/25.0 loss: 0.6849037408828735 \n",
            "Epoch:  1\n",
            "1/25.0 loss: 0.67987060546875 \n",
            "Epoch:  1\n",
            "2/25.0 loss: 0.6727096239725748 \n",
            "Epoch:  1\n",
            "3/25.0 loss: 0.661022961139679 \n",
            "Epoch:  1\n",
            "4/25.0 loss: 0.6549081683158875 \n",
            "Epoch:  1\n",
            "5/25.0 loss: 0.6520618498325348 \n",
            "Epoch:  1\n",
            "6/25.0 loss: 0.6560863682201931 \n",
            "Epoch:  1\n",
            "7/25.0 loss: 0.6512352004647255 \n",
            "Epoch:  1\n",
            "8/25.0 loss: 0.657364547252655 \n",
            "Epoch:  1\n",
            "9/25.0 loss: 0.640041035413742 \n",
            "Epoch:  1\n",
            "10/25.0 loss: 0.6402809511531483 \n",
            "Epoch:  1\n",
            "11/25.0 loss: 0.6311071068048477 \n",
            "Epoch:  1\n",
            "12/25.0 loss: 0.6218194686449491 \n",
            "Epoch:  1\n",
            "13/25.0 loss: 0.6273811374391828 \n",
            "Epoch:  1\n",
            "14/25.0 loss: 0.621566379070282 \n",
            "Epoch:  1\n",
            "15/25.0 loss: 0.6238070800900459 \n",
            "Epoch:  1\n",
            "16/25.0 loss: 0.6233813692541683 \n",
            "Epoch:  1\n",
            "17/25.0 loss: 0.6272677050696479 \n",
            "Epoch:  1\n",
            "18/25.0 loss: 0.6189436536086234 \n",
            "Epoch:  1\n",
            "19/25.0 loss: 0.6141881942749023 \n",
            "Epoch:  1\n",
            "20/25.0 loss: 0.6139606152262006 \n",
            "Epoch:  1\n",
            "21/25.0 loss: 0.6092823066494681 \n",
            "Epoch:  1\n",
            "22/25.0 loss: 0.607071049835371 \n",
            "Epoch:  1\n",
            "23/25.0 loss: 0.6079594815770785 \n",
            "Epoch:  1\n",
            "24/25.0 loss: 0.6117943930625915 \n",
            "Epoch:  2\n",
            "0/25.0 loss: 0.5613547563552856 \n",
            "Epoch:  2\n",
            "1/25.0 loss: 0.6383809447288513 \n",
            "Epoch:  2\n",
            "2/25.0 loss: 0.5806135932604471 \n",
            "Epoch:  2\n",
            "3/25.0 loss: 0.5901716202497482 \n",
            "Epoch:  2\n",
            "4/25.0 loss: 0.6082630038261414 \n",
            "Epoch:  2\n",
            "5/25.0 loss: 0.5954335729281107 \n",
            "Epoch:  2\n",
            "6/25.0 loss: 0.6064638836043221 \n",
            "Epoch:  2\n",
            "7/25.0 loss: 0.615178294479847 \n",
            "Epoch:  2\n",
            "8/25.0 loss: 0.5967641472816467 \n",
            "Epoch:  2\n",
            "9/25.0 loss: 0.58879274725914 \n",
            "Epoch:  2\n",
            "10/25.0 loss: 0.5956530408425764 \n",
            "Epoch:  2\n",
            "11/25.0 loss: 0.6012577066818873 \n",
            "Epoch:  2\n",
            "12/25.0 loss: 0.6040885540155264 \n",
            "Epoch:  2\n",
            "13/25.0 loss: 0.5977172085217067 \n",
            "Epoch:  2\n",
            "14/25.0 loss: 0.6007694602012634 \n",
            "Epoch:  2\n",
            "15/25.0 loss: 0.6072431616485119 \n",
            "Epoch:  2\n",
            "16/25.0 loss: 0.6105860787279466 \n",
            "Epoch:  2\n",
            "17/25.0 loss: 0.6095058156384362 \n",
            "Epoch:  2\n",
            "18/25.0 loss: 0.6074264520093015 \n",
            "Epoch:  2\n",
            "19/25.0 loss: 0.6035579144954681 \n",
            "Epoch:  2\n",
            "20/25.0 loss: 0.6037082104455858 \n",
            "Epoch:  2\n",
            "21/25.0 loss: 0.6024833592501554 \n",
            "Epoch:  2\n",
            "22/25.0 loss: 0.5961755384569583 \n",
            "Epoch:  2\n",
            "23/25.0 loss: 0.6025791441400846 \n",
            "Epoch:  2\n",
            "24/25.0 loss: 0.6063886165618897 \n",
            "Epoch:  3\n",
            "0/25.0 loss: 0.4646257758140564 \n",
            "Epoch:  3\n",
            "1/25.0 loss: 0.5371890962123871 \n",
            "Epoch:  3\n",
            "2/25.0 loss: 0.5751423239707947 \n",
            "Epoch:  3\n",
            "3/25.0 loss: 0.6009841412305832 \n",
            "Epoch:  3\n",
            "4/25.0 loss: 0.5982942581176758 \n",
            "Epoch:  3\n",
            "5/25.0 loss: 0.5933807094891866 \n",
            "Epoch:  3\n",
            "6/25.0 loss: 0.5754752584866115 \n",
            "Epoch:  3\n",
            "7/25.0 loss: 0.5869299694895744 \n",
            "Epoch:  3\n",
            "8/25.0 loss: 0.6054580277866788 \n",
            "Epoch:  3\n",
            "9/25.0 loss: 0.6099190473556518 \n",
            "Epoch:  3\n",
            "10/25.0 loss: 0.6038714105432684 \n",
            "Epoch:  3\n",
            "11/25.0 loss: 0.6066526373227438 \n",
            "Epoch:  3\n",
            "12/25.0 loss: 0.6021934105799749 \n",
            "Epoch:  3\n",
            "13/25.0 loss: 0.6021547743252346 \n",
            "Epoch:  3\n",
            "14/25.0 loss: 0.6050938526789348 \n",
            "Epoch:  3\n",
            "15/25.0 loss: 0.6069955192506313 \n",
            "Epoch:  3\n",
            "16/25.0 loss: 0.6028551739804885 \n",
            "Epoch:  3\n",
            "17/25.0 loss: 0.5996282067563798 \n",
            "Epoch:  3\n",
            "18/25.0 loss: 0.6072051305519907 \n",
            "Epoch:  3\n",
            "19/25.0 loss: 0.6107701629400253 \n",
            "Epoch:  3\n",
            "20/25.0 loss: 0.6147621103695461 \n",
            "Epoch:  3\n",
            "21/25.0 loss: 0.610314361073754 \n",
            "Epoch:  3\n",
            "22/25.0 loss: 0.6048338205918021 \n",
            "Epoch:  3\n",
            "23/25.0 loss: 0.6023900335033735 \n",
            "Epoch:  3\n",
            "24/25.0 loss: 0.6068280982971191 \n",
            "Epoch:  4\n",
            "0/25.0 loss: 0.6100395917892456 \n",
            "Epoch:  4\n",
            "1/25.0 loss: 0.6585919260978699 \n",
            "Epoch:  4\n",
            "2/25.0 loss: 0.6830655535062155 \n",
            "Epoch:  4\n",
            "3/25.0 loss: 0.6513722985982895 \n",
            "Epoch:  4\n",
            "4/25.0 loss: 0.6361709356307983 \n",
            "Epoch:  4\n",
            "5/25.0 loss: 0.6480127274990082 \n",
            "Epoch:  4\n",
            "6/25.0 loss: 0.6370800818715777 \n",
            "Epoch:  4\n",
            "7/25.0 loss: 0.6359876468777657 \n",
            "Epoch:  4\n",
            "8/25.0 loss: 0.6260672079192268 \n",
            "Epoch:  4\n",
            "9/25.0 loss: 0.6334374606609344 \n",
            "Epoch:  4\n",
            "10/25.0 loss: 0.6324455412951383 \n",
            "Epoch:  4\n",
            "11/25.0 loss: 0.6351757645606995 \n",
            "Epoch:  4\n",
            "12/25.0 loss: 0.6291869420271653 \n",
            "Epoch:  4\n",
            "13/25.0 loss: 0.6243156961032322 \n",
            "Epoch:  4\n",
            "14/25.0 loss: 0.616361133257548 \n",
            "Epoch:  4\n",
            "15/25.0 loss: 0.6140677556395531 \n",
            "Epoch:  4\n",
            "16/25.0 loss: 0.6160702565137077 \n",
            "Epoch:  4\n",
            "17/25.0 loss: 0.6176641086737314 \n",
            "Epoch:  4\n",
            "18/25.0 loss: 0.6187393069267273 \n",
            "Epoch:  4\n",
            "19/25.0 loss: 0.6151524692773819 \n",
            "Epoch:  4\n",
            "20/25.0 loss: 0.6144564066614423 \n",
            "Epoch:  4\n",
            "21/25.0 loss: 0.6174691861326044 \n",
            "Epoch:  4\n",
            "22/25.0 loss: 0.6153811890146007 \n",
            "Epoch:  4\n",
            "23/25.0 loss: 0.6079837282498678 \n",
            "Epoch:  4\n",
            "24/25.0 loss: 0.6060913038253785 \n",
            "Epoch:  5\n",
            "0/25.0 loss: 0.6061887741088867 \n",
            "Epoch:  5\n",
            "1/25.0 loss: 0.614674299955368 \n",
            "Epoch:  5\n",
            "2/25.0 loss: 0.5826328992843628 \n",
            "Epoch:  5\n",
            "3/25.0 loss: 0.5940200686454773 \n",
            "Epoch:  5\n",
            "4/25.0 loss: 0.6140402555465698 \n",
            "Epoch:  5\n",
            "5/25.0 loss: 0.6239837805430094 \n",
            "Epoch:  5\n",
            "6/25.0 loss: 0.600955639566694 \n",
            "Epoch:  5\n",
            "7/25.0 loss: 0.5918835252523422 \n",
            "Epoch:  5\n",
            "8/25.0 loss: 0.5948156846894158 \n",
            "Epoch:  5\n",
            "9/25.0 loss: 0.6016098022460937 \n",
            "Epoch:  5\n",
            "10/25.0 loss: 0.6030205921693281 \n",
            "Epoch:  5\n",
            "11/25.0 loss: 0.6048685908317566 \n",
            "Epoch:  5\n",
            "12/25.0 loss: 0.601583366210644 \n",
            "Epoch:  5\n",
            "13/25.0 loss: 0.6063904166221619 \n",
            "Epoch:  5\n",
            "14/25.0 loss: 0.6054067293802897 \n",
            "Epoch:  5\n",
            "15/25.0 loss: 0.6112876236438751 \n",
            "Epoch:  5\n",
            "16/25.0 loss: 0.6064868919989642 \n",
            "Epoch:  5\n",
            "17/25.0 loss: 0.6076214280393388 \n",
            "Epoch:  5\n",
            "18/25.0 loss: 0.612116979925256 \n",
            "Epoch:  5\n",
            "19/25.0 loss: 0.615452966094017 \n",
            "Epoch:  5\n",
            "20/25.0 loss: 0.6112337651706877 \n",
            "Epoch:  5\n",
            "21/25.0 loss: 0.6102480617436495 \n",
            "Epoch:  5\n",
            "22/25.0 loss: 0.6098898493725321 \n",
            "Epoch:  5\n",
            "23/25.0 loss: 0.6075780217846235 \n",
            "Epoch:  5\n",
            "24/25.0 loss: 0.609312846660614 \n",
            "Epoch:  6\n",
            "0/25.0 loss: 0.5841377973556519 \n",
            "Epoch:  6\n",
            "1/25.0 loss: 0.611223042011261 \n",
            "Epoch:  6\n",
            "2/25.0 loss: 0.5866866906483968 \n",
            "Epoch:  6\n",
            "3/25.0 loss: 0.6271509826183319 \n",
            "Epoch:  6\n",
            "4/25.0 loss: 0.6231571555137634 \n",
            "Epoch:  6\n",
            "5/25.0 loss: 0.6238972842693329 \n",
            "Epoch:  6\n",
            "6/25.0 loss: 0.6147184542247227 \n",
            "Epoch:  6\n",
            "7/25.0 loss: 0.6235757321119308 \n",
            "Epoch:  6\n",
            "8/25.0 loss: 0.6115070647663541 \n",
            "Epoch:  6\n",
            "9/25.0 loss: 0.6024942576885224 \n",
            "Epoch:  6\n",
            "10/25.0 loss: 0.5998727029020136 \n",
            "Epoch:  6\n",
            "11/25.0 loss: 0.5981751382350922 \n",
            "Epoch:  6\n",
            "12/25.0 loss: 0.6037086615195641 \n",
            "Epoch:  6\n",
            "13/25.0 loss: 0.6031238606997898 \n",
            "Epoch:  6\n",
            "14/25.0 loss: 0.5955023308595021 \n",
            "Epoch:  6\n",
            "15/25.0 loss: 0.5985827762633562 \n",
            "Epoch:  6\n",
            "16/25.0 loss: 0.5934266290243935 \n",
            "Epoch:  6\n",
            "17/25.0 loss: 0.5946745293007957 \n",
            "Epoch:  6\n",
            "18/25.0 loss: 0.5987964539151442 \n",
            "Epoch:  6\n",
            "19/25.0 loss: 0.5967324838042259 \n",
            "Epoch:  6\n",
            "20/25.0 loss: 0.5964956723508381 \n",
            "Epoch:  6\n",
            "21/25.0 loss: 0.595128442753445 \n",
            "Epoch:  6\n",
            "22/25.0 loss: 0.6006173426690309 \n",
            "Epoch:  6\n",
            "23/25.0 loss: 0.6029851871232191 \n",
            "Epoch:  6\n",
            "24/25.0 loss: 0.6080330789089203 \n",
            "Epoch:  7\n",
            "0/25.0 loss: 0.6594040393829346 \n",
            "Epoch:  7\n",
            "1/25.0 loss: 0.6630594730377197 \n",
            "Epoch:  7\n",
            "2/25.0 loss: 0.6319676041603088 \n",
            "Epoch:  7\n",
            "3/25.0 loss: 0.6302339881658554 \n",
            "Epoch:  7\n",
            "4/25.0 loss: 0.6252897143363952 \n",
            "Epoch:  7\n",
            "5/25.0 loss: 0.6334775388240814 \n",
            "Epoch:  7\n",
            "6/25.0 loss: 0.6335167714527675 \n",
            "Epoch:  7\n",
            "7/25.0 loss: 0.6365165933966637 \n",
            "Epoch:  7\n",
            "8/25.0 loss: 0.6367593010266622 \n",
            "Epoch:  7\n",
            "9/25.0 loss: 0.6362132608890534 \n",
            "Epoch:  7\n",
            "10/25.0 loss: 0.6331881338899786 \n",
            "Epoch:  7\n",
            "11/25.0 loss: 0.6319061666727066 \n",
            "Epoch:  7\n",
            "12/25.0 loss: 0.6271913647651672 \n",
            "Epoch:  7\n",
            "13/25.0 loss: 0.6176903865167073 \n",
            "Epoch:  7\n",
            "14/25.0 loss: 0.621769255399704 \n",
            "Epoch:  7\n",
            "15/25.0 loss: 0.6195833031088114 \n",
            "Epoch:  7\n",
            "16/25.0 loss: 0.6153242325081545 \n",
            "Epoch:  7\n",
            "17/25.0 loss: 0.6137116038137012 \n",
            "Epoch:  7\n",
            "18/25.0 loss: 0.6093889647408536 \n",
            "Epoch:  7\n",
            "19/25.0 loss: 0.6043771758675576 \n",
            "Epoch:  7\n",
            "20/25.0 loss: 0.6033949468817029 \n",
            "Epoch:  7\n",
            "21/25.0 loss: 0.6032315831292759 \n",
            "Epoch:  7\n",
            "22/25.0 loss: 0.6014544069766998 \n",
            "Epoch:  7\n",
            "23/25.0 loss: 0.6047057248651981 \n",
            "Epoch:  7\n",
            "24/25.0 loss: 0.607186723947525 \n",
            "Epoch:  8\n",
            "0/25.0 loss: 0.6687105298042297 \n",
            "Epoch:  8\n",
            "1/25.0 loss: 0.618983656167984 \n",
            "Epoch:  8\n",
            "2/25.0 loss: 0.5684158802032471 \n",
            "Epoch:  8\n",
            "3/25.0 loss: 0.5895904898643494 \n",
            "Epoch:  8\n",
            "4/25.0 loss: 0.604001235961914 \n",
            "Epoch:  8\n",
            "5/25.0 loss: 0.5925943454106649 \n",
            "Epoch:  8\n",
            "6/25.0 loss: 0.6036074416978019 \n",
            "Epoch:  8\n",
            "7/25.0 loss: 0.5989193469285965 \n",
            "Epoch:  8\n",
            "8/25.0 loss: 0.6036159329944186 \n",
            "Epoch:  8\n",
            "9/25.0 loss: 0.6108134746551513 \n",
            "Epoch:  8\n",
            "10/25.0 loss: 0.6130523464896462 \n",
            "Epoch:  8\n",
            "11/25.0 loss: 0.6137941479682922 \n",
            "Epoch:  8\n",
            "12/25.0 loss: 0.6077711673883291 \n",
            "Epoch:  8\n",
            "13/25.0 loss: 0.6132810711860657 \n",
            "Epoch:  8\n",
            "14/25.0 loss: 0.6041371663411458 \n",
            "Epoch:  8\n",
            "15/25.0 loss: 0.6065797433257103 \n",
            "Epoch:  8\n",
            "16/25.0 loss: 0.6070630410138298 \n",
            "Epoch:  8\n",
            "17/25.0 loss: 0.6112607849968804 \n",
            "Epoch:  8\n",
            "18/25.0 loss: 0.6070256327327929 \n",
            "Epoch:  8\n",
            "19/25.0 loss: 0.6050243705511094 \n",
            "Epoch:  8\n",
            "20/25.0 loss: 0.6014813127971831 \n",
            "Epoch:  8\n",
            "21/25.0 loss: 0.5998741388320923 \n",
            "Epoch:  8\n",
            "22/25.0 loss: 0.604523796102275 \n",
            "Epoch:  8\n",
            "23/25.0 loss: 0.6096021210153898 \n",
            "Epoch:  8\n",
            "24/25.0 loss: 0.6083403635025024 \n",
            "Epoch:  9\n",
            "0/25.0 loss: 0.7351407408714294 \n",
            "Epoch:  9\n",
            "1/25.0 loss: 0.6489094793796539 \n",
            "Epoch:  9\n",
            "2/25.0 loss: 0.6600644389788309 \n",
            "Epoch:  9\n",
            "3/25.0 loss: 0.6387088149785995 \n",
            "Epoch:  9\n",
            "4/25.0 loss: 0.6486560821533203 \n",
            "Epoch:  9\n",
            "5/25.0 loss: 0.6229345003763834 \n",
            "Epoch:  9\n",
            "6/25.0 loss: 0.6343813453401838 \n",
            "Epoch:  9\n",
            "7/25.0 loss: 0.6253516525030136 \n",
            "Epoch:  9\n",
            "8/25.0 loss: 0.6196008655760024 \n",
            "Epoch:  9\n",
            "9/25.0 loss: 0.623959755897522 \n",
            "Epoch:  9\n",
            "10/25.0 loss: 0.6267366409301758 \n",
            "Epoch:  9\n",
            "11/25.0 loss: 0.6299574275811514 \n",
            "Epoch:  9\n",
            "12/25.0 loss: 0.6334747534531814 \n",
            "Epoch:  9\n",
            "13/25.0 loss: 0.6246579885482788 \n",
            "Epoch:  9\n",
            "14/25.0 loss: 0.6189004063606263 \n",
            "Epoch:  9\n",
            "15/25.0 loss: 0.6147208511829376 \n",
            "Epoch:  9\n",
            "16/25.0 loss: 0.61327405536876 \n",
            "Epoch:  9\n",
            "17/25.0 loss: 0.6194981137911478 \n",
            "Epoch:  9\n",
            "18/25.0 loss: 0.6162380983954981 \n",
            "Epoch:  9\n",
            "19/25.0 loss: 0.6130966424942017 \n",
            "Epoch:  9\n",
            "20/25.0 loss: 0.611799731141045 \n",
            "Epoch:  9\n",
            "21/25.0 loss: 0.606510650027882 \n",
            "Epoch:  9\n",
            "22/25.0 loss: 0.6062827628591786 \n",
            "Epoch:  9\n",
            "23/25.0 loss: 0.6057349940141042 \n",
            "Epoch:  9\n",
            "24/25.0 loss: 0.604887671470642 \n",
            "Epoch:  10\n",
            "0/25.0 loss: 0.6160197257995605 \n",
            "Epoch:  10\n",
            "1/25.0 loss: 0.5914082229137421 \n",
            "Epoch:  10\n",
            "2/25.0 loss: 0.601572851339976 \n",
            "Epoch:  10\n",
            "3/25.0 loss: 0.6049136072397232 \n",
            "Epoch:  10\n",
            "4/25.0 loss: 0.5881890654563904 \n",
            "Epoch:  10\n",
            "5/25.0 loss: 0.5843047102292379 \n",
            "Epoch:  10\n",
            "6/25.0 loss: 0.5939978872026715 \n",
            "Epoch:  10\n",
            "7/25.0 loss: 0.5900046154856682 \n",
            "Epoch:  10\n",
            "8/25.0 loss: 0.598842547999488 \n",
            "Epoch:  10\n",
            "9/25.0 loss: 0.6199227869510651 \n",
            "Epoch:  10\n",
            "10/25.0 loss: 0.6144607392224398 \n",
            "Epoch:  10\n",
            "11/25.0 loss: 0.6089762151241302 \n",
            "Epoch:  10\n",
            "12/25.0 loss: 0.6093895802131066 \n",
            "Epoch:  10\n",
            "13/25.0 loss: 0.6020366634641375 \n",
            "Epoch:  10\n",
            "14/25.0 loss: 0.5983721653620402 \n",
            "Epoch:  10\n",
            "15/25.0 loss: 0.5973140746355057 \n",
            "Epoch:  10\n",
            "16/25.0 loss: 0.6021884574609644 \n",
            "Epoch:  10\n",
            "17/25.0 loss: 0.6057797537909614 \n",
            "Epoch:  10\n",
            "18/25.0 loss: 0.6064909320128592 \n",
            "Epoch:  10\n",
            "19/25.0 loss: 0.6088823676109314 \n",
            "Epoch:  10\n",
            "20/25.0 loss: 0.6053027652558827 \n",
            "Epoch:  10\n",
            "21/25.0 loss: 0.612805041399869 \n",
            "Epoch:  10\n",
            "22/25.0 loss: 0.6120115518569946 \n",
            "Epoch:  10\n",
            "23/25.0 loss: 0.6083474978804588 \n",
            "Epoch:  10\n",
            "24/25.0 loss: 0.6081154990196228 \n",
            "Epoch:  11\n",
            "0/25.0 loss: 0.603789210319519 \n",
            "Epoch:  11\n",
            "1/25.0 loss: 0.6129820346832275 \n",
            "Epoch:  11\n",
            "2/25.0 loss: 0.6044373512268066 \n",
            "Epoch:  11\n",
            "3/25.0 loss: 0.574173778295517 \n",
            "Epoch:  11\n",
            "4/25.0 loss: 0.6002421855926514 \n",
            "Epoch:  11\n",
            "5/25.0 loss: 0.5960882902145386 \n",
            "Epoch:  11\n",
            "6/25.0 loss: 0.605185866355896 \n",
            "Epoch:  11\n",
            "7/25.0 loss: 0.6018003597855568 \n",
            "Epoch:  11\n",
            "8/25.0 loss: 0.6019459631707933 \n",
            "Epoch:  11\n",
            "9/25.0 loss: 0.6040640056133271 \n",
            "Epoch:  11\n",
            "10/25.0 loss: 0.6063223047689958 \n",
            "Epoch:  11\n",
            "11/25.0 loss: 0.6049702415863673 \n",
            "Epoch:  11\n",
            "12/25.0 loss: 0.6029913058647742 \n",
            "Epoch:  11\n",
            "13/25.0 loss: 0.6080437813486371 \n",
            "Epoch:  11\n",
            "14/25.0 loss: 0.607026994228363 \n",
            "Epoch:  11\n",
            "15/25.0 loss: 0.6043280772864819 \n",
            "Epoch:  11\n",
            "16/25.0 loss: 0.6004170459859511 \n",
            "Epoch:  11\n",
            "17/25.0 loss: 0.6051992509100173 \n",
            "Epoch:  11\n",
            "18/25.0 loss: 0.6015560062308061 \n",
            "Epoch:  11\n",
            "19/25.0 loss: 0.607231855392456 \n",
            "Epoch:  11\n",
            "20/25.0 loss: 0.6081582818712506 \n",
            "Epoch:  11\n",
            "21/25.0 loss: 0.6125318489291451 \n",
            "Epoch:  11\n",
            "22/25.0 loss: 0.610942586608555 \n",
            "Epoch:  11\n",
            "23/25.0 loss: 0.6086630150675774 \n",
            "Epoch:  11\n",
            "24/25.0 loss: 0.6050483345985412 \n",
            "Epoch:  12\n",
            "0/25.0 loss: 0.5988547801971436 \n",
            "Epoch:  12\n",
            "1/25.0 loss: 0.5953013002872467 \n",
            "Epoch:  12\n",
            "2/25.0 loss: 0.5837641755739847 \n",
            "Epoch:  12\n",
            "3/25.0 loss: 0.5943950563669205 \n",
            "Epoch:  12\n",
            "4/25.0 loss: 0.614789092540741 \n",
            "Epoch:  12\n",
            "5/25.0 loss: 0.6175650656223297 \n",
            "Epoch:  12\n",
            "6/25.0 loss: 0.6252832583018711 \n",
            "Epoch:  12\n",
            "7/25.0 loss: 0.6219347715377808 \n",
            "Epoch:  12\n",
            "8/25.0 loss: 0.6296506259176466 \n",
            "Epoch:  12\n",
            "9/25.0 loss: 0.627852076292038 \n",
            "Epoch:  12\n",
            "10/25.0 loss: 0.6209282766688954 \n",
            "Epoch:  12\n",
            "11/25.0 loss: 0.6252479006846746 \n",
            "Epoch:  12\n",
            "12/25.0 loss: 0.6311281231733469 \n",
            "Epoch:  12\n",
            "13/25.0 loss: 0.6250193331922803 \n",
            "Epoch:  12\n",
            "14/25.0 loss: 0.6243344306945801 \n",
            "Epoch:  12\n",
            "15/25.0 loss: 0.6226537451148033 \n",
            "Epoch:  12\n",
            "16/25.0 loss: 0.6149449769188376 \n",
            "Epoch:  12\n",
            "17/25.0 loss: 0.6168655951817831 \n",
            "Epoch:  12\n",
            "18/25.0 loss: 0.6131934053019473 \n",
            "Epoch:  12\n",
            "19/25.0 loss: 0.6084337860345841 \n",
            "Epoch:  12\n",
            "20/25.0 loss: 0.6035198228699821 \n",
            "Epoch:  12\n",
            "21/25.0 loss: 0.6012353544885461 \n",
            "Epoch:  12\n",
            "22/25.0 loss: 0.6008998544319816 \n",
            "Epoch:  12\n",
            "23/25.0 loss: 0.6087836002310117 \n",
            "Epoch:  12\n",
            "24/25.0 loss: 0.6040626382827758 \n",
            "Epoch:  13\n",
            "0/25.0 loss: 0.5668851137161255 \n",
            "Epoch:  13\n",
            "1/25.0 loss: 0.6582748293876648 \n",
            "Epoch:  13\n",
            "2/25.0 loss: 0.6526574492454529 \n",
            "Epoch:  13\n",
            "3/25.0 loss: 0.6468769758939743 \n",
            "Epoch:  13\n",
            "4/25.0 loss: 0.6394494652748108 \n",
            "Epoch:  13\n",
            "5/25.0 loss: 0.6462174952030182 \n",
            "Epoch:  13\n",
            "6/25.0 loss: 0.641844630241394 \n",
            "Epoch:  13\n",
            "7/25.0 loss: 0.6338132545351982 \n",
            "Epoch:  13\n",
            "8/25.0 loss: 0.6386033958858914 \n",
            "Epoch:  13\n",
            "9/25.0 loss: 0.6280045330524444 \n",
            "Epoch:  13\n",
            "10/25.0 loss: 0.6283451210368763 \n",
            "Epoch:  13\n",
            "11/25.0 loss: 0.6233329673608144 \n",
            "Epoch:  13\n",
            "12/25.0 loss: 0.6171288306896503 \n",
            "Epoch:  13\n",
            "13/25.0 loss: 0.6189780064991542 \n",
            "Epoch:  13\n",
            "14/25.0 loss: 0.6179645498593648 \n",
            "Epoch:  13\n",
            "15/25.0 loss: 0.6129381768405437 \n",
            "Epoch:  13\n",
            "16/25.0 loss: 0.607654308571535 \n",
            "Epoch:  13\n",
            "17/25.0 loss: 0.5992258538802465 \n",
            "Epoch:  13\n",
            "18/25.0 loss: 0.6076546609401703 \n",
            "Epoch:  13\n",
            "19/25.0 loss: 0.6126630917191506 \n",
            "Epoch:  13\n",
            "20/25.0 loss: 0.6038477775596437 \n",
            "Epoch:  13\n",
            "21/25.0 loss: 0.601707248525186 \n",
            "Epoch:  13\n",
            "22/25.0 loss: 0.6036774969619253 \n",
            "Epoch:  13\n",
            "23/25.0 loss: 0.604972934971253 \n",
            "Epoch:  13\n",
            "24/25.0 loss: 0.6087223446369171 \n",
            "Epoch:  14\n",
            "0/25.0 loss: 0.5150367021560669 \n",
            "Epoch:  14\n",
            "1/25.0 loss: 0.5690757632255554 \n",
            "Epoch:  14\n",
            "2/25.0 loss: 0.5861462752024332 \n",
            "Epoch:  14\n",
            "3/25.0 loss: 0.5976625978946686 \n",
            "Epoch:  14\n",
            "4/25.0 loss: 0.5951573610305786 \n",
            "Epoch:  14\n",
            "5/25.0 loss: 0.5817933877309164 \n",
            "Epoch:  14\n",
            "6/25.0 loss: 0.597035322870527 \n",
            "Epoch:  14\n",
            "7/25.0 loss: 0.5936990082263947 \n",
            "Epoch:  14\n",
            "8/25.0 loss: 0.592899329132504 \n",
            "Epoch:  14\n",
            "9/25.0 loss: 0.5978294372558594 \n",
            "Epoch:  14\n",
            "10/25.0 loss: 0.6050872477618131 \n",
            "Epoch:  14\n",
            "11/25.0 loss: 0.6122751384973526 \n",
            "Epoch:  14\n",
            "12/25.0 loss: 0.6124308063433721 \n",
            "Epoch:  14\n",
            "13/25.0 loss: 0.6090481749602726 \n",
            "Epoch:  14\n",
            "14/25.0 loss: 0.6036631306012471 \n",
            "Epoch:  14\n",
            "15/25.0 loss: 0.6095516234636307 \n",
            "Epoch:  14\n",
            "16/25.0 loss: 0.6032773116055656 \n",
            "Epoch:  14\n",
            "17/25.0 loss: 0.5989443129963345 \n",
            "Epoch:  14\n",
            "18/25.0 loss: 0.5977956495786968 \n",
            "Epoch:  14\n",
            "19/25.0 loss: 0.6037016302347183 \n",
            "Epoch:  14\n",
            "20/25.0 loss: 0.6005290349324545 \n",
            "Epoch:  14\n",
            "21/25.0 loss: 0.6015885174274445 \n",
            "Epoch:  14\n",
            "22/25.0 loss: 0.6016767621040344 \n",
            "Epoch:  14\n",
            "23/25.0 loss: 0.6076783364017805 \n",
            "Epoch:  14\n",
            "24/25.0 loss: 0.6057755637168885 \n",
            "Epoch:  15\n",
            "0/25.0 loss: 0.5967357754707336 \n",
            "Epoch:  15\n",
            "1/25.0 loss: 0.5639849901199341 \n",
            "Epoch:  15\n",
            "2/25.0 loss: 0.5722876787185669 \n",
            "Epoch:  15\n",
            "3/25.0 loss: 0.581090584397316 \n",
            "Epoch:  15\n",
            "4/25.0 loss: 0.553687435388565 \n",
            "Epoch:  15\n",
            "5/25.0 loss: 0.5704071670770645 \n",
            "Epoch:  15\n",
            "6/25.0 loss: 0.5832411774567196 \n",
            "Epoch:  15\n",
            "7/25.0 loss: 0.6083046980202198 \n",
            "Epoch:  15\n",
            "8/25.0 loss: 0.5950097276104821 \n",
            "Epoch:  15\n",
            "9/25.0 loss: 0.5982387691736222 \n",
            "Epoch:  15\n",
            "10/25.0 loss: 0.6031962118365548 \n",
            "Epoch:  15\n",
            "11/25.0 loss: 0.6020389919479688 \n",
            "Epoch:  15\n",
            "12/25.0 loss: 0.5949486700388101 \n",
            "Epoch:  15\n",
            "13/25.0 loss: 0.5955794474908284 \n",
            "Epoch:  15\n",
            "14/25.0 loss: 0.5920229057470957 \n",
            "Epoch:  15\n",
            "15/25.0 loss: 0.5960719082504511 \n",
            "Epoch:  15\n",
            "16/25.0 loss: 0.5923089192194098 \n",
            "Epoch:  15\n",
            "17/25.0 loss: 0.6032636480198966 \n",
            "Epoch:  15\n",
            "18/25.0 loss: 0.6106270285029161 \n",
            "Epoch:  15\n",
            "19/25.0 loss: 0.6078569695353508 \n",
            "Epoch:  15\n",
            "20/25.0 loss: 0.6096734872886113 \n",
            "Epoch:  15\n",
            "21/25.0 loss: 0.6085601029070941 \n",
            "Epoch:  15\n",
            "22/25.0 loss: 0.6049045440943345 \n",
            "Epoch:  15\n",
            "23/25.0 loss: 0.6036617420613766 \n",
            "Epoch:  15\n",
            "24/25.0 loss: 0.6041382873058319 \n",
            "Epoch:  16\n",
            "0/25.0 loss: 0.6565176248550415 \n",
            "Epoch:  16\n",
            "1/25.0 loss: 0.5932578444480896 \n",
            "Epoch:  16\n",
            "2/25.0 loss: 0.6223459641138712 \n",
            "Epoch:  16\n",
            "3/25.0 loss: 0.6239426583051682 \n",
            "Epoch:  16\n",
            "4/25.0 loss: 0.6081995129585266 \n",
            "Epoch:  16\n",
            "5/25.0 loss: 0.620437761147817 \n",
            "Epoch:  16\n",
            "6/25.0 loss: 0.6346943974494934 \n",
            "Epoch:  16\n",
            "7/25.0 loss: 0.6247611418366432 \n",
            "Epoch:  16\n",
            "8/25.0 loss: 0.6183822883499993 \n",
            "Epoch:  16\n",
            "9/25.0 loss: 0.619022649526596 \n",
            "Epoch:  16\n",
            "10/25.0 loss: 0.613810582594438 \n",
            "Epoch:  16\n",
            "11/25.0 loss: 0.6078494489192963 \n",
            "Epoch:  16\n",
            "12/25.0 loss: 0.6147971795155451 \n",
            "Epoch:  16\n",
            "13/25.0 loss: 0.6019882985523769 \n",
            "Epoch:  16\n",
            "14/25.0 loss: 0.5989879051844279 \n",
            "Epoch:  16\n",
            "15/25.0 loss: 0.6016238890588284 \n",
            "Epoch:  16\n",
            "16/25.0 loss: 0.596346073290881 \n",
            "Epoch:  16\n",
            "17/25.0 loss: 0.5946015616257986 \n",
            "Epoch:  16\n",
            "18/25.0 loss: 0.6040718273112649 \n",
            "Epoch:  16\n",
            "19/25.0 loss: 0.6084783196449279 \n",
            "Epoch:  16\n",
            "20/25.0 loss: 0.601711338474637 \n",
            "Epoch:  16\n",
            "21/25.0 loss: 0.5984847437251698 \n",
            "Epoch:  16\n",
            "22/25.0 loss: 0.5998967238094496 \n",
            "Epoch:  16\n",
            "23/25.0 loss: 0.603679987291495 \n",
            "Epoch:  16\n",
            "24/25.0 loss: 0.6028695917129516 \n",
            "Epoch:  17\n",
            "0/25.0 loss: 0.6948453187942505 \n",
            "Epoch:  17\n",
            "1/25.0 loss: 0.6852355599403381 \n",
            "Epoch:  17\n",
            "2/25.0 loss: 0.6444109280904134 \n",
            "Epoch:  17\n",
            "3/25.0 loss: 0.6445144712924957 \n",
            "Epoch:  17\n",
            "4/25.0 loss: 0.6413553714752197 \n",
            "Epoch:  17\n",
            "5/25.0 loss: 0.632489283879598 \n",
            "Epoch:  17\n",
            "6/25.0 loss: 0.633156452860151 \n",
            "Epoch:  17\n",
            "7/25.0 loss: 0.6276803612709045 \n",
            "Epoch:  17\n",
            "8/25.0 loss: 0.6254103051291572 \n",
            "Epoch:  17\n",
            "9/25.0 loss: 0.6193337500095367 \n",
            "Epoch:  17\n",
            "10/25.0 loss: 0.6121013543822549 \n",
            "Epoch:  17\n",
            "11/25.0 loss: 0.6061657865842184 \n",
            "Epoch:  17\n",
            "12/25.0 loss: 0.5992766527029184 \n",
            "Epoch:  17\n",
            "13/25.0 loss: 0.6033657661506108 \n",
            "Epoch:  17\n",
            "14/25.0 loss: 0.6015324393908182 \n",
            "Epoch:  17\n",
            "15/25.0 loss: 0.6036948747932911 \n",
            "Epoch:  17\n",
            "16/25.0 loss: 0.6062956873108359 \n",
            "Epoch:  17\n",
            "17/25.0 loss: 0.6164153781202104 \n",
            "Epoch:  17\n",
            "18/25.0 loss: 0.6114382524239389 \n",
            "Epoch:  17\n",
            "19/25.0 loss: 0.6066312968730927 \n",
            "Epoch:  17\n",
            "20/25.0 loss: 0.6096411716370356 \n",
            "Epoch:  17\n",
            "21/25.0 loss: 0.6146563291549683 \n",
            "Epoch:  17\n",
            "22/25.0 loss: 0.6158034982888595 \n",
            "Epoch:  17\n",
            "23/25.0 loss: 0.6090140665570895 \n",
            "Epoch:  17\n",
            "24/25.0 loss: 0.6075590205192566 \n",
            "Epoch:  18\n",
            "0/25.0 loss: 0.6257768869400024 \n",
            "Epoch:  18\n",
            "1/25.0 loss: 0.5535535514354706 \n",
            "Epoch:  18\n",
            "2/25.0 loss: 0.5694320400555929 \n",
            "Epoch:  18\n",
            "3/25.0 loss: 0.5958949476480484 \n",
            "Epoch:  18\n",
            "4/25.0 loss: 0.5985714197158813 \n",
            "Epoch:  18\n",
            "5/25.0 loss: 0.6225283543268839 \n",
            "Epoch:  18\n",
            "6/25.0 loss: 0.6046272814273834 \n",
            "Epoch:  18\n",
            "7/25.0 loss: 0.603280033916235 \n",
            "Epoch:  18\n",
            "8/25.0 loss: 0.5984172655476464 \n",
            "Epoch:  18\n",
            "9/25.0 loss: 0.5957550019025802 \n",
            "Epoch:  18\n",
            "10/25.0 loss: 0.5938440913503821 \n",
            "Epoch:  18\n",
            "11/25.0 loss: 0.5957981025179228 \n",
            "Epoch:  18\n",
            "12/25.0 loss: 0.5908303421277267 \n",
            "Epoch:  18\n",
            "13/25.0 loss: 0.5962178430386952 \n",
            "Epoch:  18\n",
            "14/25.0 loss: 0.6002322693665822 \n",
            "Epoch:  18\n",
            "15/25.0 loss: 0.6018436197191477 \n",
            "Epoch:  18\n",
            "16/25.0 loss: 0.6010628079666811 \n",
            "Epoch:  18\n",
            "17/25.0 loss: 0.5992518779304292 \n",
            "Epoch:  18\n",
            "18/25.0 loss: 0.6048500365332553 \n",
            "Epoch:  18\n",
            "19/25.0 loss: 0.606172476708889 \n",
            "Epoch:  18\n",
            "20/25.0 loss: 0.6046782717818305 \n",
            "Epoch:  18\n",
            "21/25.0 loss: 0.604891992428086 \n",
            "Epoch:  18\n",
            "22/25.0 loss: 0.6017155426999797 \n",
            "Epoch:  18\n",
            "23/25.0 loss: 0.6046058721840382 \n",
            "Epoch:  18\n",
            "24/25.0 loss: 0.605038355588913 \n",
            "Epoch:  19\n",
            "0/25.0 loss: 0.5693073868751526 \n",
            "Epoch:  19\n",
            "1/25.0 loss: 0.5494408309459686 \n",
            "Epoch:  19\n",
            "2/25.0 loss: 0.558938205242157 \n",
            "Epoch:  19\n",
            "3/25.0 loss: 0.6079934984445572 \n",
            "Epoch:  19\n",
            "4/25.0 loss: 0.6142488956451416 \n",
            "Epoch:  19\n",
            "5/25.0 loss: 0.5978674093882242 \n",
            "Epoch:  19\n",
            "6/25.0 loss: 0.5898298876626151 \n",
            "Epoch:  19\n",
            "7/25.0 loss: 0.5895822644233704 \n",
            "Epoch:  19\n",
            "8/25.0 loss: 0.5906767050425211 \n",
            "Epoch:  19\n",
            "9/25.0 loss: 0.5871976554393769 \n",
            "Epoch:  19\n",
            "10/25.0 loss: 0.5911631313237277 \n",
            "Epoch:  19\n",
            "11/25.0 loss: 0.593985045949618 \n",
            "Epoch:  19\n",
            "12/25.0 loss: 0.600877216229072 \n",
            "Epoch:  19\n",
            "13/25.0 loss: 0.598207141671862 \n",
            "Epoch:  19\n",
            "14/25.0 loss: 0.6006651401519776 \n",
            "Epoch:  19\n",
            "15/25.0 loss: 0.5925344992429018 \n",
            "Epoch:  19\n",
            "16/25.0 loss: 0.5957226279903861 \n",
            "Epoch:  19\n",
            "17/25.0 loss: 0.5964221242401335 \n",
            "Epoch:  19\n",
            "18/25.0 loss: 0.5960425844317988 \n",
            "Epoch:  19\n",
            "19/25.0 loss: 0.5920904859900474 \n",
            "Epoch:  19\n",
            "20/25.0 loss: 0.589116191580182 \n",
            "Epoch:  19\n",
            "21/25.0 loss: 0.595950342037461 \n",
            "Epoch:  19\n",
            "22/25.0 loss: 0.5943933453248895 \n",
            "Epoch:  19\n",
            "23/25.0 loss: 0.5966808038453261 \n",
            "Epoch:  19\n",
            "24/25.0 loss: 0.6018115842342376 \n",
            "Epoch:  20\n",
            "0/25.0 loss: 0.6188696622848511 \n",
            "Epoch:  20\n",
            "1/25.0 loss: 0.6262079477310181 \n",
            "Epoch:  20\n",
            "2/25.0 loss: 0.6459019184112549 \n",
            "Epoch:  20\n",
            "3/25.0 loss: 0.6420695185661316 \n",
            "Epoch:  20\n",
            "4/25.0 loss: 0.6342987418174744 \n",
            "Epoch:  20\n",
            "5/25.0 loss: 0.6254748602708181 \n",
            "Epoch:  20\n",
            "6/25.0 loss: 0.6309017028127398 \n",
            "Epoch:  20\n",
            "7/25.0 loss: 0.6339866146445274 \n",
            "Epoch:  20\n",
            "8/25.0 loss: 0.6307677957746718 \n",
            "Epoch:  20\n",
            "9/25.0 loss: 0.6290668249130249 \n",
            "Epoch:  20\n",
            "10/25.0 loss: 0.624506332657554 \n",
            "Epoch:  20\n",
            "11/25.0 loss: 0.6237832009792328 \n",
            "Epoch:  20\n",
            "12/25.0 loss: 0.6225806337136489 \n",
            "Epoch:  20\n",
            "13/25.0 loss: 0.6229225950581687 \n",
            "Epoch:  20\n",
            "14/25.0 loss: 0.6241721153259278 \n",
            "Epoch:  20\n",
            "15/25.0 loss: 0.6213703602552414 \n",
            "Epoch:  20\n",
            "16/25.0 loss: 0.6123483654330758 \n",
            "Epoch:  20\n",
            "17/25.0 loss: 0.6080061379406188 \n",
            "Epoch:  20\n",
            "18/25.0 loss: 0.6102931734762693 \n",
            "Epoch:  20\n",
            "19/25.0 loss: 0.6077104404568672 \n",
            "Epoch:  20\n",
            "20/25.0 loss: 0.6038684632096972 \n",
            "Epoch:  20\n",
            "21/25.0 loss: 0.6018902835520831 \n",
            "Epoch:  20\n",
            "22/25.0 loss: 0.6071484231430552 \n",
            "Epoch:  20\n",
            "23/25.0 loss: 0.606350239366293 \n",
            "Epoch:  20\n",
            "24/25.0 loss: 0.6027225387096405 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZffdI9Wn-dT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30cda7bb-9f3a-477a-f3d9-1f388c252637"
      },
      "source": [
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "target_names = ['Real News', 'Fake News']\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        # The new model has slightly different outputs. A sigmoid() is applied to logits to bound between 0 and 1\n",
        "        loss = loss_func(logits.sigmoid(), labels)\n",
        "        numpy_logits = logits.cpu().detach().numpy()\n",
        "        \n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "        all_logits += list(numpy_logits[:, 0])\n",
        "        \n",
        "print(classification_report(test_y, bert_predicted, target_names=target_names))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Real News       0.66      1.00      0.80        66\n",
            "   Fake News       0.00      0.00      0.00        34\n",
            "\n",
            "    accuracy                           0.66       100\n",
            "   macro avg       0.33      0.50      0.40       100\n",
            "weighted avg       0.44      0.66      0.52       100\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}