{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS490Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQIlpyMhBZHTB5HEvl09l7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Johoodcoder/CS490Project/blob/hood/Notebooks/CS490Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTZbJ1SJ53XO"
      },
      "source": [
        "Non-preinstalled module installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm5_ujD458U9"
      },
      "source": [
        "!pip install pytorch-pretrained-bert\r\n",
        "!pip install pytorch-nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw-rU23d7kyr"
      },
      "source": [
        "Import Dataset used in https://towardsdatascience.com/fake-news-classification-with-bert-afbeee601f41"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTaVuz8G_3nj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "_PN2iZvR7xuA",
        "outputId": "17ac4370-cd3c-43be-e9ec-efd9d5f8f5b9"
      },
      "source": [
        "from google.colab import files\r\n",
        "\r\n",
        "uploaded = files.upload()\r\n",
        "\r\n",
        "for fn in uploaded.keys():\r\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\r\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2d8a2a02-81cd-4030-a249-0cd50c191829\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2d8a2a02-81cd-4030-a249-0cd50c191829\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving fake.csv to fake.csv\n",
            "User uploaded file \"fake.csv\" with length 56680002 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vvyvLZY3Lnl"
      },
      "source": [
        "The base code from https://github.com/spierre91/medium_code/blob/master/fake_news_classifcation.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3aaLNA53TDt",
        "outputId": "cc49c69d-3551-4283-bf62-2ddf548aede1"
      },
      "source": [
        "\"\"\"\r\n",
        "Created on Tue Nov 19 14:17:24 2019\r\n",
        "\r\n",
        "@author: sadrachpierre\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "import torch.nn as nn\r\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\r\n",
        "import torch\r\n",
        "from torchnlp.datasets import imdb_dataset\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "\r\n",
        "pd.set_option('display.max_columns', None)\r\n",
        "train_data, test_data = imdb_dataset(train=True, test=True)\r\n",
        "df = pd.read_csv(\"fake.csv\")\r\n",
        "df = df[['text', 'type']]\r\n",
        "print(len(df))\r\n",
        "\r\n",
        "\r\n",
        "from collections import Counter \r\n",
        "\r\n",
        "print(Counter(df['type'].values))\r\n",
        "\r\n",
        "\r\n",
        "df = df[df['type'].isin(['fake', 'satire'])]\r\n",
        "df.dropna(inplace = True)\r\n",
        "df_fake = df[df['type'] == 'fake'] \r\n",
        "df_statire = df[df['type'] == 'satire'] \r\n",
        "df_statire = df_statire.sample(n=len(df_fake))\r\n",
        "df = df_statire.append(df_fake)\r\n",
        "df = df.sample(frac=1, random_state = 24).reset_index(drop=True)\r\n",
        "\r\n",
        "print(Counter(df['type'].values))\r\n",
        "\r\n",
        "train_data = df.head(19)\r\n",
        "test_data = df.tail(19)\r\n",
        "\r\n",
        "print(train_data)\r\n",
        "train_data = [{'text': text, 'type': type_data } for text in list(train_data['text']) for type_data in list(train_data['type'])]\r\n",
        "test_data = [{'text': text, 'type': type_data } for text in list(test_data['text']) for type_data in list(test_data['type'])]\r\n",
        "\r\n",
        "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['type']), train_data)))\r\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['type']), test_data)))\r\n",
        "\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\r\n",
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\r\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\r\n",
        "\r\n",
        "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\r\n",
        "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\r\n",
        "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\r\n",
        "\r\n",
        "\r\n",
        "train_y = np.array(train_labels) == 'fake'\r\n",
        "test_y = np.array(test_labels) == 'fake'\r\n",
        "\r\n",
        "#\r\n",
        "#\r\n",
        "class BertBinaryClassifier(nn.Module):\r\n",
        "    def __init__(self, dropout=0.1):\r\n",
        "        super(BertBinaryClassifier, self).__init__()\r\n",
        "\r\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        self.linear = nn.Linear(768, 1)\r\n",
        "        self.sigmoid = nn.Sigmoid()\r\n",
        "    \r\n",
        "    def forward(self, tokens, masks=None):\r\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\r\n",
        "        dropout_output = self.dropout(pooled_output)\r\n",
        "        linear_output = self.linear(dropout_output)\r\n",
        "        proba = self.sigmoid(linear_output)\r\n",
        "        return proba\r\n",
        "\r\n",
        "BATCH_SIZE = 1\r\n",
        "EPOCHS = 1\r\n",
        "\r\n",
        "\r\n",
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\r\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\r\n",
        "train_masks_tensor = torch.tensor(train_masks)\r\n",
        "test_masks_tensor = torch.tensor(test_masks)\r\n",
        "\r\n",
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\r\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\r\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\r\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\r\n",
        "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\r\n",
        "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\r\n",
        "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\r\n",
        "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\r\n",
        "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\r\n",
        "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\r\n",
        "\r\n",
        "\r\n",
        "bert_clf = BertBinaryClassifier()\r\n",
        "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\r\n",
        "\r\n",
        "for epoch_num in range(EPOCHS):\r\n",
        "    bert_clf.train()\r\n",
        "    train_loss = 0\r\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\r\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\r\n",
        "        probas = bert_clf(token_ids, masks)\r\n",
        "        loss_func = nn.BCELoss()\r\n",
        "        batch_loss = loss_func(probas, labels)\r\n",
        "        train_loss += batch_loss.item()\r\n",
        "        bert_clf.zero_grad()\r\n",
        "        batch_loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        print('Epoch: ', epoch_num + 1)\r\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\r\n",
        "\r\n",
        "bert_clf.eval()\r\n",
        "bert_predicted = []\r\n",
        "all_logits = []\r\n",
        "with torch.no_grad():\r\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\r\n",
        "\r\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\r\n",
        "\r\n",
        "        logits = bert_clf(token_ids, masks)\r\n",
        "        loss_func = nn.BCELoss()\r\n",
        "        loss = loss_func(logits, labels)\r\n",
        "        numpy_logits = logits.cpu().detach().numpy()\r\n",
        "        \r\n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\r\n",
        "        all_logits += list(numpy_logits[:, 0])\r\n",
        "        \r\n",
        "print(classification_report(test_y, bert_predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 84.1MB [00:09, 9.20MB/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "12999\n",
            "Counter({'bs': 11492, 'bias': 443, 'conspiracy': 430, 'hate': 246, 'satire': 146, 'state': 121, 'junksci': 102, 'fake': 19})\n",
            "Counter({'fake': 19, 'satire': 19})\n",
            "                                                 text    type\n",
            "0   adobochron 1 Comment Moyers \\nWASHINGTON, D.C....    fake\n",
            "1   I had a garage sale today, and a number of peo...  satire\n",
            "2   64 SHARE President Obama has signed an Executi...    fake\n",
            "3   Email \\nWell, here we are, gang. \\nThis mornin...  satire\n",
            "4   How Haunted Is Your House? Posted today Do you...  satire\n",
            "5   Email \\nIf this doesn’t prove what’s beautiful...  satire\n",
            "6   adobochron 7 Comments A rendering of the Trump...    fake\n",
            "7   Email Jane Goodall has dedicated her life to s...  satire\n",
            "8   Email Yoda is a sick, elderly reptile who live...  satire\n",
            "9   Search for: About Us \\nTHE ADOBO CHRONICLES is...    fake\n",
            "10  Humor Home Leftist Corruption Lady Gaga’s Twit...    fake\n",
            "11  adobochron Leave a comment \\nCUPERTINO, Califo...    fake\n",
            "12  Email Ever wonder what’s on the mind of today’...  satire\n",
            "13  Home Election 2016 Hillary Collapses On Her Wa...    fake\n",
            "14  Email Ever wonder what’s on the mind of today’...  satire\n",
            "15  Email , don’t miss our unbelievable roundup of...  satire\n",
            "16  adobochron 1 Comment \\nMANILA, Philippines (Th...    fake\n",
            "17  adobochron 10 Comments \\nSan Francisco, Califo...    fake\n",
            "18  Email \\nThis kid just seriously hit the jackpo...  satire\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 321814.13B/s]\n",
            "100%|██████████| 407873900/407873900 [00:27<00:00, 15028059.72B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "0/361.0 loss: 1.0572142601013184 \n",
            "Epoch:  1\n",
            "1/361.0 loss: 0.7538347244262695 \n",
            "Epoch:  1\n",
            "2/361.0 loss: 0.8466126124064127 \n",
            "Epoch:  1\n",
            "3/361.0 loss: 0.8546307981014252 \n",
            "Epoch:  1\n",
            "4/361.0 loss: 0.8722740054130554 \n",
            "Epoch:  1\n",
            "5/361.0 loss: 0.8197704056898752 \n",
            "Epoch:  1\n",
            "6/361.0 loss: 0.8600320219993591 \n",
            "Epoch:  1\n",
            "7/361.0 loss: 0.8688874766230583 \n",
            "Epoch:  1\n",
            "8/361.0 loss: 0.8304447796609666 \n",
            "Epoch:  1\n",
            "9/361.0 loss: 0.789332064986229 \n",
            "Epoch:  1\n",
            "10/361.0 loss: 0.7944554713639346 \n",
            "Epoch:  1\n",
            "11/361.0 loss: 0.795060507953167 \n",
            "Epoch:  1\n",
            "12/361.0 loss: 0.7770332258481246 \n",
            "Epoch:  1\n",
            "13/361.0 loss: 0.7610272147825786 \n",
            "Epoch:  1\n",
            "14/361.0 loss: 0.754082069794337 \n",
            "Epoch:  1\n",
            "15/361.0 loss: 0.7545354422181845 \n",
            "Epoch:  1\n",
            "16/361.0 loss: 0.7421477524673238 \n",
            "Epoch:  1\n",
            "17/361.0 loss: 0.751728731724951 \n",
            "Epoch:  1\n",
            "18/361.0 loss: 0.7414310872554779 \n",
            "Epoch:  1\n",
            "19/361.0 loss: 0.7449503496289254 \n",
            "Epoch:  1\n",
            "20/361.0 loss: 0.7364695171515147 \n",
            "Epoch:  1\n",
            "21/361.0 loss: 0.7321086376905441 \n",
            "Epoch:  1\n",
            "22/361.0 loss: 0.7354529279729595 \n",
            "Epoch:  1\n",
            "23/361.0 loss: 0.7316579384108385 \n",
            "Epoch:  1\n",
            "24/361.0 loss: 0.7230507338047027 \n",
            "Epoch:  1\n",
            "25/361.0 loss: 0.7293567760632589 \n",
            "Epoch:  1\n",
            "26/361.0 loss: 0.7298127401758123 \n",
            "Epoch:  1\n",
            "27/361.0 loss: 0.7232008438025203 \n",
            "Epoch:  1\n",
            "28/361.0 loss: 0.7281396255410951 \n",
            "Epoch:  1\n",
            "29/361.0 loss: 0.7188107748826345 \n",
            "Epoch:  1\n",
            "30/361.0 loss: 0.7169390436141722 \n",
            "Epoch:  1\n",
            "31/361.0 loss: 0.713169276714325 \n",
            "Epoch:  1\n",
            "32/361.0 loss: 0.7088361021244165 \n",
            "Epoch:  1\n",
            "33/361.0 loss: 0.7047261122395011 \n",
            "Epoch:  1\n",
            "34/361.0 loss: 0.7048997589520045 \n",
            "Epoch:  1\n",
            "35/361.0 loss: 0.7057390428251691 \n",
            "Epoch:  1\n",
            "36/361.0 loss: 0.715231342895611 \n",
            "Epoch:  1\n",
            "37/361.0 loss: 0.7082526385784149 \n",
            "Epoch:  1\n",
            "38/361.0 loss: 0.7162487185918368 \n",
            "Epoch:  1\n",
            "39/361.0 loss: 0.7112958014011384 \n",
            "Epoch:  1\n",
            "40/361.0 loss: 0.7180086635961765 \n",
            "Epoch:  1\n",
            "41/361.0 loss: 0.7198979741051084 \n",
            "Epoch:  1\n",
            "42/361.0 loss: 0.7165842472120773 \n",
            "Epoch:  1\n",
            "43/361.0 loss: 0.7138597680763765 \n",
            "Epoch:  1\n",
            "44/361.0 loss: 0.7121245092815823 \n",
            "Epoch:  1\n",
            "45/361.0 loss: 0.7095591399980627 \n",
            "Epoch:  1\n",
            "46/361.0 loss: 0.7079761281926581 \n",
            "Epoch:  1\n",
            "47/361.0 loss: 0.712404441088438 \n",
            "Epoch:  1\n",
            "48/361.0 loss: 0.7136997714334604 \n",
            "Epoch:  1\n",
            "49/361.0 loss: 0.7191487848758698 \n",
            "Epoch:  1\n",
            "50/361.0 loss: 0.7216711359865525 \n",
            "Epoch:  1\n",
            "51/361.0 loss: 0.7233838461912595 \n",
            "Epoch:  1\n",
            "52/361.0 loss: 0.7217047574385157 \n",
            "Epoch:  1\n",
            "53/361.0 loss: 0.7197142287536904 \n",
            "Epoch:  1\n",
            "54/361.0 loss: 0.7179598515683955 \n",
            "Epoch:  1\n",
            "55/361.0 loss: 0.7217982443315643 \n",
            "Epoch:  1\n",
            "56/361.0 loss: 0.7233390693078962 \n",
            "Epoch:  1\n",
            "57/361.0 loss: 0.7220189489167312 \n",
            "Epoch:  1\n",
            "58/361.0 loss: 0.7246091396121656 \n",
            "Epoch:  1\n",
            "59/361.0 loss: 0.7250467697779338 \n",
            "Epoch:  1\n",
            "60/361.0 loss: 0.7253609201947196 \n",
            "Epoch:  1\n",
            "61/361.0 loss: 0.7248940208265858 \n",
            "Epoch:  1\n",
            "62/361.0 loss: 0.7254405513642326 \n",
            "Epoch:  1\n",
            "63/361.0 loss: 0.7240983434021473 \n",
            "Epoch:  1\n",
            "64/361.0 loss: 0.7262414317864638 \n",
            "Epoch:  1\n",
            "65/361.0 loss: 0.7245901624361674 \n",
            "Epoch:  1\n",
            "66/361.0 loss: 0.7215655846382255 \n",
            "Epoch:  1\n",
            "67/361.0 loss: 0.7238534320803249 \n",
            "Epoch:  1\n",
            "68/361.0 loss: 0.723871440127276 \n",
            "Epoch:  1\n",
            "69/361.0 loss: 0.7241234787872859 \n",
            "Epoch:  1\n",
            "70/361.0 loss: 0.7231906425785011 \n",
            "Epoch:  1\n",
            "71/361.0 loss: 0.7241817737619082 \n",
            "Epoch:  1\n",
            "72/361.0 loss: 0.7237455975519468 \n",
            "Epoch:  1\n",
            "73/361.0 loss: 0.7244256791230794 \n",
            "Epoch:  1\n",
            "74/361.0 loss: 0.7229554867744445 \n",
            "Epoch:  1\n",
            "75/361.0 loss: 0.7222940474748611 \n",
            "Epoch:  1\n",
            "76/361.0 loss: 0.7214886801583427 \n",
            "Epoch:  1\n",
            "77/361.0 loss: 0.7223862264400873 \n",
            "Epoch:  1\n",
            "78/361.0 loss: 0.7218962854976896 \n",
            "Epoch:  1\n",
            "79/361.0 loss: 0.7221525758504868 \n",
            "Epoch:  1\n",
            "80/361.0 loss: 0.7225446325761301 \n",
            "Epoch:  1\n",
            "81/361.0 loss: 0.72178078570017 \n",
            "Epoch:  1\n",
            "82/361.0 loss: 0.7226560453334486 \n",
            "Epoch:  1\n",
            "83/361.0 loss: 0.7244733451377778 \n",
            "Epoch:  1\n",
            "84/361.0 loss: 0.723016613371232 \n",
            "Epoch:  1\n",
            "85/361.0 loss: 0.7231655793134556 \n",
            "Epoch:  1\n",
            "86/361.0 loss: 0.7225113552192162 \n",
            "Epoch:  1\n",
            "87/361.0 loss: 0.7209300561384722 \n",
            "Epoch:  1\n",
            "88/361.0 loss: 0.7222713211948952 \n",
            "Epoch:  1\n",
            "89/361.0 loss: 0.722200232744217 \n",
            "Epoch:  1\n",
            "90/361.0 loss: 0.7230574148041862 \n",
            "Epoch:  1\n",
            "91/361.0 loss: 0.7226766406194024 \n",
            "Epoch:  1\n",
            "92/361.0 loss: 0.7234296228296013 \n",
            "Epoch:  1\n",
            "93/361.0 loss: 0.7234995770961681 \n",
            "Epoch:  1\n",
            "94/361.0 loss: 0.7211337318545894 \n",
            "Epoch:  1\n",
            "95/361.0 loss: 0.721734412945807 \n",
            "Epoch:  1\n",
            "96/361.0 loss: 0.7223082378353041 \n",
            "Epoch:  1\n",
            "97/361.0 loss: 0.7222450536732771 \n",
            "Epoch:  1\n",
            "98/361.0 loss: 0.7221929855418928 \n",
            "Epoch:  1\n",
            "99/361.0 loss: 0.7234871736168862 \n",
            "Epoch:  1\n",
            "100/361.0 loss: 0.7230273293386592 \n",
            "Epoch:  1\n",
            "101/361.0 loss: 0.7234545154314415 \n",
            "Epoch:  1\n",
            "102/361.0 loss: 0.722929601241084 \n",
            "Epoch:  1\n",
            "103/361.0 loss: 0.7226715268423924 \n",
            "Epoch:  1\n",
            "104/361.0 loss: 0.7238237202167511 \n",
            "Epoch:  1\n",
            "105/361.0 loss: 0.7241088143497143 \n",
            "Epoch:  1\n",
            "106/361.0 loss: 0.7233349510999484 \n",
            "Epoch:  1\n",
            "107/361.0 loss: 0.7221922822020672 \n",
            "Epoch:  1\n",
            "108/361.0 loss: 0.7222234215758262 \n",
            "Epoch:  1\n",
            "109/361.0 loss: 0.7216204710982063 \n",
            "Epoch:  1\n",
            "110/361.0 loss: 0.7222794237974528 \n",
            "Epoch:  1\n",
            "111/361.0 loss: 0.723454172323857 \n",
            "Epoch:  1\n",
            "112/361.0 loss: 0.7239812177366916 \n",
            "Epoch:  1\n",
            "113/361.0 loss: 0.7234167197817251 \n",
            "Epoch:  1\n",
            "114/361.0 loss: 0.7242655632288559 \n",
            "Epoch:  1\n",
            "115/361.0 loss: 0.7249143064535898 \n",
            "Epoch:  1\n",
            "116/361.0 loss: 0.7249382013439113 \n",
            "Epoch:  1\n",
            "117/361.0 loss: 0.7255459278821945 \n",
            "Epoch:  1\n",
            "118/361.0 loss: 0.7239225985122328 \n",
            "Epoch:  1\n",
            "119/361.0 loss: 0.7224630820254485 \n",
            "Epoch:  1\n",
            "120/361.0 loss: 0.7215450270609423 \n",
            "Epoch:  1\n",
            "121/361.0 loss: 0.7219706673602588 \n",
            "Epoch:  1\n",
            "122/361.0 loss: 0.7216401814929838 \n",
            "Epoch:  1\n",
            "123/361.0 loss: 0.7232214118684491 \n",
            "Epoch:  1\n",
            "124/361.0 loss: 0.7250928056240081 \n",
            "Epoch:  1\n",
            "125/361.0 loss: 0.7243095818493102 \n",
            "Epoch:  1\n",
            "126/361.0 loss: 0.7232637515687567 \n",
            "Epoch:  1\n",
            "127/361.0 loss: 0.7224920985754579 \n",
            "Epoch:  1\n",
            "128/361.0 loss: 0.7219921158727749 \n",
            "Epoch:  1\n",
            "129/361.0 loss: 0.7226003970091159 \n",
            "Epoch:  1\n",
            "130/361.0 loss: 0.7223929510316776 \n",
            "Epoch:  1\n",
            "131/361.0 loss: 0.72242729162628 \n",
            "Epoch:  1\n",
            "132/361.0 loss: 0.7231549699055521 \n",
            "Epoch:  1\n",
            "133/361.0 loss: 0.7229351888396847 \n",
            "Epoch:  1\n",
            "134/361.0 loss: 0.7218137001549756 \n",
            "Epoch:  1\n",
            "135/361.0 loss: 0.7220068650648874 \n",
            "Epoch:  1\n",
            "136/361.0 loss: 0.7206998117213702 \n",
            "Epoch:  1\n",
            "137/361.0 loss: 0.7200614876937175 \n",
            "Epoch:  1\n",
            "138/361.0 loss: 0.7190888959298031 \n",
            "Epoch:  1\n",
            "139/361.0 loss: 0.7177829995751381 \n",
            "Epoch:  1\n",
            "140/361.0 loss: 0.7178102522454364 \n",
            "Epoch:  1\n",
            "141/361.0 loss: 0.7168358868696321 \n",
            "Epoch:  1\n",
            "142/361.0 loss: 0.7169206519643744 \n",
            "Epoch:  1\n",
            "143/361.0 loss: 0.716313538245029 \n",
            "Epoch:  1\n",
            "144/361.0 loss: 0.7166694030679506 \n",
            "Epoch:  1\n",
            "145/361.0 loss: 0.7158436524133159 \n",
            "Epoch:  1\n",
            "146/361.0 loss: 0.7156094367406807 \n",
            "Epoch:  1\n",
            "147/361.0 loss: 0.7151630514779607 \n",
            "Epoch:  1\n",
            "148/361.0 loss: 0.7158586968911574 \n",
            "Epoch:  1\n",
            "149/361.0 loss: 0.7183352905511856 \n",
            "Epoch:  1\n",
            "150/361.0 loss: 0.7173101221094068 \n",
            "Epoch:  1\n",
            "151/361.0 loss: 0.7163744198256418 \n",
            "Epoch:  1\n",
            "152/361.0 loss: 0.7151993047567754 \n",
            "Epoch:  1\n",
            "153/361.0 loss: 0.716277970315574 \n",
            "Epoch:  1\n",
            "154/361.0 loss: 0.7173856625633855 \n",
            "Epoch:  1\n",
            "155/361.0 loss: 0.7162597842323475 \n",
            "Epoch:  1\n",
            "156/361.0 loss: 0.7163340439842005 \n",
            "Epoch:  1\n",
            "157/361.0 loss: 0.7154350044983852 \n",
            "Epoch:  1\n",
            "158/361.0 loss: 0.7159831641979937 \n",
            "Epoch:  1\n",
            "159/361.0 loss: 0.715949030406773 \n",
            "Epoch:  1\n",
            "160/361.0 loss: 0.7147492979254041 \n",
            "Epoch:  1\n",
            "161/361.0 loss: 0.7155872070127063 \n",
            "Epoch:  1\n",
            "162/361.0 loss: 0.7153445703485992 \n",
            "Epoch:  1\n",
            "163/361.0 loss: 0.7146072825644074 \n",
            "Epoch:  1\n",
            "164/361.0 loss: 0.7146541116815625 \n",
            "Epoch:  1\n",
            "165/361.0 loss: 0.7154497452170016 \n",
            "Epoch:  1\n",
            "166/361.0 loss: 0.7154740817889482 \n",
            "Epoch:  1\n",
            "167/361.0 loss: 0.7148855862518152 \n",
            "Epoch:  1\n",
            "168/361.0 loss: 0.7140984272463082 \n",
            "Epoch:  1\n",
            "169/361.0 loss: 0.7153285526177462 \n",
            "Epoch:  1\n",
            "170/361.0 loss: 0.7157045963563418 \n",
            "Epoch:  1\n",
            "171/361.0 loss: 0.7157991017366565 \n",
            "Epoch:  1\n",
            "172/361.0 loss: 0.7156549611188083 \n",
            "Epoch:  1\n",
            "173/361.0 loss: 0.715253941457847 \n",
            "Epoch:  1\n",
            "174/361.0 loss: 0.7149697864055633 \n",
            "Epoch:  1\n",
            "175/361.0 loss: 0.7147375821051273 \n",
            "Epoch:  1\n",
            "176/361.0 loss: 0.7149442099245255 \n",
            "Epoch:  1\n",
            "177/361.0 loss: 0.7140170501524143 \n",
            "Epoch:  1\n",
            "178/361.0 loss: 0.7128919830868364 \n",
            "Epoch:  1\n",
            "179/361.0 loss: 0.7132087431020206 \n",
            "Epoch:  1\n",
            "180/361.0 loss: 0.7126667069796041 \n",
            "Epoch:  1\n",
            "181/361.0 loss: 0.712114114519004 \n",
            "Epoch:  1\n",
            "182/361.0 loss: 0.7111304762259207 \n",
            "Epoch:  1\n",
            "183/361.0 loss: 0.7110469916268535 \n",
            "Epoch:  1\n",
            "184/361.0 loss: 0.7125788329420863 \n",
            "Epoch:  1\n",
            "185/361.0 loss: 0.7138136207096039 \n",
            "Epoch:  1\n",
            "186/361.0 loss: 0.7130772664266474 \n",
            "Epoch:  1\n",
            "187/361.0 loss: 0.7139831402517379 \n",
            "Epoch:  1\n",
            "188/361.0 loss: 0.7141659901571021 \n",
            "Epoch:  1\n",
            "189/361.0 loss: 0.7152713576429769 \n",
            "Epoch:  1\n",
            "190/361.0 loss: 0.7145200988072999 \n",
            "Epoch:  1\n",
            "191/361.0 loss: 0.7135369974809388 \n",
            "Epoch:  1\n",
            "192/361.0 loss: 0.7130702188904421 \n",
            "Epoch:  1\n",
            "193/361.0 loss: 0.7147153688153041 \n",
            "Epoch:  1\n",
            "194/361.0 loss: 0.715922373227584 \n",
            "Epoch:  1\n",
            "195/361.0 loss: 0.7170703169338557 \n",
            "Epoch:  1\n",
            "196/361.0 loss: 0.7169608544879759 \n",
            "Epoch:  1\n",
            "197/361.0 loss: 0.7160442436885353 \n",
            "Epoch:  1\n",
            "198/361.0 loss: 0.7166207512119906 \n",
            "Epoch:  1\n",
            "199/361.0 loss: 0.7161752693355083 \n",
            "Epoch:  1\n",
            "200/361.0 loss: 0.716778017246901 \n",
            "Epoch:  1\n",
            "201/361.0 loss: 0.716595003067857 \n",
            "Epoch:  1\n",
            "202/361.0 loss: 0.7171473583858001 \n",
            "Epoch:  1\n",
            "203/361.0 loss: 0.7171114214202937 \n",
            "Epoch:  1\n",
            "204/361.0 loss: 0.7168489742569807 \n",
            "Epoch:  1\n",
            "205/361.0 loss: 0.7162619110160661 \n",
            "Epoch:  1\n",
            "206/361.0 loss: 0.71629298467567 \n",
            "Epoch:  1\n",
            "207/361.0 loss: 0.7159158772287461 \n",
            "Epoch:  1\n",
            "208/361.0 loss: 0.7166401631238928 \n",
            "Epoch:  1\n",
            "209/361.0 loss: 0.716765404173306 \n",
            "Epoch:  1\n",
            "210/361.0 loss: 0.7171378221839525 \n",
            "Epoch:  1\n",
            "211/361.0 loss: 0.7171233652054139 \n",
            "Epoch:  1\n",
            "212/361.0 loss: 0.7173678313902286 \n",
            "Epoch:  1\n",
            "213/361.0 loss: 0.7179865287007573 \n",
            "Epoch:  1\n",
            "214/361.0 loss: 0.7180155292499897 \n",
            "Epoch:  1\n",
            "215/361.0 loss: 0.7176742648912801 \n",
            "Epoch:  1\n",
            "216/361.0 loss: 0.7176759343268135 \n",
            "Epoch:  1\n",
            "217/361.0 loss: 0.7169990674891603 \n",
            "Epoch:  1\n",
            "218/361.0 loss: 0.7165175438199414 \n",
            "Epoch:  1\n",
            "219/361.0 loss: 0.7166112280704758 \n",
            "Epoch:  1\n",
            "220/361.0 loss: 0.7163941273592177 \n",
            "Epoch:  1\n",
            "221/361.0 loss: 0.7162426244299691 \n",
            "Epoch:  1\n",
            "222/361.0 loss: 0.7158225155999308 \n",
            "Epoch:  1\n",
            "223/361.0 loss: 0.7156420260933893 \n",
            "Epoch:  1\n",
            "224/361.0 loss: 0.7152223238680098 \n",
            "Epoch:  1\n",
            "225/361.0 loss: 0.7153772299004867 \n",
            "Epoch:  1\n",
            "226/361.0 loss: 0.7151333730388844 \n",
            "Epoch:  1\n",
            "227/361.0 loss: 0.71510269519007 \n",
            "Epoch:  1\n",
            "228/361.0 loss: 0.7147604103431952 \n",
            "Epoch:  1\n",
            "229/361.0 loss: 0.7149836792894032 \n",
            "Epoch:  1\n",
            "230/361.0 loss: 0.714145042808541 \n",
            "Epoch:  1\n",
            "231/361.0 loss: 0.7135599228585588 \n",
            "Epoch:  1\n",
            "232/361.0 loss: 0.7136460280469559 \n",
            "Epoch:  1\n",
            "233/361.0 loss: 0.7134814049698349 \n",
            "Epoch:  1\n",
            "234/361.0 loss: 0.7131303286298792 \n",
            "Epoch:  1\n",
            "235/361.0 loss: 0.7128598827679279 \n",
            "Epoch:  1\n",
            "236/361.0 loss: 0.7130773134120909 \n",
            "Epoch:  1\n",
            "237/361.0 loss: 0.7121660579152468 \n",
            "Epoch:  1\n",
            "238/361.0 loss: 0.711937257934315 \n",
            "Epoch:  1\n",
            "239/361.0 loss: 0.7118347195287545 \n",
            "Epoch:  1\n",
            "240/361.0 loss: 0.7119364181989456 \n",
            "Epoch:  1\n",
            "241/361.0 loss: 0.7125359805654888 \n",
            "Epoch:  1\n",
            "242/361.0 loss: 0.7121580336319566 \n",
            "Epoch:  1\n",
            "243/361.0 loss: 0.7114974849048208 \n",
            "Epoch:  1\n",
            "244/361.0 loss: 0.7105002029817932 \n",
            "Epoch:  1\n",
            "245/361.0 loss: 0.7101195768369892 \n",
            "Epoch:  1\n",
            "246/361.0 loss: 0.7093823425441619 \n",
            "Epoch:  1\n",
            "247/361.0 loss: 0.710434241040099 \n",
            "Epoch:  1\n",
            "248/361.0 loss: 0.7109064832030529 \n",
            "Epoch:  1\n",
            "249/361.0 loss: 0.711709598660469 \n",
            "Epoch:  1\n",
            "250/361.0 loss: 0.7130272090435028 \n",
            "Epoch:  1\n",
            "251/361.0 loss: 0.7137301958033017 \n",
            "Epoch:  1\n",
            "252/361.0 loss: 0.7133104053175026 \n",
            "Epoch:  1\n",
            "253/361.0 loss: 0.7136204797686554 \n",
            "Epoch:  1\n",
            "254/361.0 loss: 0.7129589565828734 \n",
            "Epoch:  1\n",
            "255/361.0 loss: 0.7134393086889759 \n",
            "Epoch:  1\n",
            "256/361.0 loss: 0.7125940110664887 \n",
            "Epoch:  1\n",
            "257/361.0 loss: 0.7122016460165497 \n",
            "Epoch:  1\n",
            "258/361.0 loss: 0.7117038846706332 \n",
            "Epoch:  1\n",
            "259/361.0 loss: 0.711089538037777 \n",
            "Epoch:  1\n",
            "260/361.0 loss: 0.7116790405863547 \n",
            "Epoch:  1\n",
            "261/361.0 loss: 0.7126270317621813 \n",
            "Epoch:  1\n",
            "262/361.0 loss: 0.7130938386509174 \n",
            "Epoch:  1\n",
            "263/361.0 loss: 0.7125313678248362 \n",
            "Epoch:  1\n",
            "264/361.0 loss: 0.7116933551599395 \n",
            "Epoch:  1\n",
            "265/361.0 loss: 0.7122194147423694 \n",
            "Epoch:  1\n",
            "266/361.0 loss: 0.7123113556970818 \n",
            "Epoch:  1\n",
            "267/361.0 loss: 0.7116121406208223 \n",
            "Epoch:  1\n",
            "268/361.0 loss: 0.7126421193872686 \n",
            "Epoch:  1\n",
            "269/361.0 loss: 0.7119983276835194 \n",
            "Epoch:  1\n",
            "270/361.0 loss: 0.7125782644396779 \n",
            "Epoch:  1\n",
            "271/361.0 loss: 0.7136838220279006 \n",
            "Epoch:  1\n",
            "272/361.0 loss: 0.7131703660819994 \n",
            "Epoch:  1\n",
            "273/361.0 loss: 0.7130531658873941 \n",
            "Epoch:  1\n",
            "274/361.0 loss: 0.7122678473862735 \n",
            "Epoch:  1\n",
            "275/361.0 loss: 0.7125958773321 \n",
            "Epoch:  1\n",
            "276/361.0 loss: 0.7121211082711547 \n",
            "Epoch:  1\n",
            "277/361.0 loss: 0.711637898850784 \n",
            "Epoch:  1\n",
            "278/361.0 loss: 0.7119130245459977 \n",
            "Epoch:  1\n",
            "279/361.0 loss: 0.7124549168561186 \n",
            "Epoch:  1\n",
            "280/361.0 loss: 0.7129528457796022 \n",
            "Epoch:  1\n",
            "281/361.0 loss: 0.7124237226890334 \n",
            "Epoch:  1\n",
            "282/361.0 loss: 0.7119048643238552 \n",
            "Epoch:  1\n",
            "283/361.0 loss: 0.7120037654965696 \n",
            "Epoch:  1\n",
            "284/361.0 loss: 0.712304810578363 \n",
            "Epoch:  1\n",
            "285/361.0 loss: 0.7119624617841694 \n",
            "Epoch:  1\n",
            "286/361.0 loss: 0.7124733371394021 \n",
            "Epoch:  1\n",
            "287/361.0 loss: 0.7126688803028729 \n",
            "Epoch:  1\n",
            "288/361.0 loss: 0.7125275600121508 \n",
            "Epoch:  1\n",
            "289/361.0 loss: 0.7122005241698233 \n",
            "Epoch:  1\n",
            "290/361.0 loss: 0.7126161018802538 \n",
            "Epoch:  1\n",
            "291/361.0 loss: 0.7130517460507889 \n",
            "Epoch:  1\n",
            "292/361.0 loss: 0.7125877421667145 \n",
            "Epoch:  1\n",
            "293/361.0 loss: 0.7120369815704773 \n",
            "Epoch:  1\n",
            "294/361.0 loss: 0.7116491098525161 \n",
            "Epoch:  1\n",
            "295/361.0 loss: 0.7121412771175036 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}