{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS490Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPay9vg6x2ZOGifZzKY35m3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Johoodcoder/CS490Project/blob/hood/Notebooks/CS490ProjectHuggingImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTZbJ1SJ53XO"
      },
      "source": [
        "Non-preinstalled module installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm5_ujD458U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342594b0-cfad-439a-e9ac-9b75ef44016e"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWz664Rd6MeU"
      },
      "source": [
        "Import all the stuff we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbufH4ZX8X5N"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
        "import zipfile\n",
        "\n",
        "CONST_BATCH_SIZE = 32\n",
        "CONST_MAX_SEQ_LENGTH = 128\n",
        "CONST_NUM_EPOCHS = 4\n",
        "CONST_LEARN_RATE = 3e-5\n",
        "CONST_EPSILON = 1e-8\n",
        "CONST_TRAIN_SAMPLES = 3200\n",
        "CONST_TEST_SAMPLES = 800\n",
        "CONST_LABEL = 'label_fnn'\n",
        "CONST_TEXT = 'fullText_based_content'\n",
        "CONST_FILE_TYPE = 'csv'  # 'csv' or 'tsv'\n",
        "CONST_FILE_NAME = 'fnn_train.csv'\n",
        "\n",
        "!unzip fnn_train.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2AUQtRf6SiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49270d1-e428-46df-c315-7d5997834863"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj9SFswG6GWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b999a3df-80d7-4a9d-c0d1-84c520445512"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUA3aAGM7G7u"
      },
      "source": [
        "Load our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a-5pyC08dzO"
      },
      "source": [
        "# Load dataset\n",
        "if CONST_FILE_TYPE == 'csv':\n",
        "  df = pd.read_csv(CONST_FILE_NAME)\n",
        "\n",
        "if CONST_FILE_TYPE == 'tsv':\n",
        "  df = pd.read_csv(CONST_FILE_NAME, sep = '\\t', header = 0)\n",
        "\n",
        "df = df[[CONST_TEXT, CONST_LABEL]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HHnQVRq8z0n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "00038d8c-6c06-4f33-f462-b7aed30951d5"
      },
      "source": [
        "df = df[df[CONST_LABEL].isin(['fake', 'real'])] # Grab only labels we want\n",
        "# Scramble data indexes from dataset. This is producing random scrambles for some reason even with the random_state seed remaining unchanged between runs.\n",
        "# The data is scrambled again down the line but this remains as an artefact from an older version.\n",
        "df = df.dropna()\n",
        "df = df.sample(frac=1, random_state = 23).reset_index(drop=True)\n",
        "print(\"========== TOTAL DATA ==========\")\n",
        "print('Total number of data entries: {:,}\\n'.format(df.shape[0]))\n",
        "print(\"Data balance percentages\")\n",
        "display(df[CONST_LABEL].value_counts(normalize=True))\n",
        "print(\"\\nData balance counts\")\n",
        "display(df[CONST_LABEL].value_counts())\n",
        "print(\"\\n\")\n",
        "display(df.sample(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========== TOTAL DATA ==========\n",
            "Total number of data entries: 15,212\n",
            "\n",
            "Data balance percentages\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "fake    0.500986\n",
              "real    0.499014\n",
              "Name: label_fnn, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Data balance counts\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "fake    7621\n",
              "real    7591\n",
              "Name: label_fnn, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fullText_based_content</th>\n",
              "      <th>label_fnn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2106</th>\n",
              "      <td>Members of Congress recently decamped to their...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10550</th>\n",
              "      <td>The Republican National Committee recently put...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3993</th>\n",
              "      <td>A \"Rick Perry 2016\" image popped up on Faceboo...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>Texas Gov. Rick Perry said in a Republican pre...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3445</th>\n",
              "      <td>Trivia time: How many aircraft carriers does i...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2860</th>\n",
              "      <td>Work on the Republican-authored state budget i...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>589</th>\n",
              "      <td>President Barack Obama’s unilateral move to li...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4695</th>\n",
              "      <td>In a new television advertisement airing March...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11385</th>\n",
              "      <td>An article shared on Facebook suggested that a...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7948</th>\n",
              "      <td>Ben Carson -- currently the top-polling Republ...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  fullText_based_content label_fnn\n",
              "2106   Members of Congress recently decamped to their...      real\n",
              "10550  The Republican National Committee recently put...      real\n",
              "3993   A \"Rick Perry 2016\" image popped up on Faceboo...      real\n",
              "88     Texas Gov. Rick Perry said in a Republican pre...      real\n",
              "3445   Trivia time: How many aircraft carriers does i...      real\n",
              "2860   Work on the Republican-authored state budget i...      fake\n",
              "589    President Barack Obama’s unilateral move to li...      real\n",
              "4695   In a new television advertisement airing March...      fake\n",
              "11385  An article shared on Facebook suggested that a...      fake\n",
              "7948   Ben Carson -- currently the top-polling Republ...      fake"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okSluqzG8-lh"
      },
      "source": [
        "Seperate training and testing subsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wOwpOde8_WC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "7781579a-135c-4b2e-dcb9-1c63caa5b8d8"
      },
      "source": [
        "train_data_df = df.head(CONST_TRAIN_SAMPLES)\n",
        "test_data_df = df.tail(CONST_TEST_SAMPLES)\n",
        "\n",
        "print(\"========== TRAINING DATA ==========\")\n",
        "print('Total number of data entries: {:,}\\n'.format(train_data_df.shape[0]))\n",
        "print(\"Data balance percentages\")\n",
        "display(train_data_df[CONST_LABEL].value_counts(normalize=True))\n",
        "print(\"\\nData balance counts\")\n",
        "display(train_data_df[CONST_LABEL].value_counts())\n",
        "print(\"\\n========== TESTING DATA ==========\")\n",
        "print('Total number of data entries: {:,}\\n'.format(test_data_df.shape[0]))\n",
        "print(\"Data balance percentages\")\n",
        "display(test_data_df[CONST_LABEL].value_counts(normalize=True))\n",
        "print(\"\\nData balance counts\")\n",
        "display(test_data_df[CONST_LABEL].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========== TRAINING DATA ==========\n",
            "Total number of data entries: 3,200\n",
            "\n",
            "Data balance percentages\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "real    0.500938\n",
              "fake    0.499063\n",
              "Name: label_fnn, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Data balance counts\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "real    1603\n",
              "fake    1597\n",
              "Name: label_fnn, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "========== TESTING DATA ==========\n",
            "Total number of data entries: 800\n",
            "\n",
            "Data balance percentages\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "fake    0.515\n",
              "real    0.485\n",
              "Name: label_fnn, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Data balance counts\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "fake    412\n",
              "real    388\n",
              "Name: label_fnn, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_VsapAk87Kk"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "train_texts = train_data_df[CONST_TEXT]\n",
        "train_labels = train_data_df[CONST_LABEL]\n",
        "\n",
        "test_texts = test_data_df[CONST_TEXT]\n",
        "test_labels = test_data_df[CONST_LABEL]\n",
        "\n",
        "# If value == fake then make it 1. Else 0. Funky workaround as bert expects longs\n",
        "train_labels = np.array(train_labels == 'fake')\n",
        "train_labels = np.multiply(train_labels,1)\n",
        "test_labels = np.array(test_labels == 'fake')\n",
        "test_labels = np.multiply(test_labels,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86XhtxYQ94QM"
      },
      "source": [
        "Load and test tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQi8q3Fs9tqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "772a5b93-fe0e-43c6-f735-5c5a3d92c6e1"
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Print the original sentence.\n",
        "print(' Original: ', train_texts[0:1])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(train_texts[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_texts[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n",
            " Original:  0    Democrats and Republicans have drawn the battl...\n",
            "Name: fullText_based_content, dtype: object\n",
            "Tokenized:  ['democrats', 'and', 'republicans', 'have', 'drawn', 'the', 'battle', 'lines', 'for', 'pine', '##llas', 'county', '’', 's', 'open', 'u', '.', 's', '.', 'house', 'seat', ',', 'with', 'candidates', '’', 'backgrounds', 'serving', 'as', 'ammunition', '.', 'while', 'democrats', 'go', 'after', 'david', 'jolly', '’', 's', 'history', 'as', 'a', 'lobby', '##ist', 'in', 'washington', ',', 'd', '.', 'c', '.', ',', 'the', 'go', '##p', 'is', 'attacking', 'alex', 'sink', '’', 's', 'record', 'on', 'issues', 'in', 'florida', '.', 'one', 'of', '##t', '-', 'repeated', 'charge', 'is', 'that', 'sink', 'is', 'a', '\"', 'tax', 'and', 'spend', '##er', 'florida', 'can', '’', 't', 'afford', '.', '\"', 'in', 'a', 'tv', 'commercial', 'released', 'on', 'jan', '.', '22', ',', '2014', ',', 'the', 'national', 'republican', 'congressional', 'committee', 'charges', 'that', 'sink', 'supports', 'obama', '##care', ',', 'then', 'adds', 'charges', 'about', 'her', 'stance', 'on', 'taxes', '.', '\"', 'sink', 'has', 'supported', 'higher', 'taxes', ',', 'too', ',', '\"', 'the', 'ad', 'says', '.', '\"', 'higher', 'property', 'taxes', '.', 'higher', 'sales', 'taxes', '.', 'more', 'taxes', 'on', 'water', 'and', 'tv', 'too', '.', '\"', 'sink', ',', 'a', 'former', 'banking', 'executive', ',', 'only', 'held', 'one', 'elected', 'office', '-', '-', 'florida', '’', 's', 'chief', 'financial', 'officer', 'from', '2007', 'to', '2010', '-', '-', 'so', 'we', 'weren', '’', 't', 'sure', 'whether', 'it', 'was', 'accurate', 'that', 'she', 'supported', 'higher', 'taxes', '\"', 'on', 'water', 'and', 'tv', '.', '\"', 'and', 'since', 'both', 'are', 'important', 'to', 'her', 'prospective', 'constituents', ',', 'we', 'decided', 'to', 'tune', 'in', 'focus', 'on', 'that', 'claim', 'in', 'this', 'item', '.', 'an', 'old', 'problem', 'the', 'nr', '##cc', '’', 's', 'claim', 'stems', 'from', 'sink', '’', 's', 'tenure', 'on', 'the', 'governor', \"'\", 's', 'commission', 'on', 'education', ',', 'back', 'in', '1997', '.', 'the', 'bi', '##partisan', 'group', 'of', 'educators', ',', 'business', 'leaders', 'and', 'law', '##makers', 'was', 'co', '-', 'chaired', 'by', 'democratic', 'lt', '.', 'gov', '.', 'buddy', 'mackay', ',', 'a', 'democrat', ',', 'and', 'jack', 'cr', '##itch', '##field', ',', 'a', 'republican', 'and', 'chairman', 'of', 'florida', 'progress', 'corp', '.', 'sink', ',', 'then', 'president', 'of', 'the', 'florida', 'banking', 'group', 'for', 'nations', '##bank', ',', 'was', 'joined', 'on', 'the', 'board', 'by', 'executives', 'of', 'such', 'companies', 'as', 'walt', 'disney', 'attractions', ',', 'ec', '##ker', '##d', 'corp', '.', 'and', 'cs', '##x', 'transportation', '.', 'the', 'group', ',', 'appointed', 'by', 'democratic', 'gov', '.', 'law', '##ton', 'chile', '##s', 'in', '1996', ',', 'had', 'no', 'legislative', 'power', ',', 'but', 'it', 'did', 'make', 'recommendations', 'about', 'how', 'to', 'pay', 'for', 'school', 'construction', 'at', 'a', 'time', 'when', 'over', '##crow', '##ding', 'was', 'pl', '##ag', '##uing', 'some', 'districts', '.', 'faced', 'with', 'full', 'schools', 'and', 'outdated', 'facilities', ',', 'one', 'topic', 'that', 'drew', 'the', 'board', '’', 's', 'attention', 'was', 'searching', 'for', 'ways', 'to', 'build', 'more', 'classroom', 'space', '.', 'one', 'of', 'the', 'commission', '’', 's', 'potential', 'solutions', 'was', 'unveiled', 'in', 'february', '1997', ':', 'expand', 'a', '2', '.', '5', 'percent', 'utilities', 'tax', 'on', 'the', 'gross', 'receipts', 'of', 'electricity', ',', 'natural', 'and', 'manufactured', 'gas', 'and', 'telecommunication', 'services', 'to', 'include', 'water', ',', 'sewer', ',', 'cable', 'and', 'solid', 'waste', 'utilities', '.', 'the', 'group', 'proposed', 'institut', '##ing', 'the', 'new', 'tax', 'on', 'those', 'utilities', 'in', '0', '.', '5', 'percent', 'inc', '##rem', '##ents', 'over', 'five', 'years', ',', 'which', 'would', 'eventually', 'cost', 'the', 'average', 'household', '$', '24', 'per', 'year', ',', 'and', 'would', 'raise', 'millions', 'for', 'school', 'construction', '.', 'sink', 'was', 'in', 'favor', 'of', 'institut', '##ing', 'the', 'entire', 'tax', 'all', 'at', 'once', ',', 'rather', 'than', 'over', 'the', 'course', 'of', 'five', 'years', '.', 'law', '##makers', 'opposed', 'this', ',', 'especially', 'house', 'republicans', ',', 'who', 'in', '1997', 'held', 'a', 'majority', 'in', 'that', 'wing', 'of', 'the', 'legislature', 'for', 'the', 'first', 'time', 'in', '122', 'years', '.', '\"', 'we', 'ought', 'to', 'say', 'we', 'have', 'a', 'crisis', ',', 'and', 'this', 'is', 'what', 'we', 'really', 'think', 'should', 'happen', ',', '\"', 'sink', 'said', '.', '\"', 'if', 'the', 'politics', 'go', 'the', 'other', 'way', ',', 'so', 'be', 'it', '.', '\"', 'the', 'idea', 'of', 'institut', '##ing', 'the', 'tax', 'all', 'at', 'once', 'failed', 'in', 'a', 'vote', 'by', 'the', 'commissioners', ',', '18', '-', '12', '.', 'the', 'group', 'spent', 'the', 'rest', 'of', 'the', 'year', 'looking', 'for', 'other', 'solutions', ',', 'including', 'a', 'video', 'lottery', 'and', 'a', 'variety', 'of', 'local', 'taxes', '.', 'ultimately', ',', 'all', 'was', 'for', 'na', '##ught', ',', 'as', 'the', 'divided', 'legislature', 'debated', 'new', 'taxes', 'without', 'coming', 'to', 'an', 'agreement', 'during', 'the', 'regular', 'session', '.', 'chile', '##s', 'called', 'a', 'special', 'session', 'in', 'the', 'fall', 'to', 'deal', 'with', 'the', 'school', 'construction', 'issue', ',', 'with', 'law', '##makers', 'eventually', 'agreeing', 'to', 'use', 'florida', 'lottery', 'profits', 'for', '$', '2', '.', '5', 'billion', 'in', 'bonds', ',', 'plus', '$', '200', 'million', 'from', 'surplus', 'revenues', '.', 'that', 'lottery', 'money', 'amounted', 'to', '$', '180', 'million', 'per', 'year', 'over', '30', 'years', '.', 'the', 'deal', 'was', 'considered', 'a', 'victory', 'by', 'both', 'democrats', 'and', 'republicans', ',', 'since', 'it', 'built', 'new', 'schools', 'without', 'relying', 'on', 'new', 'taxes', '.']\n",
            "Token IDs:  [8037, 1998, 10643, 2031, 4567, 1996, 2645, 3210, 2005, 7222, 25816, 2221, 1521, 1055, 2330, 1057, 1012, 1055, 1012, 2160, 2835, 1010, 2007, 5347, 1521, 15406, 3529, 2004, 9290, 1012, 2096, 8037, 2175, 2044, 2585, 22193, 1521, 1055, 2381, 2004, 1037, 9568, 2923, 1999, 2899, 1010, 1040, 1012, 1039, 1012, 1010, 1996, 2175, 2361, 2003, 7866, 4074, 7752, 1521, 1055, 2501, 2006, 3314, 1999, 3516, 1012, 2028, 1997, 2102, 1011, 5567, 3715, 2003, 2008, 7752, 2003, 1037, 1000, 4171, 1998, 5247, 2121, 3516, 2064, 1521, 1056, 8984, 1012, 1000, 1999, 1037, 2694, 3293, 2207, 2006, 5553, 1012, 2570, 1010, 2297, 1010, 1996, 2120, 3951, 7740, 2837, 5571, 2008, 7752, 6753, 8112, 16302, 1010, 2059, 9909, 5571, 2055, 2014, 11032, 2006, 7773, 1012, 1000, 7752, 2038, 3569, 3020, 7773, 1010, 2205, 1010, 1000, 1996, 4748, 2758, 1012, 1000, 3020, 3200, 7773, 1012, 3020, 4341, 7773, 1012, 2062, 7773, 2006, 2300, 1998, 2694, 2205, 1012, 1000, 7752, 1010, 1037, 2280, 8169, 3237, 1010, 2069, 2218, 2028, 2700, 2436, 1011, 1011, 3516, 1521, 1055, 2708, 3361, 2961, 2013, 2289, 2000, 2230, 1011, 1011, 2061, 2057, 4694, 1521, 1056, 2469, 3251, 2009, 2001, 8321, 2008, 2016, 3569, 3020, 7773, 1000, 2006, 2300, 1998, 2694, 1012, 1000, 1998, 2144, 2119, 2024, 2590, 2000, 2014, 17464, 24355, 1010, 2057, 2787, 2000, 8694, 1999, 3579, 2006, 2008, 4366, 1999, 2023, 8875, 1012, 2019, 2214, 3291, 1996, 17212, 9468, 1521, 1055, 4366, 12402, 2013, 7752, 1521, 1055, 7470, 2006, 1996, 3099, 1005, 1055, 3222, 2006, 2495, 1010, 2067, 1999, 2722, 1012, 1996, 12170, 26053, 2177, 1997, 19156, 1010, 2449, 4177, 1998, 2375, 12088, 2001, 2522, 1011, 12282, 2011, 3537, 8318, 1012, 18079, 1012, 8937, 17090, 1010, 1037, 7672, 1010, 1998, 2990, 13675, 20189, 3790, 1010, 1037, 3951, 1998, 3472, 1997, 3516, 5082, 13058, 1012, 7752, 1010, 2059, 2343, 1997, 1996, 3516, 8169, 2177, 2005, 3741, 9299, 1010, 2001, 2587, 2006, 1996, 2604, 2011, 12706, 1997, 2107, 3316, 2004, 10598, 6373, 13051, 1010, 14925, 5484, 2094, 13058, 1012, 1998, 20116, 2595, 5193, 1012, 1996, 2177, 1010, 2805, 2011, 3537, 18079, 1012, 2375, 2669, 7029, 2015, 1999, 2727, 1010, 2018, 2053, 4884, 2373, 1010, 2021, 2009, 2106, 2191, 11433, 2055, 2129, 2000, 3477, 2005, 2082, 2810, 2012, 1037, 2051, 2043, 2058, 24375, 4667, 2001, 20228, 8490, 25165, 2070, 4733, 1012, 4320, 2007, 2440, 2816, 1998, 25963, 4128, 1010, 2028, 8476, 2008, 3881, 1996, 2604, 1521, 1055, 3086, 2001, 6575, 2005, 3971, 2000, 3857, 2062, 9823, 2686, 1012, 2028, 1997, 1996, 3222, 1521, 1055, 4022, 7300, 2001, 11521, 1999, 2337, 2722, 1024, 7818, 1037, 1016, 1012, 1019, 3867, 16548, 4171, 2006, 1996, 7977, 28258, 1997, 6451, 1010, 3019, 1998, 7609, 3806, 1998, 25958, 2578, 2000, 2421, 2300, 1010, 22365, 1010, 5830, 1998, 5024, 5949, 16548, 1012, 1996, 2177, 3818, 17126, 2075, 1996, 2047, 4171, 2006, 2216, 16548, 1999, 1014, 1012, 1019, 3867, 4297, 28578, 11187, 2058, 2274, 2086, 1010, 2029, 2052, 2776, 3465, 1996, 2779, 4398, 1002, 2484, 2566, 2095, 1010, 1998, 2052, 5333, 8817, 2005, 2082, 2810, 1012, 7752, 2001, 1999, 5684, 1997, 17126, 2075, 1996, 2972, 4171, 2035, 2012, 2320, 1010, 2738, 2084, 2058, 1996, 2607, 1997, 2274, 2086, 1012, 2375, 12088, 4941, 2023, 1010, 2926, 2160, 10643, 1010, 2040, 1999, 2722, 2218, 1037, 3484, 1999, 2008, 3358, 1997, 1996, 6372, 2005, 1996, 2034, 2051, 1999, 13092, 2086, 1012, 1000, 2057, 11276, 2000, 2360, 2057, 2031, 1037, 5325, 1010, 1998, 2023, 2003, 2054, 2057, 2428, 2228, 2323, 4148, 1010, 1000, 7752, 2056, 1012, 1000, 2065, 1996, 4331, 2175, 1996, 2060, 2126, 1010, 2061, 2022, 2009, 1012, 1000, 1996, 2801, 1997, 17126, 2075, 1996, 4171, 2035, 2012, 2320, 3478, 1999, 1037, 3789, 2011, 1996, 12396, 1010, 2324, 1011, 2260, 1012, 1996, 2177, 2985, 1996, 2717, 1997, 1996, 2095, 2559, 2005, 2060, 7300, 1010, 2164, 1037, 2678, 15213, 1998, 1037, 3528, 1997, 2334, 7773, 1012, 4821, 1010, 2035, 2001, 2005, 6583, 18533, 1010, 2004, 1996, 4055, 6372, 15268, 2047, 7773, 2302, 2746, 2000, 2019, 3820, 2076, 1996, 3180, 5219, 1012, 7029, 2015, 2170, 1037, 2569, 5219, 1999, 1996, 2991, 2000, 3066, 2007, 1996, 2082, 2810, 3277, 1010, 2007, 2375, 12088, 2776, 16191, 2000, 2224, 3516, 15213, 11372, 2005, 1002, 1016, 1012, 1019, 4551, 1999, 9547, 1010, 4606, 1002, 3263, 2454, 2013, 15726, 12594, 1012, 2008, 15213, 2769, 18779, 2000, 1002, 8380, 2454, 2566, 2095, 2058, 2382, 2086, 1012, 1996, 3066, 2001, 2641, 1037, 3377, 2011, 2119, 8037, 1998, 10643, 1010, 2144, 2009, 2328, 2047, 2816, 2302, 18345, 2006, 2047, 7773, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPRn2Sta-k1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773b0e71-9af1-4499-a76a-fcde779fe6bd"
      },
      "source": [
        "# Report the number of sentences.\n",
        "print('Number of training entries: {:,}\\n'.format(train_data_df.shape[0]))\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for text in train_texts:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = CONST_MAX_SEQ_LENGTH,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(train_labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', train_texts[0:1])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training entries: 3,200\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  0    Democrats and Republicans have drawn the battl...\n",
            "Name: fullText_based_content, dtype: object\n",
            "Token IDs: tensor([  101,  8037,  1998, 10643,  2031,  4567,  1996,  2645,  3210,  2005,\n",
            "         7222, 25816,  2221,  1521,  1055,  2330,  1057,  1012,  1055,  1012,\n",
            "         2160,  2835,  1010,  2007,  5347,  1521, 15406,  3529,  2004,  9290,\n",
            "         1012,  2096,  8037,  2175,  2044,  2585, 22193,  1521,  1055,  2381,\n",
            "         2004,  1037,  9568,  2923,  1999,  2899,  1010,  1040,  1012,  1039,\n",
            "         1012,  1010,  1996,  2175,  2361,  2003,  7866,  4074,  7752,  1521,\n",
            "         1055,  2501,  2006,  3314,  1999,  3516,  1012,  2028,  1997,  2102,\n",
            "         1011,  5567,  3715,  2003,  2008,  7752,  2003,  1037,  1000,  4171,\n",
            "         1998,  5247,  2121,  3516,  2064,  1521,  1056,  8984,  1012,  1000,\n",
            "         1999,  1037,  2694,  3293,  2207,  2006,  5553,  1012,  2570,  1010,\n",
            "         2297,  1010,  1996,  2120,  3951,  7740,  2837,  5571,  2008,  7752,\n",
            "         6753,  8112, 16302,  1010,  2059,  9909,  5571,  2055,  2014, 11032,\n",
            "         2006,  7773,  1012,  1000,  7752,  2038,  3569,   102])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbspJd0B_zWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7942288a-c691-4f31-c2f9-4f2218431c28"
      },
      "source": [
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2,880 training samples\n",
            "  320 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ0x6YgtAFm8"
      },
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = CONST_BATCH_SIZE\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZRYYD-5AOdY"
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iznqE09hAXat"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3lXGKtmAckv"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = CONST_LEARN_RATE, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = CONST_EPSILON # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5QfRjwoAkMc"
      },
      "source": [
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = CONST_NUM_EPOCHS\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWtGyTIKAt-B"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58So89vWA7wk"
      },
      "source": [
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 10 batches.\n",
        "        if step % 10 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        outputs = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            outputs = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of5AwpaONAph"
      },
      "source": [
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drU08e--BJEI"
      },
      "source": [
        "% matplotlib inline\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsQGHV-6NTrg"
      },
      "source": [
        "# Report the number of sentences.\n",
        "print('Number of test entries: {:,}\\n'.format(test_data_df.shape[0]))\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for text in test_texts:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = CONST_MAX_SEQ_LENGTH,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(test_labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = CONST_BATCH_SIZE  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LarLkCN1Qh5E"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = outputs.logits\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAtj6RjqThiP"
      },
      "source": [
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP1PY1UFVwIT"
      },
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrXvz8jhWANB"
      },
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}