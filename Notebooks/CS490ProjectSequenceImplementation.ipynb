{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS490Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNptahyOkjnbRtbWe7ZQARQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Johoodcoder/CS490Project/blob/hood/Notebooks/CS490ProjectSequenceImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTZbJ1SJ53XO"
      },
      "source": [
        "Non-preinstalled module installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm5_ujD458U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a4c1cd-8252-4ee1-f82b-a21ee572afcb"
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.17.33)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.33 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.20.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.6)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.33->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.33->boto3->pytorch-pretrained-bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbufH4ZX8X5N"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
        "import torch\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a-5pyC08dzO",
        "outputId": "8c427615-bdb0-41b2-8fd6-aeae9f4721d2"
      },
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"condensed_fake_real_news.csv\")\n",
        "df = df[['text', 'type']]\n",
        "print(len(df))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HHnQVRq8z0n",
        "outputId": "66d37120-db90-44e3-8be3-87e4b01fb18d"
      },
      "source": [
        "df = df[df['type'].isin(['fake', 'real'])]\n",
        "# Scramble data indexes from dataset. Random_state is a seed.\n",
        "df = df.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "\n",
        "print(Counter(df['type'].values))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'fake': 4000, 'real': 4000})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wOwpOde8_WC",
        "outputId": "1010ae1e-b737-4ff4-d018-792db5c0693f"
      },
      "source": [
        "train_data_df = df.head(640)\n",
        "test_data_df = df.tail(160)\n",
        "print(train_data_df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  text  type\n",
            "0    Donald Trump s most recent secretive actions t...  fake\n",
            "1    WASHINGTON (Reuters) - U.S. President Donald T...  real\n",
            "2    WASHINGTON (Reuters) - U.S. President Donald T...  real\n",
            "3    WASHINGTON (Reuters) - Congressional leaders a...  real\n",
            "4    One of Trump s biggest campaign promises was t...  fake\n",
            "..                                                 ...   ...\n",
            "635  President Barack Obama gave an amazing farewel...  fake\n",
            "636  When Donald Trump kicked off  Made in America ...  fake\n",
            "637  With Donald Trump winning the election, albeit...  fake\n",
            "638  WASHINGTON (Reuters) - The U.S. House of Repre...  real\n",
            "639  (Reuters) - The Republican Party will resume f...  real\n",
            "\n",
            "[640 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bborPzYM9CUR"
      },
      "source": [
        "train_data = []\n",
        "for index, row in train_data_df.iterrows():\n",
        "    train_data.append({'text': row['text'], 'type': row['type']})\n",
        "\n",
        "test_data = []\n",
        "for index, row in test_data_df.iterrows():\n",
        "    test_data.append({'text': row['text'], 'type': row['type']})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G7zoLaF9gNf"
      },
      "source": [
        "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['type']), train_data)))\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['type']), test_data)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KNrngW99ooH"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\n",
        "\n",
        "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
        "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
        "\n",
        "\n",
        "\n",
        "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAKhviVo9ujZ"
      },
      "source": [
        "# If value == fake then make it true. Otherwise false.\n",
        "train_y = np.array(train_labels) == 'fake'\n",
        "test_y = np.array(test_labels) == 'fake'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjP7PqON92FH"
      },
      "source": [
        "# Input masks differentiate padding tokens from legitimate data token. 1 == data, 0 == padding\n",
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1poQHOBe-DM6"
      },
      "source": [
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "# Testing datatypes for speed and compatability\n",
        "# ----------------------------------------- CUDA -----------------------------------------------------\n",
        "# train_masks_tensor = train_masks_tensor.to('cuda')\n",
        "# test_masks_tensor = test_masks_tensor.to('cuda')\n",
        "\n",
        "# train_tokens_tensor = train_tokens_tensor.to('cuda')\n",
        "# test_tokens_tensor = test_tokens_tensor.to('cuda')\n",
        "\n",
        "train_y_tensor = train_y_tensor.to('cuda')\n",
        "test_y_tensor = test_y_tensor.to('cuda')\n",
        "\n",
        "# ----------------------------------------- CUDA LONG -----------------------------------------------------\n",
        "cuda = torch.device('cuda')\n",
        "train_masks_tensor = train_masks_tensor.to(cuda, dtype = torch.long)\n",
        "test_masks_tensor = test_masks_tensor.to(cuda, dtype = torch.long)\n",
        "\n",
        "train_tokens_tensor = train_tokens_tensor.to(cuda, dtype = torch.long)\n",
        "test_tokens_tensor = test_tokens_tensor.to(cuda, dtype = torch.long)\n",
        "\n",
        "# train_y_tensor = train_y_tensor.to(cuda, dtype = torch.long)\n",
        "# test_y_tensor = test_y_tensor.to(cuda, dtype = torch.long)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kHKfGwF-DZp"
      },
      "source": [
        "BATCH_SIZE = 5\n",
        "EPOCHS = 5\n",
        "\n",
        "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
        "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
        "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "num_labels = 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kdg4sqN-DkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bfaf451-c284-4452-89c1-294dd3b35485"
      },
      "source": [
        "bert_clf = BertForSequenceClassification(config, num_labels)\n",
        "bert_clf.to('cuda')\n",
        "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
        "\n",
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "        probas = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        # The new model has slightly different outputs. A sigmoid() is applied to probas to bound between 0 and 1\n",
        "        batch_loss = loss_func(probas.sigmoid(), labels)\n",
        "        train_loss += batch_loss.item()\n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "0/128.0 loss: 0.7291291356086731 \n",
            "Epoch:  1\n",
            "1/128.0 loss: 0.7233211696147919 \n",
            "Epoch:  1\n",
            "2/128.0 loss: 0.6980193853378296 \n",
            "Epoch:  1\n",
            "3/128.0 loss: 0.6823318302631378 \n",
            "Epoch:  1\n",
            "4/128.0 loss: 0.6543238162994385 \n",
            "Epoch:  1\n",
            "5/128.0 loss: 0.6595535178979238 \n",
            "Epoch:  1\n",
            "6/128.0 loss: 0.6220407826559884 \n",
            "Epoch:  1\n",
            "7/128.0 loss: 0.6074939891695976 \n",
            "Epoch:  1\n",
            "8/128.0 loss: 0.6184754305415683 \n",
            "Epoch:  1\n",
            "9/128.0 loss: 0.6709721386432648 \n",
            "Epoch:  1\n",
            "10/128.0 loss: 0.6979695937850259 \n",
            "Epoch:  1\n",
            "11/128.0 loss: 0.746024951338768 \n",
            "Epoch:  1\n",
            "12/128.0 loss: 0.725562008527609 \n",
            "Epoch:  1\n",
            "13/128.0 loss: 0.7131225935050419 \n",
            "Epoch:  1\n",
            "14/128.0 loss: 0.720375406742096 \n",
            "Epoch:  1\n",
            "15/128.0 loss: 0.7148989476263523 \n",
            "Epoch:  1\n",
            "16/128.0 loss: 0.7161467531148125 \n",
            "Epoch:  1\n",
            "17/128.0 loss: 0.7174962527222104 \n",
            "Epoch:  1\n",
            "18/128.0 loss: 0.7163020309649015 \n",
            "Epoch:  1\n",
            "19/128.0 loss: 0.7114687711000443 \n",
            "Epoch:  1\n",
            "20/128.0 loss: 0.7069394418171474 \n",
            "Epoch:  1\n",
            "21/128.0 loss: 0.7030957991426642 \n",
            "Epoch:  1\n",
            "22/128.0 loss: 0.6963521319886913 \n",
            "Epoch:  1\n",
            "23/128.0 loss: 0.6954650903741518 \n",
            "Epoch:  1\n",
            "24/128.0 loss: 0.691684422492981 \n",
            "Epoch:  1\n",
            "25/128.0 loss: 0.6931319053356464 \n",
            "Epoch:  1\n",
            "26/128.0 loss: 0.6924707271434642 \n",
            "Epoch:  1\n",
            "27/128.0 loss: 0.694516971707344 \n",
            "Epoch:  1\n",
            "28/128.0 loss: 0.6897128142159561 \n",
            "Epoch:  1\n",
            "29/128.0 loss: 0.6812728136777878 \n",
            "Epoch:  1\n",
            "30/128.0 loss: 0.6782914871169675 \n",
            "Epoch:  1\n",
            "31/128.0 loss: 0.678680238313973 \n",
            "Epoch:  1\n",
            "32/128.0 loss: 0.6784153372952433 \n",
            "Epoch:  1\n",
            "33/128.0 loss: 0.6770366070901647 \n",
            "Epoch:  1\n",
            "34/128.0 loss: 0.6768246148313795 \n",
            "Epoch:  1\n",
            "35/128.0 loss: 0.6762136295437813 \n",
            "Epoch:  1\n",
            "36/128.0 loss: 0.6771645988966968 \n",
            "Epoch:  1\n",
            "37/128.0 loss: 0.6708373576402664 \n",
            "Epoch:  1\n",
            "38/128.0 loss: 0.6673445525841836 \n",
            "Epoch:  1\n",
            "39/128.0 loss: 0.6632514767348766 \n",
            "Epoch:  1\n",
            "40/128.0 loss: 0.659630066737896 \n",
            "Epoch:  1\n",
            "41/128.0 loss: 0.6583658123300189 \n",
            "Epoch:  1\n",
            "42/128.0 loss: 0.655115136573481 \n",
            "Epoch:  1\n",
            "43/128.0 loss: 0.6508576978336681 \n",
            "Epoch:  1\n",
            "44/128.0 loss: 0.6518263604905871 \n",
            "Epoch:  1\n",
            "45/128.0 loss: 0.6493307831494705 \n",
            "Epoch:  1\n",
            "46/128.0 loss: 0.6487218319101536 \n",
            "Epoch:  1\n",
            "47/128.0 loss: 0.648359856257836 \n",
            "Epoch:  1\n",
            "48/128.0 loss: 0.6485695936241929 \n",
            "Epoch:  1\n",
            "49/128.0 loss: 0.6471587538719177 \n",
            "Epoch:  1\n",
            "50/128.0 loss: 0.6487658713378158 \n",
            "Epoch:  1\n",
            "51/128.0 loss: 0.6478609075913062 \n",
            "Epoch:  1\n",
            "52/128.0 loss: 0.6478471519812098 \n",
            "Epoch:  1\n",
            "53/128.0 loss: 0.6519725874618247 \n",
            "Epoch:  1\n",
            "54/128.0 loss: 0.6502673853527415 \n",
            "Epoch:  1\n",
            "55/128.0 loss: 0.6496740824409893 \n",
            "Epoch:  1\n",
            "56/128.0 loss: 0.6501774631048504 \n",
            "Epoch:  1\n",
            "57/128.0 loss: 0.6489036617607906 \n",
            "Epoch:  1\n",
            "58/128.0 loss: 0.6507814556865369 \n",
            "Epoch:  1\n",
            "59/128.0 loss: 0.6525208065907161 \n",
            "Epoch:  1\n",
            "60/128.0 loss: 0.6557830708925841 \n",
            "Epoch:  1\n",
            "61/128.0 loss: 0.6595495093253351 \n",
            "Epoch:  1\n",
            "62/128.0 loss: 0.6605760284832546 \n",
            "Epoch:  1\n",
            "63/128.0 loss: 0.6574531854130328 \n",
            "Epoch:  1\n",
            "64/128.0 loss: 0.655657952106916 \n",
            "Epoch:  1\n",
            "65/128.0 loss: 0.6547452974500079 \n",
            "Epoch:  1\n",
            "66/128.0 loss: 0.6529032921613153 \n",
            "Epoch:  1\n",
            "67/128.0 loss: 0.6526864140349276 \n",
            "Epoch:  1\n",
            "68/128.0 loss: 0.6516430140405461 \n",
            "Epoch:  1\n",
            "69/128.0 loss: 0.6492450501237597 \n",
            "Epoch:  1\n",
            "70/128.0 loss: 0.649320620886037 \n",
            "Epoch:  1\n",
            "71/128.0 loss: 0.6469447153309981 \n",
            "Epoch:  1\n",
            "72/128.0 loss: 0.6458833997380243 \n",
            "Epoch:  1\n",
            "73/128.0 loss: 0.6430232142274445 \n",
            "Epoch:  1\n",
            "74/128.0 loss: 0.6423798787593842 \n",
            "Epoch:  1\n",
            "75/128.0 loss: 0.6373476115496535 \n",
            "Epoch:  1\n",
            "76/128.0 loss: 0.6382397556459749 \n",
            "Epoch:  1\n",
            "77/128.0 loss: 0.638375019797912 \n",
            "Epoch:  1\n",
            "78/128.0 loss: 0.6433609954163998 \n",
            "Epoch:  1\n",
            "79/128.0 loss: 0.640748293697834 \n",
            "Epoch:  1\n",
            "80/128.0 loss: 0.6398047687094889 \n",
            "Epoch:  1\n",
            "81/128.0 loss: 0.639628595695263 \n",
            "Epoch:  1\n",
            "82/128.0 loss: 0.6396051192858133 \n",
            "Epoch:  1\n",
            "83/128.0 loss: 0.6396684582744326 \n",
            "Epoch:  1\n",
            "84/128.0 loss: 0.639620944331674 \n",
            "Epoch:  1\n",
            "85/128.0 loss: 0.6378255202326664 \n",
            "Epoch:  1\n",
            "86/128.0 loss: 0.6375795212285272 \n",
            "Epoch:  1\n",
            "87/128.0 loss: 0.638669683851979 \n",
            "Epoch:  1\n",
            "88/128.0 loss: 0.6394806015357543 \n",
            "Epoch:  1\n",
            "89/128.0 loss: 0.6396449877156152 \n",
            "Epoch:  1\n",
            "90/128.0 loss: 0.6380625180490724 \n",
            "Epoch:  1\n",
            "91/128.0 loss: 0.6381262790249742 \n",
            "Epoch:  1\n",
            "92/128.0 loss: 0.6372773419785244 \n",
            "Epoch:  1\n",
            "93/128.0 loss: 0.6359594594290916 \n",
            "Epoch:  1\n",
            "94/128.0 loss: 0.6341697266227321 \n",
            "Epoch:  1\n",
            "95/128.0 loss: 0.6321651445080837 \n",
            "Epoch:  1\n",
            "96/128.0 loss: 0.6313083853918252 \n",
            "Epoch:  1\n",
            "97/128.0 loss: 0.6341319972155045 \n",
            "Epoch:  1\n",
            "98/128.0 loss: 0.6350067805762243 \n",
            "Epoch:  1\n",
            "99/128.0 loss: 0.633571230173111 \n",
            "Epoch:  1\n",
            "100/128.0 loss: 0.6374505510424623 \n",
            "Epoch:  1\n",
            "101/128.0 loss: 0.6377494528013117 \n",
            "Epoch:  1\n",
            "102/128.0 loss: 0.6397896015528336 \n",
            "Epoch:  1\n",
            "103/128.0 loss: 0.638463740165417 \n",
            "Epoch:  1\n",
            "104/128.0 loss: 0.6373022164617266 \n",
            "Epoch:  1\n",
            "105/128.0 loss: 0.6370388674286177 \n",
            "Epoch:  1\n",
            "106/128.0 loss: 0.63622130690334 \n",
            "Epoch:  1\n",
            "107/128.0 loss: 0.6348706127868758 \n",
            "Epoch:  1\n",
            "108/128.0 loss: 0.6337317788819654 \n",
            "Epoch:  1\n",
            "109/128.0 loss: 0.6318841668692502 \n",
            "Epoch:  1\n",
            "110/128.0 loss: 0.6353513480306746 \n",
            "Epoch:  1\n",
            "111/128.0 loss: 0.6341669538191387 \n",
            "Epoch:  1\n",
            "112/128.0 loss: 0.6321645183900816 \n",
            "Epoch:  1\n",
            "113/128.0 loss: 0.6322895968169496 \n",
            "Epoch:  1\n",
            "114/128.0 loss: 0.6319660938304403 \n",
            "Epoch:  1\n",
            "115/128.0 loss: 0.6306633083470936 \n",
            "Epoch:  1\n",
            "116/128.0 loss: 0.6330464836369213 \n",
            "Epoch:  1\n",
            "117/128.0 loss: 0.6314493882454048 \n",
            "Epoch:  1\n",
            "118/128.0 loss: 0.6304449809699499 \n",
            "Epoch:  1\n",
            "119/128.0 loss: 0.6295481656988462 \n",
            "Epoch:  1\n",
            "120/128.0 loss: 0.6280494704226817 \n",
            "Epoch:  1\n",
            "121/128.0 loss: 0.6283285615385555 \n",
            "Epoch:  1\n",
            "122/128.0 loss: 0.6283495404371401 \n",
            "Epoch:  1\n",
            "123/128.0 loss: 0.6302800493374947 \n",
            "Epoch:  1\n",
            "124/128.0 loss: 0.630837810754776 \n",
            "Epoch:  1\n",
            "125/128.0 loss: 0.6336460215231728 \n",
            "Epoch:  1\n",
            "126/128.0 loss: 0.6329039933643942 \n",
            "Epoch:  1\n",
            "127/128.0 loss: 0.6310500078834593 \n",
            "Epoch:  2\n",
            "0/128.0 loss: 0.6592898368835449 \n",
            "Epoch:  2\n",
            "1/128.0 loss: 0.7224398553371429 \n",
            "Epoch:  2\n",
            "2/128.0 loss: 0.6457220713297526 \n",
            "Epoch:  2\n",
            "3/128.0 loss: 0.6141297668218613 \n",
            "Epoch:  2\n",
            "4/128.0 loss: 0.5826282382011414 \n",
            "Epoch:  2\n",
            "5/128.0 loss: 0.5478787124156952 \n",
            "Epoch:  2\n",
            "6/128.0 loss: 0.5519167355128697 \n",
            "Epoch:  2\n",
            "7/128.0 loss: 0.562093511223793 \n",
            "Epoch:  2\n",
            "8/128.0 loss: 0.5610801643795438 \n",
            "Epoch:  2\n",
            "9/128.0 loss: 0.5612929105758667 \n",
            "Epoch:  2\n",
            "10/128.0 loss: 0.5639249086380005 \n",
            "Epoch:  2\n",
            "11/128.0 loss: 0.5837709258000056 \n",
            "Epoch:  2\n",
            "12/128.0 loss: 0.5814215861834012 \n",
            "Epoch:  2\n",
            "13/128.0 loss: 0.5823145466191428 \n",
            "Epoch:  2\n",
            "14/128.0 loss: 0.579564094543457 \n",
            "Epoch:  2\n",
            "15/128.0 loss: 0.5723177623003721 \n",
            "Epoch:  2\n",
            "16/128.0 loss: 0.5754087900414186 \n",
            "Epoch:  2\n",
            "17/128.0 loss: 0.5834078441063563 \n",
            "Epoch:  2\n",
            "18/128.0 loss: 0.5835321216206801 \n",
            "Epoch:  2\n",
            "19/128.0 loss: 0.5829335674643517 \n",
            "Epoch:  2\n",
            "20/128.0 loss: 0.5897922586827051 \n",
            "Epoch:  2\n",
            "21/128.0 loss: 0.5955155478282408 \n",
            "Epoch:  2\n",
            "22/128.0 loss: 0.5895503642766372 \n",
            "Epoch:  2\n",
            "23/128.0 loss: 0.5965944789350033 \n",
            "Epoch:  2\n",
            "24/128.0 loss: 0.5956499350070953 \n",
            "Epoch:  2\n",
            "25/128.0 loss: 0.5966185182332993 \n",
            "Epoch:  2\n",
            "26/128.0 loss: 0.587530619568295 \n",
            "Epoch:  2\n",
            "27/128.0 loss: 0.5917681413037437 \n",
            "Epoch:  2\n",
            "28/128.0 loss: 0.5877498213587136 \n",
            "Epoch:  2\n",
            "29/128.0 loss: 0.5785578449567159 \n",
            "Epoch:  2\n",
            "30/128.0 loss: 0.5806361494525787 \n",
            "Epoch:  2\n",
            "31/128.0 loss: 0.5823222119361162 \n",
            "Epoch:  2\n",
            "32/128.0 loss: 0.5813278790676233 \n",
            "Epoch:  2\n",
            "33/128.0 loss: 0.5801641204777885 \n",
            "Epoch:  2\n",
            "34/128.0 loss: 0.5830154401915414 \n",
            "Epoch:  2\n",
            "35/128.0 loss: 0.580107583767838 \n",
            "Epoch:  2\n",
            "36/128.0 loss: 0.585145461398202 \n",
            "Epoch:  2\n",
            "37/128.0 loss: 0.5851996078302986 \n",
            "Epoch:  2\n",
            "38/128.0 loss: 0.577833218452258 \n",
            "Epoch:  2\n",
            "39/128.0 loss: 0.5870431229472161 \n",
            "Epoch:  2\n",
            "40/128.0 loss: 0.5892249825524121 \n",
            "Epoch:  2\n",
            "41/128.0 loss: 0.5878390726589021 \n",
            "Epoch:  2\n",
            "42/128.0 loss: 0.5871440795964973 \n",
            "Epoch:  2\n",
            "43/128.0 loss: 0.5866490358656103 \n",
            "Epoch:  2\n",
            "44/128.0 loss: 0.5792286740409003 \n",
            "Epoch:  2\n",
            "45/128.0 loss: 0.5745968715004299 \n",
            "Epoch:  2\n",
            "46/128.0 loss: 0.5773459457336588 \n",
            "Epoch:  2\n",
            "47/128.0 loss: 0.5750851109623909 \n",
            "Epoch:  2\n",
            "48/128.0 loss: 0.584495700135523 \n",
            "Epoch:  2\n",
            "49/128.0 loss: 0.5879795897006989 \n",
            "Epoch:  2\n",
            "50/128.0 loss: 0.5922271457372927 \n",
            "Epoch:  2\n",
            "51/128.0 loss: 0.592140250481092 \n",
            "Epoch:  2\n",
            "52/128.0 loss: 0.5935423340437547 \n",
            "Epoch:  2\n",
            "53/128.0 loss: 0.5920352207289802 \n",
            "Epoch:  2\n",
            "54/128.0 loss: 0.5895324869589372 \n",
            "Epoch:  2\n",
            "55/128.0 loss: 0.5856117930795465 \n",
            "Epoch:  2\n",
            "56/128.0 loss: 0.5853785124787113 \n",
            "Epoch:  2\n",
            "57/128.0 loss: 0.5906476465792492 \n",
            "Epoch:  2\n",
            "58/128.0 loss: 0.594592067649809 \n",
            "Epoch:  2\n",
            "59/128.0 loss: 0.5952298109730084 \n",
            "Epoch:  2\n",
            "60/128.0 loss: 0.5951589888236561 \n",
            "Epoch:  2\n",
            "61/128.0 loss: 0.5935176485969175 \n",
            "Epoch:  2\n",
            "62/128.0 loss: 0.5934707238560631 \n",
            "Epoch:  2\n",
            "63/128.0 loss: 0.5951855145394802 \n",
            "Epoch:  2\n",
            "64/128.0 loss: 0.5946721141154949 \n",
            "Epoch:  2\n",
            "65/128.0 loss: 0.595213869304368 \n",
            "Epoch:  2\n",
            "66/128.0 loss: 0.5948546893561064 \n",
            "Epoch:  2\n",
            "67/128.0 loss: 0.5958173967459622 \n",
            "Epoch:  2\n",
            "68/128.0 loss: 0.5964932735415472 \n",
            "Epoch:  2\n",
            "69/128.0 loss: 0.5984427085944585 \n",
            "Epoch:  2\n",
            "70/128.0 loss: 0.5991996501533079 \n",
            "Epoch:  2\n",
            "71/128.0 loss: 0.5958284640477763 \n",
            "Epoch:  2\n",
            "72/128.0 loss: 0.593550977233338 \n",
            "Epoch:  2\n",
            "73/128.0 loss: 0.5939014896347716 \n",
            "Epoch:  2\n",
            "74/128.0 loss: 0.593909562031428 \n",
            "Epoch:  2\n",
            "75/128.0 loss: 0.5949210010861096 \n",
            "Epoch:  2\n",
            "76/128.0 loss: 0.5960903798604941 \n",
            "Epoch:  2\n",
            "77/128.0 loss: 0.59741954658276 \n",
            "Epoch:  2\n",
            "78/128.0 loss: 0.5949823992161811 \n",
            "Epoch:  2\n",
            "79/128.0 loss: 0.5910989504307509 \n",
            "Epoch:  2\n",
            "80/128.0 loss: 0.5955942298895047 \n",
            "Epoch:  2\n",
            "81/128.0 loss: 0.5959835775741716 \n",
            "Epoch:  2\n",
            "82/128.0 loss: 0.594696489802326 \n",
            "Epoch:  2\n",
            "83/128.0 loss: 0.5936977490782738 \n",
            "Epoch:  2\n",
            "84/128.0 loss: 0.5903958005063674 \n",
            "Epoch:  2\n",
            "85/128.0 loss: 0.5950713414092397 \n",
            "Epoch:  2\n",
            "86/128.0 loss: 0.5943159618596922 \n",
            "Epoch:  2\n",
            "87/128.0 loss: 0.5902447541328993 \n",
            "Epoch:  2\n",
            "88/128.0 loss: 0.5895117933160803 \n",
            "Epoch:  2\n",
            "89/128.0 loss: 0.5947746061616473 \n",
            "Epoch:  2\n",
            "90/128.0 loss: 0.6008445283213815 \n",
            "Epoch:  2\n",
            "91/128.0 loss: 0.6022567389451939 \n",
            "Epoch:  2\n",
            "92/128.0 loss: 0.6034794587601897 \n",
            "Epoch:  2\n",
            "93/128.0 loss: 0.6032148907159237 \n",
            "Epoch:  2\n",
            "94/128.0 loss: 0.6035018547585136 \n",
            "Epoch:  2\n",
            "95/128.0 loss: 0.6026209254438678 \n",
            "Epoch:  2\n",
            "96/128.0 loss: 0.601592761646841 \n",
            "Epoch:  2\n",
            "97/128.0 loss: 0.6015117737103481 \n",
            "Epoch:  2\n",
            "98/128.0 loss: 0.6026728318797218 \n",
            "Epoch:  2\n",
            "99/128.0 loss: 0.602316624224186 \n",
            "Epoch:  2\n",
            "100/128.0 loss: 0.6019719623693145 \n",
            "Epoch:  2\n",
            "101/128.0 loss: 0.600780491735421 \n",
            "Epoch:  2\n",
            "102/128.0 loss: 0.5980476971390178 \n",
            "Epoch:  2\n",
            "103/128.0 loss: 0.5956463931271663 \n",
            "Epoch:  2\n",
            "104/128.0 loss: 0.5931854055041359 \n",
            "Epoch:  2\n",
            "105/128.0 loss: 0.5945226820010059 \n",
            "Epoch:  2\n",
            "106/128.0 loss: 0.5960891714719968 \n",
            "Epoch:  2\n",
            "107/128.0 loss: 0.5966308884046696 \n",
            "Epoch:  2\n",
            "108/128.0 loss: 0.5946596603874766 \n",
            "Epoch:  2\n",
            "109/128.0 loss: 0.595542848110199 \n",
            "Epoch:  2\n",
            "110/128.0 loss: 0.5941894567227578 \n",
            "Epoch:  2\n",
            "111/128.0 loss: 0.5965736947421517 \n",
            "Epoch:  2\n",
            "112/128.0 loss: 0.5963638533005672 \n",
            "Epoch:  2\n",
            "113/128.0 loss: 0.5960883903398848 \n",
            "Epoch:  2\n",
            "114/128.0 loss: 0.5964002171288366 \n",
            "Epoch:  2\n",
            "115/128.0 loss: 0.5955052707215835 \n",
            "Epoch:  2\n",
            "116/128.0 loss: 0.5943238898220226 \n",
            "Epoch:  2\n",
            "117/128.0 loss: 0.5928471525341777 \n",
            "Epoch:  2\n",
            "118/128.0 loss: 0.5919805907902598 \n",
            "Epoch:  2\n",
            "119/128.0 loss: 0.5924181091288726 \n",
            "Epoch:  2\n",
            "120/128.0 loss: 0.5927389813356163 \n",
            "Epoch:  2\n",
            "121/128.0 loss: 0.5910106623759035 \n",
            "Epoch:  2\n",
            "122/128.0 loss: 0.5918728995129345 \n",
            "Epoch:  2\n",
            "123/128.0 loss: 0.5923490658883126 \n",
            "Epoch:  2\n",
            "124/128.0 loss: 0.5916787486076355 \n",
            "Epoch:  2\n",
            "125/128.0 loss: 0.5915999256429219 \n",
            "Epoch:  2\n",
            "126/128.0 loss: 0.5917646983477074 \n",
            "Epoch:  2\n",
            "127/128.0 loss: 0.5917282248847187 \n",
            "Epoch:  3\n",
            "0/128.0 loss: 0.6447694301605225 \n",
            "Epoch:  3\n",
            "1/128.0 loss: 0.6011402010917664 \n",
            "Epoch:  3\n",
            "2/128.0 loss: 0.6612276434898376 \n",
            "Epoch:  3\n",
            "3/128.0 loss: 0.6716446876525879 \n",
            "Epoch:  3\n",
            "4/128.0 loss: 0.6738311052322388 \n",
            "Epoch:  3\n",
            "5/128.0 loss: 0.6186393896738688 \n",
            "Epoch:  3\n",
            "6/128.0 loss: 0.5655375825507301 \n",
            "Epoch:  3\n",
            "7/128.0 loss: 0.5385016109794378 \n",
            "Epoch:  3\n",
            "8/128.0 loss: 0.5424731423457464 \n",
            "Epoch:  3\n",
            "9/128.0 loss: 0.5446684643626213 \n",
            "Epoch:  3\n",
            "10/128.0 loss: 0.5440003966743295 \n",
            "Epoch:  3\n",
            "11/128.0 loss: 0.5561624107261499 \n",
            "Epoch:  3\n",
            "12/128.0 loss: 0.5452701254532888 \n",
            "Epoch:  3\n",
            "13/128.0 loss: 0.5485325870769364 \n",
            "Epoch:  3\n",
            "14/128.0 loss: 0.5465665688117345 \n",
            "Epoch:  3\n",
            "15/128.0 loss: 0.5476170545443892 \n",
            "Epoch:  3\n",
            "16/128.0 loss: 0.5473867470727247 \n",
            "Epoch:  3\n",
            "17/128.0 loss: 0.5551976453926828 \n",
            "Epoch:  3\n",
            "18/128.0 loss: 0.5468191794658962 \n",
            "Epoch:  3\n",
            "19/128.0 loss: 0.5428423516452312 \n",
            "Epoch:  3\n",
            "20/128.0 loss: 0.5599162088973182 \n",
            "Epoch:  3\n",
            "21/128.0 loss: 0.5618776455521584 \n",
            "Epoch:  3\n",
            "22/128.0 loss: 0.5620009244784064 \n",
            "Epoch:  3\n",
            "23/128.0 loss: 0.573629068210721 \n",
            "Epoch:  3\n",
            "24/128.0 loss: 0.5760461109876632 \n",
            "Epoch:  3\n",
            "25/128.0 loss: 0.5734382999631075 \n",
            "Epoch:  3\n",
            "26/128.0 loss: 0.5754298407722402 \n",
            "Epoch:  3\n",
            "27/128.0 loss: 0.5711400088454995 \n",
            "Epoch:  3\n",
            "28/128.0 loss: 0.5703831628478807 \n",
            "Epoch:  3\n",
            "29/128.0 loss: 0.5740464970469474 \n",
            "Epoch:  3\n",
            "30/128.0 loss: 0.5743467639530858 \n",
            "Epoch:  3\n",
            "31/128.0 loss: 0.5785748553462327 \n",
            "Epoch:  3\n",
            "32/128.0 loss: 0.5821291411464865 \n",
            "Epoch:  3\n",
            "33/128.0 loss: 0.5789957129779983 \n",
            "Epoch:  3\n",
            "34/128.0 loss: 0.5794570118188858 \n",
            "Epoch:  3\n",
            "35/128.0 loss: 0.580905625803603 \n",
            "Epoch:  3\n",
            "36/128.0 loss: 0.5834013203511367 \n",
            "Epoch:  3\n",
            "37/128.0 loss: 0.5811494341806361 \n",
            "Epoch:  3\n",
            "38/128.0 loss: 0.5843770874616427 \n",
            "Epoch:  3\n",
            "39/128.0 loss: 0.5862589437514544 \n",
            "Epoch:  3\n",
            "40/128.0 loss: 0.5832115400855135 \n",
            "Epoch:  3\n",
            "41/128.0 loss: 0.5795195737764949 \n",
            "Epoch:  3\n",
            "42/128.0 loss: 0.5774519058854081 \n",
            "Epoch:  3\n",
            "43/128.0 loss: 0.573796178468249 \n",
            "Epoch:  3\n",
            "44/128.0 loss: 0.577901139193111 \n",
            "Epoch:  3\n",
            "45/128.0 loss: 0.5726228748326716 \n",
            "Epoch:  3\n",
            "46/128.0 loss: 0.573508916066048 \n",
            "Epoch:  3\n",
            "47/128.0 loss: 0.5674040556574861 \n",
            "Epoch:  3\n",
            "48/128.0 loss: 0.5693740093586396 \n",
            "Epoch:  3\n",
            "49/128.0 loss: 0.5676580139994621 \n",
            "Epoch:  3\n",
            "50/128.0 loss: 0.5696061016882167 \n",
            "Epoch:  3\n",
            "51/128.0 loss: 0.5644066073000431 \n",
            "Epoch:  3\n",
            "52/128.0 loss: 0.5661982784293732 \n",
            "Epoch:  3\n",
            "53/128.0 loss: 0.5703720949866153 \n",
            "Epoch:  3\n",
            "54/128.0 loss: 0.5733398245139556 \n",
            "Epoch:  3\n",
            "55/128.0 loss: 0.5757820928203208 \n",
            "Epoch:  3\n",
            "56/128.0 loss: 0.57706476434281 \n",
            "Epoch:  3\n",
            "57/128.0 loss: 0.5762183052198641 \n",
            "Epoch:  3\n",
            "58/128.0 loss: 0.5769767531398999 \n",
            "Epoch:  3\n",
            "59/128.0 loss: 0.5775346957147122 \n",
            "Epoch:  3\n",
            "60/128.0 loss: 0.5768008537468363 \n",
            "Epoch:  3\n",
            "61/128.0 loss: 0.5791633237273462 \n",
            "Epoch:  3\n",
            "62/128.0 loss: 0.5800712432653184 \n",
            "Epoch:  3\n",
            "63/128.0 loss: 0.5817166345659643 \n",
            "Epoch:  3\n",
            "64/128.0 loss: 0.5811555525431267 \n",
            "Epoch:  3\n",
            "65/128.0 loss: 0.5811414874412797 \n",
            "Epoch:  3\n",
            "66/128.0 loss: 0.5778772517371533 \n",
            "Epoch:  3\n",
            "67/128.0 loss: 0.5752720208290745 \n",
            "Epoch:  3\n",
            "68/128.0 loss: 0.5752257357041041 \n",
            "Epoch:  3\n",
            "69/128.0 loss: 0.5750544801354408 \n",
            "Epoch:  3\n",
            "70/128.0 loss: 0.5765028744935989 \n",
            "Epoch:  3\n",
            "71/128.0 loss: 0.5777687370363209 \n",
            "Epoch:  3\n",
            "72/128.0 loss: 0.5743815949926637 \n",
            "Epoch:  3\n",
            "73/128.0 loss: 0.5811372514109354 \n",
            "Epoch:  3\n",
            "74/128.0 loss: 0.5818019384145736 \n",
            "Epoch:  3\n",
            "75/128.0 loss: 0.5802019001229813 \n",
            "Epoch:  3\n",
            "76/128.0 loss: 0.5801065128732037 \n",
            "Epoch:  3\n",
            "77/128.0 loss: 0.5770042471778698 \n",
            "Epoch:  3\n",
            "78/128.0 loss: 0.5782936700159991 \n",
            "Epoch:  3\n",
            "79/128.0 loss: 0.5781606795266271 \n",
            "Epoch:  3\n",
            "80/128.0 loss: 0.5780278946514483 \n",
            "Epoch:  3\n",
            "81/128.0 loss: 0.5820369605974454 \n",
            "Epoch:  3\n",
            "82/128.0 loss: 0.5796815180275814 \n",
            "Epoch:  3\n",
            "83/128.0 loss: 0.579287529169094 \n",
            "Epoch:  3\n",
            "84/128.0 loss: 0.5785528275896521 \n",
            "Epoch:  3\n",
            "85/128.0 loss: 0.5776592827467031 \n",
            "Epoch:  3\n",
            "86/128.0 loss: 0.5793904622738388 \n",
            "Epoch:  3\n",
            "87/128.0 loss: 0.5783424226736481 \n",
            "Epoch:  3\n",
            "88/128.0 loss: 0.5779567894975791 \n",
            "Epoch:  3\n",
            "89/128.0 loss: 0.577185397512383 \n",
            "Epoch:  3\n",
            "90/128.0 loss: 0.5759809169467989 \n",
            "Epoch:  3\n",
            "91/128.0 loss: 0.5748789260244888 \n",
            "Epoch:  3\n",
            "92/128.0 loss: 0.5734030980897206 \n",
            "Epoch:  3\n",
            "93/128.0 loss: 0.5755029972246353 \n",
            "Epoch:  3\n",
            "94/128.0 loss: 0.5761951495158045 \n",
            "Epoch:  3\n",
            "95/128.0 loss: 0.576878975921621 \n",
            "Epoch:  3\n",
            "96/128.0 loss: 0.575748494451808 \n",
            "Epoch:  3\n",
            "97/128.0 loss: 0.5747448368644228 \n",
            "Epoch:  3\n",
            "98/128.0 loss: 0.5739438667742893 \n",
            "Epoch:  3\n",
            "99/128.0 loss: 0.5744270832836628 \n",
            "Epoch:  3\n",
            "100/128.0 loss: 0.5770734414310739 \n",
            "Epoch:  3\n",
            "101/128.0 loss: 0.5777073241039818 \n",
            "Epoch:  3\n",
            "102/128.0 loss: 0.5768345812860044 \n",
            "Epoch:  3\n",
            "103/128.0 loss: 0.5778766313137916 \n",
            "Epoch:  3\n",
            "104/128.0 loss: 0.5784923449868248 \n",
            "Epoch:  3\n",
            "105/128.0 loss: 0.5781246754077246 \n",
            "Epoch:  3\n",
            "106/128.0 loss: 0.5790384304579174 \n",
            "Epoch:  3\n",
            "107/128.0 loss: 0.5772866826090548 \n",
            "Epoch:  3\n",
            "108/128.0 loss: 0.5761672456603532 \n",
            "Epoch:  3\n",
            "109/128.0 loss: 0.5755540171807463 \n",
            "Epoch:  3\n",
            "110/128.0 loss: 0.5748271444090852 \n",
            "Epoch:  3\n",
            "111/128.0 loss: 0.5731023644496288 \n",
            "Epoch:  3\n",
            "112/128.0 loss: 0.5721116631695654 \n",
            "Epoch:  3\n",
            "113/128.0 loss: 0.5713677206321767 \n",
            "Epoch:  3\n",
            "114/128.0 loss: 0.5716290249772694 \n",
            "Epoch:  3\n",
            "115/128.0 loss: 0.5718195390855444 \n",
            "Epoch:  3\n",
            "116/128.0 loss: 0.5714810978398364 \n",
            "Epoch:  3\n",
            "117/128.0 loss: 0.5717807314911131 \n",
            "Epoch:  3\n",
            "118/128.0 loss: 0.5724388104276497 \n",
            "Epoch:  3\n",
            "119/128.0 loss: 0.5709224846214056 \n",
            "Epoch:  3\n",
            "120/128.0 loss: 0.5705476163093709 \n",
            "Epoch:  3\n",
            "121/128.0 loss: 0.5692310803493515 \n",
            "Epoch:  3\n",
            "122/128.0 loss: 0.5677141192240444 \n",
            "Epoch:  3\n",
            "123/128.0 loss: 0.5670512156380761 \n",
            "Epoch:  3\n",
            "124/128.0 loss: 0.5681653467416763 \n",
            "Epoch:  3\n",
            "125/128.0 loss: 0.5713584031614046 \n",
            "Epoch:  3\n",
            "126/128.0 loss: 0.5738463764350246 \n",
            "Epoch:  3\n",
            "127/128.0 loss: 0.5728803823003545 \n",
            "Epoch:  4\n",
            "0/128.0 loss: 0.6592308878898621 \n",
            "Epoch:  4\n",
            "1/128.0 loss: 0.6550096571445465 \n",
            "Epoch:  4\n",
            "2/128.0 loss: 0.6124754746754965 \n",
            "Epoch:  4\n",
            "3/128.0 loss: 0.6361172050237656 \n",
            "Epoch:  4\n",
            "4/128.0 loss: 0.636823308467865 \n",
            "Epoch:  4\n",
            "5/128.0 loss: 0.626208484172821 \n",
            "Epoch:  4\n",
            "6/128.0 loss: 0.5966122448444366 \n",
            "Epoch:  4\n",
            "7/128.0 loss: 0.5853165946900845 \n",
            "Epoch:  4\n",
            "8/128.0 loss: 0.5577854712804159 \n",
            "Epoch:  4\n",
            "9/128.0 loss: 0.5618083059787751 \n",
            "Epoch:  4\n",
            "10/128.0 loss: 0.5597074411132119 \n",
            "Epoch:  4\n",
            "11/128.0 loss: 0.5589324484268824 \n",
            "Epoch:  4\n",
            "12/128.0 loss: 0.5458435301597302 \n",
            "Epoch:  4\n",
            "13/128.0 loss: 0.5364686889307839 \n",
            "Epoch:  4\n",
            "14/128.0 loss: 0.527643483877182 \n",
            "Epoch:  4\n",
            "15/128.0 loss: 0.5229520443826914 \n",
            "Epoch:  4\n",
            "16/128.0 loss: 0.5277519804589889 \n",
            "Epoch:  4\n",
            "17/128.0 loss: 0.5391960293054581 \n",
            "Epoch:  4\n",
            "18/128.0 loss: 0.538114037952925 \n",
            "Epoch:  4\n",
            "19/128.0 loss: 0.5304863542318344 \n",
            "Epoch:  4\n",
            "20/128.0 loss: 0.5347446572212946 \n",
            "Epoch:  4\n",
            "21/128.0 loss: 0.5376196639104323 \n",
            "Epoch:  4\n",
            "22/128.0 loss: 0.543420540249866 \n",
            "Epoch:  4\n",
            "23/128.0 loss: 0.5393189763029417 \n",
            "Epoch:  4\n",
            "24/128.0 loss: 0.5421626257896424 \n",
            "Epoch:  4\n",
            "25/128.0 loss: 0.5310042649507523 \n",
            "Epoch:  4\n",
            "26/128.0 loss: 0.5352375143104129 \n",
            "Epoch:  4\n",
            "27/128.0 loss: 0.5408909118601254 \n",
            "Epoch:  4\n",
            "28/128.0 loss: 0.5385582251795407 \n",
            "Epoch:  4\n",
            "29/128.0 loss: 0.5407589266697566 \n",
            "Epoch:  4\n",
            "30/128.0 loss: 0.5454045543747563 \n",
            "Epoch:  4\n",
            "31/128.0 loss: 0.5463382629677653 \n",
            "Epoch:  4\n",
            "32/128.0 loss: 0.5417695460897504 \n",
            "Epoch:  4\n",
            "33/128.0 loss: 0.5413527173154494 \n",
            "Epoch:  4\n",
            "34/128.0 loss: 0.542987152508327 \n",
            "Epoch:  4\n",
            "35/128.0 loss: 0.5441194590595033 \n",
            "Epoch:  4\n",
            "36/128.0 loss: 0.5442958264737516 \n",
            "Epoch:  4\n",
            "37/128.0 loss: 0.5370170458366996 \n",
            "Epoch:  4\n",
            "38/128.0 loss: 0.5357785752186408 \n",
            "Epoch:  4\n",
            "39/128.0 loss: 0.5339080780744553 \n",
            "Epoch:  4\n",
            "40/128.0 loss: 0.5350969288407302 \n",
            "Epoch:  4\n",
            "41/128.0 loss: 0.5373033114842006 \n",
            "Epoch:  4\n",
            "42/128.0 loss: 0.5336546897888184 \n",
            "Epoch:  4\n",
            "43/128.0 loss: 0.5431027439507571 \n",
            "Epoch:  4\n",
            "44/128.0 loss: 0.539601578315099 \n",
            "Epoch:  4\n",
            "45/128.0 loss: 0.5382652101309403 \n",
            "Epoch:  4\n",
            "46/128.0 loss: 0.5337543734844695 \n",
            "Epoch:  4\n",
            "47/128.0 loss: 0.5400376760711273 \n",
            "Epoch:  4\n",
            "48/128.0 loss: 0.5380916875235888 \n",
            "Epoch:  4\n",
            "49/128.0 loss: 0.5358790481090545 \n",
            "Epoch:  4\n",
            "50/128.0 loss: 0.5359493996582779 \n",
            "Epoch:  4\n",
            "51/128.0 loss: 0.5348621142598299 \n",
            "Epoch:  4\n",
            "52/128.0 loss: 0.5390154859929714 \n",
            "Epoch:  4\n",
            "53/128.0 loss: 0.5375745583463598 \n",
            "Epoch:  4\n",
            "54/128.0 loss: 0.5376293377442793 \n",
            "Epoch:  4\n",
            "55/128.0 loss: 0.530877913747515 \n",
            "Epoch:  4\n",
            "56/128.0 loss: 0.5332329482362982 \n",
            "Epoch:  4\n",
            "57/128.0 loss: 0.5312243684612471 \n",
            "Epoch:  4\n",
            "58/128.0 loss: 0.5272422028800189 \n",
            "Epoch:  4\n",
            "59/128.0 loss: 0.5300072858730952 \n",
            "Epoch:  4\n",
            "60/128.0 loss: 0.5302346954580213 \n",
            "Epoch:  4\n",
            "61/128.0 loss: 0.5279172183044495 \n",
            "Epoch:  4\n",
            "62/128.0 loss: 0.5265881494870261 \n",
            "Epoch:  4\n",
            "63/128.0 loss: 0.5251453761011362 \n",
            "Epoch:  4\n",
            "64/128.0 loss: 0.5271505621763376 \n",
            "Epoch:  4\n",
            "65/128.0 loss: 0.5257081850008531 \n",
            "Epoch:  4\n",
            "66/128.0 loss: 0.5281556666786991 \n",
            "Epoch:  4\n",
            "67/128.0 loss: 0.5269608396817657 \n",
            "Epoch:  4\n",
            "68/128.0 loss: 0.5265836184439452 \n",
            "Epoch:  4\n",
            "69/128.0 loss: 0.5276508284466607 \n",
            "Epoch:  4\n",
            "70/128.0 loss: 0.5285838569553805 \n",
            "Epoch:  4\n",
            "71/128.0 loss: 0.5277375686499808 \n",
            "Epoch:  4\n",
            "72/128.0 loss: 0.5264430311444688 \n",
            "Epoch:  4\n",
            "73/128.0 loss: 0.5282357524375658 \n",
            "Epoch:  4\n",
            "74/128.0 loss: 0.5318019998073578 \n",
            "Epoch:  4\n",
            "75/128.0 loss: 0.5292296190010873 \n",
            "Epoch:  4\n",
            "76/128.0 loss: 0.5264859075670119 \n",
            "Epoch:  4\n",
            "77/128.0 loss: 0.5302138748841408 \n",
            "Epoch:  4\n",
            "78/128.0 loss: 0.5321623539622826 \n",
            "Epoch:  4\n",
            "79/128.0 loss: 0.5305830147117376 \n",
            "Epoch:  4\n",
            "80/128.0 loss: 0.5328495631247391 \n",
            "Epoch:  4\n",
            "81/128.0 loss: 0.5328314082651604 \n",
            "Epoch:  4\n",
            "82/128.0 loss: 0.5344027327485832 \n",
            "Epoch:  4\n",
            "83/128.0 loss: 0.5373101656635603 \n",
            "Epoch:  4\n",
            "84/128.0 loss: 0.5374456949093762 \n",
            "Epoch:  4\n",
            "85/128.0 loss: 0.5393049193676128 \n",
            "Epoch:  4\n",
            "86/128.0 loss: 0.5382939501740467 \n",
            "Epoch:  4\n",
            "87/128.0 loss: 0.5401055189696226 \n",
            "Epoch:  4\n",
            "88/128.0 loss: 0.5423136219549715 \n",
            "Epoch:  4\n",
            "89/128.0 loss: 0.5419836514525943 \n",
            "Epoch:  4\n",
            "90/128.0 loss: 0.5403059462269584 \n",
            "Epoch:  4\n",
            "91/128.0 loss: 0.5387830083136973 \n",
            "Epoch:  4\n",
            "92/128.0 loss: 0.5409556245932015 \n",
            "Epoch:  4\n",
            "93/128.0 loss: 0.5431816219649417 \n",
            "Epoch:  4\n",
            "94/128.0 loss: 0.5446630035576068 \n",
            "Epoch:  4\n",
            "95/128.0 loss: 0.5447195653493205 \n",
            "Epoch:  4\n",
            "96/128.0 loss: 0.5457893938747878 \n",
            "Epoch:  4\n",
            "97/128.0 loss: 0.5473039639847619 \n",
            "Epoch:  4\n",
            "98/128.0 loss: 0.5485850154149412 \n",
            "Epoch:  4\n",
            "99/128.0 loss: 0.5508928826451301 \n",
            "Epoch:  4\n",
            "100/128.0 loss: 0.5514136980665792 \n",
            "Epoch:  4\n",
            "101/128.0 loss: 0.5515276792587018 \n",
            "Epoch:  4\n",
            "102/128.0 loss: 0.5519509422547609 \n",
            "Epoch:  4\n",
            "103/128.0 loss: 0.5526851080358028 \n",
            "Epoch:  4\n",
            "104/128.0 loss: 0.5545933601402101 \n",
            "Epoch:  4\n",
            "105/128.0 loss: 0.5552267620585999 \n",
            "Epoch:  4\n",
            "106/128.0 loss: 0.5588712734039699 \n",
            "Epoch:  4\n",
            "107/128.0 loss: 0.5570758719135214 \n",
            "Epoch:  4\n",
            "108/128.0 loss: 0.5577947902023246 \n",
            "Epoch:  4\n",
            "109/128.0 loss: 0.5567110744389621 \n",
            "Epoch:  4\n",
            "110/128.0 loss: 0.5582242661768252 \n",
            "Epoch:  4\n",
            "111/128.0 loss: 0.5563511393432107 \n",
            "Epoch:  4\n",
            "112/128.0 loss: 0.5560486039756674 \n",
            "Epoch:  4\n",
            "113/128.0 loss: 0.558055001654123 \n",
            "Epoch:  4\n",
            "114/128.0 loss: 0.5570805425229279 \n",
            "Epoch:  4\n",
            "115/128.0 loss: 0.5571814965585182 \n",
            "Epoch:  4\n",
            "116/128.0 loss: 0.5564581904655848 \n",
            "Epoch:  4\n",
            "117/128.0 loss: 0.5580992658259505 \n",
            "Epoch:  4\n",
            "118/128.0 loss: 0.557511674255884 \n",
            "Epoch:  4\n",
            "119/128.0 loss: 0.5570100665092468 \n",
            "Epoch:  4\n",
            "120/128.0 loss: 0.5556501421061429 \n",
            "Epoch:  4\n",
            "121/128.0 loss: 0.5551492708628295 \n",
            "Epoch:  4\n",
            "122/128.0 loss: 0.5533318437211882 \n",
            "Epoch:  4\n",
            "123/128.0 loss: 0.554380081353649 \n",
            "Epoch:  4\n",
            "124/128.0 loss: 0.5528554251194 \n",
            "Epoch:  4\n",
            "125/128.0 loss: 0.5521601793311891 \n",
            "Epoch:  4\n",
            "126/128.0 loss: 0.5526355636401439 \n",
            "Epoch:  4\n",
            "127/128.0 loss: 0.5517781598027796 \n",
            "Epoch:  5\n",
            "0/128.0 loss: 0.6297469735145569 \n",
            "Epoch:  5\n",
            "1/128.0 loss: 0.6440260708332062 \n",
            "Epoch:  5\n",
            "2/128.0 loss: 0.653537929058075 \n",
            "Epoch:  5\n",
            "3/128.0 loss: 0.6081124916672707 \n",
            "Epoch:  5\n",
            "4/128.0 loss: 0.5990510880947113 \n",
            "Epoch:  5\n",
            "5/128.0 loss: 0.5966428766647974 \n",
            "Epoch:  5\n",
            "6/128.0 loss: 0.5902738187994275 \n",
            "Epoch:  5\n",
            "7/128.0 loss: 0.5638512559235096 \n",
            "Epoch:  5\n",
            "8/128.0 loss: 0.5446243418587579 \n",
            "Epoch:  5\n",
            "9/128.0 loss: 0.5512865662574769 \n",
            "Epoch:  5\n",
            "10/128.0 loss: 0.560019460591403 \n",
            "Epoch:  5\n",
            "11/128.0 loss: 0.5325991300245126 \n",
            "Epoch:  5\n",
            "12/128.0 loss: 0.5408545766885464 \n",
            "Epoch:  5\n",
            "13/128.0 loss: 0.5405992088573319 \n",
            "Epoch:  5\n",
            "14/128.0 loss: 0.5753372102975846 \n",
            "Epoch:  5\n",
            "15/128.0 loss: 0.5628229258581996 \n",
            "Epoch:  5\n",
            "16/128.0 loss: 0.5638980015235788 \n",
            "Epoch:  5\n",
            "17/128.0 loss: 0.5604185056355264 \n",
            "Epoch:  5\n",
            "18/128.0 loss: 0.5595980247384623 \n",
            "Epoch:  5\n",
            "19/128.0 loss: 0.5584879599511623 \n",
            "Epoch:  5\n",
            "20/128.0 loss: 0.5484114835659663 \n",
            "Epoch:  5\n",
            "21/128.0 loss: 0.5432523048736833 \n",
            "Epoch:  5\n",
            "22/128.0 loss: 0.5411297231912613 \n",
            "Epoch:  5\n",
            "23/128.0 loss: 0.5416765231639147 \n",
            "Epoch:  5\n",
            "24/128.0 loss: 0.544819273352623 \n",
            "Epoch:  5\n",
            "25/128.0 loss: 0.5587650279586132 \n",
            "Epoch:  5\n",
            "26/128.0 loss: 0.5496673909602342 \n",
            "Epoch:  5\n",
            "27/128.0 loss: 0.551536427544696 \n",
            "Epoch:  5\n",
            "28/128.0 loss: 0.5507535970416563 \n",
            "Epoch:  5\n",
            "29/128.0 loss: 0.5464830403526624 \n",
            "Epoch:  5\n",
            "30/128.0 loss: 0.5461327842166347 \n",
            "Epoch:  5\n",
            "31/128.0 loss: 0.5421884343959391 \n",
            "Epoch:  5\n",
            "32/128.0 loss: 0.5380474640564485 \n",
            "Epoch:  5\n",
            "33/128.0 loss: 0.5421539058580118 \n",
            "Epoch:  5\n",
            "34/128.0 loss: 0.5384515553712845 \n",
            "Epoch:  5\n",
            "35/128.0 loss: 0.5347201381292608 \n",
            "Epoch:  5\n",
            "36/128.0 loss: 0.5405125944195567 \n",
            "Epoch:  5\n",
            "37/128.0 loss: 0.5342293628736546 \n",
            "Epoch:  5\n",
            "38/128.0 loss: 0.5406725097161073 \n",
            "Epoch:  5\n",
            "39/128.0 loss: 0.5357742201536894 \n",
            "Epoch:  5\n",
            "40/128.0 loss: 0.5330325405045253 \n",
            "Epoch:  5\n",
            "41/128.0 loss: 0.5332261036549296 \n",
            "Epoch:  5\n",
            "42/128.0 loss: 0.5307949708644734 \n",
            "Epoch:  5\n",
            "43/128.0 loss: 0.5254640623249791 \n",
            "Epoch:  5\n",
            "44/128.0 loss: 0.5274257583750619 \n",
            "Epoch:  5\n",
            "45/128.0 loss: 0.530570350587368 \n",
            "Epoch:  5\n",
            "46/128.0 loss: 0.5228487532189552 \n",
            "Epoch:  5\n",
            "47/128.0 loss: 0.526166512320439 \n",
            "Epoch:  5\n",
            "48/128.0 loss: 0.5240680021899087 \n",
            "Epoch:  5\n",
            "49/128.0 loss: 0.5264312773942947 \n",
            "Epoch:  5\n",
            "50/128.0 loss: 0.5183461160928595 \n",
            "Epoch:  5\n",
            "51/128.0 loss: 0.5166361926553341 \n",
            "Epoch:  5\n",
            "52/128.0 loss: 0.5209721325422233 \n",
            "Epoch:  5\n",
            "53/128.0 loss: 0.5238674392026884 \n",
            "Epoch:  5\n",
            "54/128.0 loss: 0.5230683065273545 \n",
            "Epoch:  5\n",
            "55/128.0 loss: 0.5239571618980595 \n",
            "Epoch:  5\n",
            "56/128.0 loss: 0.5205470703934368 \n",
            "Epoch:  5\n",
            "57/128.0 loss: 0.5213585263439293 \n",
            "Epoch:  5\n",
            "58/128.0 loss: 0.5170602615354425 \n",
            "Epoch:  5\n",
            "59/128.0 loss: 0.5190244683374962 \n",
            "Epoch:  5\n",
            "60/128.0 loss: 0.5174155517435465 \n",
            "Epoch:  5\n",
            "61/128.0 loss: 0.5152406941258139 \n",
            "Epoch:  5\n",
            "62/128.0 loss: 0.5149929929110739 \n",
            "Epoch:  5\n",
            "63/128.0 loss: 0.5130213660886511 \n",
            "Epoch:  5\n",
            "64/128.0 loss: 0.5119279814454225 \n",
            "Epoch:  5\n",
            "65/128.0 loss: 0.5123150383658481 \n",
            "Epoch:  5\n",
            "66/128.0 loss: 0.5099619639231198 \n",
            "Epoch:  5\n",
            "67/128.0 loss: 0.5095362039830755 \n",
            "Epoch:  5\n",
            "68/128.0 loss: 0.507261727059233 \n",
            "Epoch:  5\n",
            "69/128.0 loss: 0.5087897785007953 \n",
            "Epoch:  5\n",
            "70/128.0 loss: 0.5109646327688661 \n",
            "Epoch:  5\n",
            "71/128.0 loss: 0.5090794082110127 \n",
            "Epoch:  5\n",
            "72/128.0 loss: 0.5086380568881558 \n",
            "Epoch:  5\n",
            "73/128.0 loss: 0.5088740376805937 \n",
            "Epoch:  5\n",
            "74/128.0 loss: 0.5104397733012835 \n",
            "Epoch:  5\n",
            "75/128.0 loss: 0.5100492645839327 \n",
            "Epoch:  5\n",
            "76/128.0 loss: 0.5100228638424502 \n",
            "Epoch:  5\n",
            "77/128.0 loss: 0.5105448478880601 \n",
            "Epoch:  5\n",
            "78/128.0 loss: 0.5102179619141772 \n",
            "Epoch:  5\n",
            "79/128.0 loss: 0.5090966037474572 \n",
            "Epoch:  5\n",
            "80/128.0 loss: 0.5086981612775061 \n",
            "Epoch:  5\n",
            "81/128.0 loss: 0.5127112184901063 \n",
            "Epoch:  5\n",
            "82/128.0 loss: 0.5107977234994072 \n",
            "Epoch:  5\n",
            "83/128.0 loss: 0.5164439304776135 \n",
            "Epoch:  5\n",
            "84/128.0 loss: 0.5152390307363341 \n",
            "Epoch:  5\n",
            "85/128.0 loss: 0.513639711623275 \n",
            "Epoch:  5\n",
            "86/128.0 loss: 0.5088901563451208 \n",
            "Epoch:  5\n",
            "87/128.0 loss: 0.5083777927370234 \n",
            "Epoch:  5\n",
            "88/128.0 loss: 0.5078914981712116 \n",
            "Epoch:  5\n",
            "89/128.0 loss: 0.5098983101546765 \n",
            "Epoch:  5\n",
            "90/128.0 loss: 0.5094666119951469 \n",
            "Epoch:  5\n",
            "91/128.0 loss: 0.5160100504594005 \n",
            "Epoch:  5\n",
            "92/128.0 loss: 0.5217955899975633 \n",
            "Epoch:  5\n",
            "93/128.0 loss: 0.5316257989628518 \n",
            "Epoch:  5\n",
            "94/128.0 loss: 0.5304701230243632 \n",
            "Epoch:  5\n",
            "95/128.0 loss: 0.5293369162051628 \n",
            "Epoch:  5\n",
            "96/128.0 loss: 0.5303578277684978 \n",
            "Epoch:  5\n",
            "97/128.0 loss: 0.5299769380719078 \n",
            "Epoch:  5\n",
            "98/128.0 loss: 0.528404886599141 \n",
            "Epoch:  5\n",
            "99/128.0 loss: 0.5278993975371122 \n",
            "Epoch:  5\n",
            "100/128.0 loss: 0.5292630814532242 \n",
            "Epoch:  5\n",
            "101/128.0 loss: 0.5285590643830159 \n",
            "Epoch:  5\n",
            "102/128.0 loss: 0.528429759575904 \n",
            "Epoch:  5\n",
            "103/128.0 loss: 0.5304921965043132 \n",
            "Epoch:  5\n",
            "104/128.0 loss: 0.5335467431516875 \n",
            "Epoch:  5\n",
            "105/128.0 loss: 0.5372123495447185 \n",
            "Epoch:  5\n",
            "106/128.0 loss: 0.5368508924251405 \n",
            "Epoch:  5\n",
            "107/128.0 loss: 0.5367654103233859 \n",
            "Epoch:  5\n",
            "108/128.0 loss: 0.5381692040267341 \n",
            "Epoch:  5\n",
            "109/128.0 loss: 0.5376340721818533 \n",
            "Epoch:  5\n",
            "110/128.0 loss: 0.5404857044687142 \n",
            "Epoch:  5\n",
            "111/128.0 loss: 0.540946584926652 \n",
            "Epoch:  5\n",
            "112/128.0 loss: 0.5412262943861759 \n",
            "Epoch:  5\n",
            "113/128.0 loss: 0.5409786494023967 \n",
            "Epoch:  5\n",
            "114/128.0 loss: 0.541847100659557 \n",
            "Epoch:  5\n",
            "115/128.0 loss: 0.5417380494161926 \n",
            "Epoch:  5\n",
            "116/128.0 loss: 0.5407007960045439 \n",
            "Epoch:  5\n",
            "117/128.0 loss: 0.539343598977489 \n",
            "Epoch:  5\n",
            "118/128.0 loss: 0.5393875035292962 \n",
            "Epoch:  5\n",
            "119/128.0 loss: 0.5396039490277569 \n",
            "Epoch:  5\n",
            "120/128.0 loss: 0.5410571270122015 \n",
            "Epoch:  5\n",
            "121/128.0 loss: 0.5397423133864755 \n",
            "Epoch:  5\n",
            "122/128.0 loss: 0.5389674535490633 \n",
            "Epoch:  5\n",
            "123/128.0 loss: 0.5385180874577453 \n",
            "Epoch:  5\n",
            "124/128.0 loss: 0.5359335618615151 \n",
            "Epoch:  5\n",
            "125/128.0 loss: 0.5355860681406089 \n",
            "Epoch:  5\n",
            "126/128.0 loss: 0.5375858649142146 \n",
            "Epoch:  5\n",
            "127/128.0 loss: 0.5401858059340157 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZffdI9Wn-dT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1510c8-8f09-4884-c241-a15e3ad473e0"
      },
      "source": [
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        # The new model has slightly different outputs. A sigmoid() is applied to logits to bound between 0 and 1\n",
        "        loss = loss_func(logits.sigmoid(), labels)\n",
        "        numpy_logits = logits.cpu().detach().numpy()\n",
        "        \n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "        all_logits += list(numpy_logits[:, 0])\n",
        "        \n",
        "print(classification_report(test_y, bert_predicted))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.97      0.38      0.54        85\n",
            "        True       0.58      0.99      0.73        75\n",
            "\n",
            "    accuracy                           0.66       160\n",
            "   macro avg       0.78      0.68      0.64       160\n",
            "weighted avg       0.79      0.66      0.63       160\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}