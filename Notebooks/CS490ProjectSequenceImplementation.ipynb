{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS490Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8/RNWvFltXwMqX8lU+4pz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Johoodcoder/CS490Project/blob/hood/Notebooks/CS490ProjectSequenceImplementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTZbJ1SJ53XO"
      },
      "source": [
        "Non-preinstalled module installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm5_ujD458U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141aa4cd-9857-46a0-d77b-d94663854a1b"
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.17.33)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.6)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.33 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.20.33)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.33->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.33->boto3->pytorch-pretrained-bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbufH4ZX8X5N"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
        "import torch\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a-5pyC08dzO",
        "outputId": "b385525d-5f85-4650-b523-e005ef353bf5"
      },
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv(\"condensed_fake_real_news.csv\")\n",
        "df = df[['text', 'type']]\n",
        "print(len(df))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HHnQVRq8z0n",
        "outputId": "bb638a0d-c90e-40c2-c34a-230c1909818a"
      },
      "source": [
        "df = df[df['type'].isin(['fake', 'real'])]\n",
        "# Scramble data indexes from dataset. Random_state is a seed.\n",
        "df = df.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "\n",
        "print(Counter(df['type'].values))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'fake': 4000, 'real': 4000})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wOwpOde8_WC",
        "outputId": "3347a840-5afb-430d-e51b-beff4b7b1537"
      },
      "source": [
        "train_data_df = df.head(640)\n",
        "test_data_df = df.tail(160)\n",
        "print(train_data_df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  text  type\n",
            "0    Donald Trump s most recent secretive actions t...  fake\n",
            "1    WASHINGTON (Reuters) - U.S. President Donald T...  real\n",
            "2    WASHINGTON (Reuters) - U.S. President Donald T...  real\n",
            "3    WASHINGTON (Reuters) - Congressional leaders a...  real\n",
            "4    One of Trump s biggest campaign promises was t...  fake\n",
            "..                                                 ...   ...\n",
            "635  President Barack Obama gave an amazing farewel...  fake\n",
            "636  When Donald Trump kicked off  Made in America ...  fake\n",
            "637  With Donald Trump winning the election, albeit...  fake\n",
            "638  WASHINGTON (Reuters) - The U.S. House of Repre...  real\n",
            "639  (Reuters) - The Republican Party will resume f...  real\n",
            "\n",
            "[640 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bborPzYM9CUR"
      },
      "source": [
        "train_data = []\n",
        "for index, row in train_data_df.iterrows():\n",
        "    train_data.append({'text': row['text'], 'type': row['type']})\n",
        "\n",
        "test_data = []\n",
        "for index, row in test_data_df.iterrows():\n",
        "    test_data.append({'text': row['text'], 'type': row['type']})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G7zoLaF9gNf"
      },
      "source": [
        "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['type']), train_data)))\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['type']), test_data)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KNrngW99ooH"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\n",
        "\n",
        "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
        "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
        "\n",
        "\n",
        "\n",
        "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAKhviVo9ujZ"
      },
      "source": [
        "# If value == fake then make it true. Otherwise false.\n",
        "train_y = np.array(train_labels) == 'fake'\n",
        "test_y = np.array(test_labels) == 'fake'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjP7PqON92FH"
      },
      "source": [
        "# Input masks differentiate padding tokens from lefitimate data token. 1 == data, 0 == padding\n",
        "# If this could be int we may increase speed\n",
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1poQHOBe-DM6"
      },
      "source": [
        "# If we can leave these tensors as int processing speed should increase\n",
        "\n",
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "# train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).int()\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "# test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).int()\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "# ----------------------------------------- CUDA -----------------------------------------------------\n",
        "# train_masks_tensor = train_masks_tensor.to('cuda')\n",
        "# test_masks_tensor = test_masks_tensor.to('cuda')\n",
        "\n",
        "train_tokens_tensor = train_tokens_tensor.to('cuda')\n",
        "test_tokens_tensor = test_tokens_tensor.to('cuda')\n",
        "\n",
        "train_y_tensor = train_y_tensor.to('cuda')\n",
        "test_y_tensor = test_y_tensor.to('cuda')\n",
        "\n",
        "# ----------------------------------------- CUDA LONG -----------------------------------------------------\n",
        "cuda = torch.device('cuda')\n",
        "train_masks_tensor = train_masks_tensor.to(cuda, dtype = torch.long)\n",
        "test_masks_tensor = test_masks_tensor.to(cuda, dtype = torch.long)\n",
        "\n",
        "# train_tokens_tensor = train_tokens_tensor.to(cuda, dtype = torch.long)\n",
        "# test_tokens_tensor = test_tokens_tensor.to(cuda, dtype = torch.long)\n",
        "\n",
        "# train_y_tensor = train_y_tensor.to(cuda, dtype = torch.long)\n",
        "# test_y_tensor = test_y_tensor.to(cuda, dtype = torch.long)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kHKfGwF-DZp"
      },
      "source": [
        "BATCH_SIZE = 10\n",
        "EPOCHS = 5\n",
        "\n",
        "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
        "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
        "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "num_labels = 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kdg4sqN-DkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df110aa4-cf14-418c-f6d2-8425edd00bb0"
      },
      "source": [
        "bert_clf = BertForSequenceClassification(config, num_labels)\n",
        "bert_clf.to('cuda')\n",
        "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
        "\n",
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "        probas = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        # The new model has slightly different outputs. A sigmoid() is applied to probas to bound between 0 and 1\n",
        "        batch_loss = loss_func(probas.sigmoid(), labels)\n",
        "        train_loss += batch_loss.item()\n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "0/64.0 loss: 0.6771923303604126 \n",
            "Epoch:  1\n",
            "1/64.0 loss: 0.721467524766922 \n",
            "Epoch:  1\n",
            "2/64.0 loss: 0.7092098395029703 \n",
            "Epoch:  1\n",
            "3/64.0 loss: 0.7025067359209061 \n",
            "Epoch:  1\n",
            "4/64.0 loss: 0.6893310427665711 \n",
            "Epoch:  1\n",
            "5/64.0 loss: 0.6923704842726389 \n",
            "Epoch:  1\n",
            "6/64.0 loss: 0.6998102068901062 \n",
            "Epoch:  1\n",
            "7/64.0 loss: 0.6939983367919922 \n",
            "Epoch:  1\n",
            "8/64.0 loss: 0.6846200890011258 \n",
            "Epoch:  1\n",
            "9/64.0 loss: 0.6842139184474945 \n",
            "Epoch:  1\n",
            "10/64.0 loss: 0.6675998622720892 \n",
            "Epoch:  1\n",
            "11/64.0 loss: 0.6636156042416891 \n",
            "Epoch:  1\n",
            "12/64.0 loss: 0.6601145267486572 \n",
            "Epoch:  1\n",
            "13/64.0 loss: 0.6587604284286499 \n",
            "Epoch:  1\n",
            "14/64.0 loss: 0.6567230224609375 \n",
            "Epoch:  1\n",
            "15/64.0 loss: 0.6536340229213238 \n",
            "Epoch:  1\n",
            "16/64.0 loss: 0.6526544479762807 \n",
            "Epoch:  1\n",
            "17/64.0 loss: 0.6521809233559502 \n",
            "Epoch:  1\n",
            "18/64.0 loss: 0.65560845011159 \n",
            "Epoch:  1\n",
            "19/64.0 loss: 0.6540781944990158 \n",
            "Epoch:  1\n",
            "20/64.0 loss: 0.6508544456391108 \n",
            "Epoch:  1\n",
            "21/64.0 loss: 0.6487918929620222 \n",
            "Epoch:  1\n",
            "22/64.0 loss: 0.6489210958066194 \n",
            "Epoch:  1\n",
            "23/64.0 loss: 0.6487870489557584 \n",
            "Epoch:  1\n",
            "24/64.0 loss: 0.6466168713569641 \n",
            "Epoch:  1\n",
            "25/64.0 loss: 0.6478796601295471 \n",
            "Epoch:  1\n",
            "26/64.0 loss: 0.6481178049687986 \n",
            "Epoch:  1\n",
            "27/64.0 loss: 0.6463131436279842 \n",
            "Epoch:  1\n",
            "28/64.0 loss: 0.6480102621275803 \n",
            "Epoch:  1\n",
            "29/64.0 loss: 0.6432726045449575 \n",
            "Epoch:  1\n",
            "30/64.0 loss: 0.6467510288761508 \n",
            "Epoch:  1\n",
            "31/64.0 loss: 0.6475576907396317 \n",
            "Epoch:  1\n",
            "32/64.0 loss: 0.6457777510989796 \n",
            "Epoch:  1\n",
            "33/64.0 loss: 0.6463831628070158 \n",
            "Epoch:  1\n",
            "34/64.0 loss: 0.6433870673179627 \n",
            "Epoch:  1\n",
            "35/64.0 loss: 0.6426372528076172 \n",
            "Epoch:  1\n",
            "36/64.0 loss: 0.6453004704939352 \n",
            "Epoch:  1\n",
            "37/64.0 loss: 0.6465263131417727 \n",
            "Epoch:  1\n",
            "38/64.0 loss: 0.6450384549605541 \n",
            "Epoch:  1\n",
            "39/64.0 loss: 0.6417064353823662 \n",
            "Epoch:  1\n",
            "40/64.0 loss: 0.6433600245452509 \n",
            "Epoch:  1\n",
            "41/64.0 loss: 0.6419930372919355 \n",
            "Epoch:  1\n",
            "42/64.0 loss: 0.6428234646486681 \n",
            "Epoch:  1\n",
            "43/64.0 loss: 0.6419889764352278 \n",
            "Epoch:  1\n",
            "44/64.0 loss: 0.6431494169765049 \n",
            "Epoch:  1\n",
            "45/64.0 loss: 0.6413277905920277 \n",
            "Epoch:  1\n",
            "46/64.0 loss: 0.6396358342880898 \n",
            "Epoch:  1\n",
            "47/64.0 loss: 0.6417337618768215 \n",
            "Epoch:  1\n",
            "48/64.0 loss: 0.6411013323433545 \n",
            "Epoch:  1\n",
            "49/64.0 loss: 0.6420020544528962 \n",
            "Epoch:  1\n",
            "50/64.0 loss: 0.640354760721618 \n",
            "Epoch:  1\n",
            "51/64.0 loss: 0.6388947872015146 \n",
            "Epoch:  1\n",
            "52/64.0 loss: 0.6368935749215899 \n",
            "Epoch:  1\n",
            "53/64.0 loss: 0.6330864523296003 \n",
            "Epoch:  1\n",
            "54/64.0 loss: 0.6319392112168398 \n",
            "Epoch:  1\n",
            "55/64.0 loss: 0.6312827196504388 \n",
            "Epoch:  1\n",
            "56/64.0 loss: 0.6285944079097948 \n",
            "Epoch:  1\n",
            "57/64.0 loss: 0.6302540877769733 \n",
            "Epoch:  1\n",
            "58/64.0 loss: 0.6329528911639069 \n",
            "Epoch:  1\n",
            "59/64.0 loss: 0.6314246892929077 \n",
            "Epoch:  1\n",
            "60/64.0 loss: 0.6290316909063057 \n",
            "Epoch:  1\n",
            "61/64.0 loss: 0.6305894231603991 \n",
            "Epoch:  1\n",
            "62/64.0 loss: 0.6296192998923953 \n",
            "Epoch:  1\n",
            "63/64.0 loss: 0.6286492659710348 \n",
            "Epoch:  2\n",
            "0/64.0 loss: 0.6077151894569397 \n",
            "Epoch:  2\n",
            "1/64.0 loss: 0.6226013004779816 \n",
            "Epoch:  2\n",
            "2/64.0 loss: 0.5702445904413859 \n",
            "Epoch:  2\n",
            "3/64.0 loss: 0.6132160574197769 \n",
            "Epoch:  2\n",
            "4/64.0 loss: 0.6049599647521973 \n",
            "Epoch:  2\n",
            "5/64.0 loss: 0.5847987482945124 \n",
            "Epoch:  2\n",
            "6/64.0 loss: 0.596559077501297 \n",
            "Epoch:  2\n",
            "7/64.0 loss: 0.5961914323270321 \n",
            "Epoch:  2\n",
            "8/64.0 loss: 0.5909814735253652 \n",
            "Epoch:  2\n",
            "9/64.0 loss: 0.5855835050344467 \n",
            "Epoch:  2\n",
            "10/64.0 loss: 0.5814040547067468 \n",
            "Epoch:  2\n",
            "11/64.0 loss: 0.5752428298195204 \n",
            "Epoch:  2\n",
            "12/64.0 loss: 0.5837889199073498 \n",
            "Epoch:  2\n",
            "13/64.0 loss: 0.5956167983157294 \n",
            "Epoch:  2\n",
            "14/64.0 loss: 0.5935200949509939 \n",
            "Epoch:  2\n",
            "15/64.0 loss: 0.6014312487095594 \n",
            "Epoch:  2\n",
            "16/64.0 loss: 0.6025976486065808 \n",
            "Epoch:  2\n",
            "17/64.0 loss: 0.5963375121355057 \n",
            "Epoch:  2\n",
            "18/64.0 loss: 0.6024974035589319 \n",
            "Epoch:  2\n",
            "19/64.0 loss: 0.5960666000843048 \n",
            "Epoch:  2\n",
            "20/64.0 loss: 0.5927551417123704 \n",
            "Epoch:  2\n",
            "21/64.0 loss: 0.5927138247273185 \n",
            "Epoch:  2\n",
            "22/64.0 loss: 0.5940878080285114 \n",
            "Epoch:  2\n",
            "23/64.0 loss: 0.5902858053644499 \n",
            "Epoch:  2\n",
            "24/64.0 loss: 0.585189425945282 \n",
            "Epoch:  2\n",
            "25/64.0 loss: 0.5864557257065406 \n",
            "Epoch:  2\n",
            "26/64.0 loss: 0.5897383932714109 \n",
            "Epoch:  2\n",
            "27/64.0 loss: 0.5920772084168026 \n",
            "Epoch:  2\n",
            "28/64.0 loss: 0.5965080466763727 \n",
            "Epoch:  2\n",
            "29/64.0 loss: 0.5991012454032898 \n",
            "Epoch:  2\n",
            "30/64.0 loss: 0.6006596030727509 \n",
            "Epoch:  2\n",
            "31/64.0 loss: 0.5996275767683983 \n",
            "Epoch:  2\n",
            "32/64.0 loss: 0.6091131712451126 \n",
            "Epoch:  2\n",
            "33/64.0 loss: 0.6083515128668617 \n",
            "Epoch:  2\n",
            "34/64.0 loss: 0.6054555586406163 \n",
            "Epoch:  2\n",
            "35/64.0 loss: 0.6063470194737116 \n",
            "Epoch:  2\n",
            "36/64.0 loss: 0.6051742708360827 \n",
            "Epoch:  2\n",
            "37/64.0 loss: 0.6047265451205405 \n",
            "Epoch:  2\n",
            "38/64.0 loss: 0.6052183019809234 \n",
            "Epoch:  2\n",
            "39/64.0 loss: 0.6030819922685623 \n",
            "Epoch:  2\n",
            "40/64.0 loss: 0.6009051450868932 \n",
            "Epoch:  2\n",
            "41/64.0 loss: 0.595750636997677 \n",
            "Epoch:  2\n",
            "42/64.0 loss: 0.5934697479702705 \n",
            "Epoch:  2\n",
            "43/64.0 loss: 0.5966499088840052 \n",
            "Epoch:  2\n",
            "44/64.0 loss: 0.5946960429350535 \n",
            "Epoch:  2\n",
            "45/64.0 loss: 0.5992044229870257 \n",
            "Epoch:  2\n",
            "46/64.0 loss: 0.598542867188758 \n",
            "Epoch:  2\n",
            "47/64.0 loss: 0.5988779769589504 \n",
            "Epoch:  2\n",
            "48/64.0 loss: 0.6007991937958465 \n",
            "Epoch:  2\n",
            "49/64.0 loss: 0.5993654924631119 \n",
            "Epoch:  2\n",
            "50/64.0 loss: 0.5986721369565702 \n",
            "Epoch:  2\n",
            "51/64.0 loss: 0.5984441620799211 \n",
            "Epoch:  2\n",
            "52/64.0 loss: 0.6003624116474727 \n",
            "Epoch:  2\n",
            "53/64.0 loss: 0.6005619725695363 \n",
            "Epoch:  2\n",
            "54/64.0 loss: 0.5994725698774511 \n",
            "Epoch:  2\n",
            "55/64.0 loss: 0.5967628226748535 \n",
            "Epoch:  2\n",
            "56/64.0 loss: 0.5946448319836667 \n",
            "Epoch:  2\n",
            "57/64.0 loss: 0.593349458842442 \n",
            "Epoch:  2\n",
            "58/64.0 loss: 0.5926528148731943 \n",
            "Epoch:  2\n",
            "59/64.0 loss: 0.5925075848897298 \n",
            "Epoch:  2\n",
            "60/64.0 loss: 0.5919750772538732 \n",
            "Epoch:  2\n",
            "61/64.0 loss: 0.592443444075123 \n",
            "Epoch:  2\n",
            "62/64.0 loss: 0.5935923561217293 \n",
            "Epoch:  2\n",
            "63/64.0 loss: 0.5940202344208956 \n",
            "Epoch:  3\n",
            "0/64.0 loss: 0.5716233253479004 \n",
            "Epoch:  3\n",
            "1/64.0 loss: 0.5533171594142914 \n",
            "Epoch:  3\n",
            "2/64.0 loss: 0.5281323393185934 \n",
            "Epoch:  3\n",
            "3/64.0 loss: 0.5631285160779953 \n",
            "Epoch:  3\n",
            "4/64.0 loss: 0.552970004081726 \n",
            "Epoch:  3\n",
            "5/64.0 loss: 0.5671884218851725 \n",
            "Epoch:  3\n",
            "6/64.0 loss: 0.5531185780252729 \n",
            "Epoch:  3\n",
            "7/64.0 loss: 0.5525765419006348 \n",
            "Epoch:  3\n",
            "8/64.0 loss: 0.5842985577053494 \n",
            "Epoch:  3\n",
            "9/64.0 loss: 0.5875251591205597 \n",
            "Epoch:  3\n",
            "10/64.0 loss: 0.6047439087520946 \n",
            "Epoch:  3\n",
            "11/64.0 loss: 0.5912632669011751 \n",
            "Epoch:  3\n",
            "12/64.0 loss: 0.5780100455650916 \n",
            "Epoch:  3\n",
            "13/64.0 loss: 0.5817949516432626 \n",
            "Epoch:  3\n",
            "14/64.0 loss: 0.5849830468495687 \n",
            "Epoch:  3\n",
            "15/64.0 loss: 0.5892336703836918 \n",
            "Epoch:  3\n",
            "16/64.0 loss: 0.5793709702351514 \n",
            "Epoch:  3\n",
            "17/64.0 loss: 0.5757979469166862 \n",
            "Epoch:  3\n",
            "18/64.0 loss: 0.5832529083678597 \n",
            "Epoch:  3\n",
            "19/64.0 loss: 0.5768624395132065 \n",
            "Epoch:  3\n",
            "20/64.0 loss: 0.5782172566368466 \n",
            "Epoch:  3\n",
            "21/64.0 loss: 0.583815642378547 \n",
            "Epoch:  3\n",
            "22/64.0 loss: 0.5837416545204495 \n",
            "Epoch:  3\n",
            "23/64.0 loss: 0.5802672356367111 \n",
            "Epoch:  3\n",
            "24/64.0 loss: 0.5757493090629577 \n",
            "Epoch:  3\n",
            "25/64.0 loss: 0.5753324284003332 \n",
            "Epoch:  3\n",
            "26/64.0 loss: 0.5705996939429531 \n",
            "Epoch:  3\n",
            "27/64.0 loss: 0.5805868836385863 \n",
            "Epoch:  3\n",
            "28/64.0 loss: 0.5844540791264896 \n",
            "Epoch:  3\n",
            "29/64.0 loss: 0.5871996730566025 \n",
            "Epoch:  3\n",
            "30/64.0 loss: 0.5895370389184644 \n",
            "Epoch:  3\n",
            "31/64.0 loss: 0.5902677224949002 \n",
            "Epoch:  3\n",
            "32/64.0 loss: 0.5921177222873225 \n",
            "Epoch:  3\n",
            "33/64.0 loss: 0.5900027357480105 \n",
            "Epoch:  3\n",
            "34/64.0 loss: 0.5890762678214482 \n",
            "Epoch:  3\n",
            "35/64.0 loss: 0.593568423555957 \n",
            "Epoch:  3\n",
            "36/64.0 loss: 0.5891203670888334 \n",
            "Epoch:  3\n",
            "37/64.0 loss: 0.5887020167551542 \n",
            "Epoch:  3\n",
            "38/64.0 loss: 0.5874158037014496 \n",
            "Epoch:  3\n",
            "39/64.0 loss: 0.5918713286519051 \n",
            "Epoch:  3\n",
            "40/64.0 loss: 0.5915870128608332 \n",
            "Epoch:  3\n",
            "41/64.0 loss: 0.5900627005667913 \n",
            "Epoch:  3\n",
            "42/64.0 loss: 0.5889321427012599 \n",
            "Epoch:  3\n",
            "43/64.0 loss: 0.5896124812689695 \n",
            "Epoch:  3\n",
            "44/64.0 loss: 0.5853786124123468 \n",
            "Epoch:  3\n",
            "45/64.0 loss: 0.5859981995561848 \n",
            "Epoch:  3\n",
            "46/64.0 loss: 0.5862185194137248 \n",
            "Epoch:  3\n",
            "47/64.0 loss: 0.5833539019028345 \n",
            "Epoch:  3\n",
            "48/64.0 loss: 0.5800987719273081 \n",
            "Epoch:  3\n",
            "49/64.0 loss: 0.5806648129224777 \n",
            "Epoch:  3\n",
            "50/64.0 loss: 0.5847006457693437 \n",
            "Epoch:  3\n",
            "51/64.0 loss: 0.5845163928774687 \n",
            "Epoch:  3\n",
            "52/64.0 loss: 0.5845793075156662 \n",
            "Epoch:  3\n",
            "53/64.0 loss: 0.5801963397750148 \n",
            "Epoch:  3\n",
            "54/64.0 loss: 0.5792698968540538 \n",
            "Epoch:  3\n",
            "55/64.0 loss: 0.5783555805683136 \n",
            "Epoch:  3\n",
            "56/64.0 loss: 0.57982197351623 \n",
            "Epoch:  3\n",
            "57/64.0 loss: 0.5797662550005419 \n",
            "Epoch:  3\n",
            "58/64.0 loss: 0.5793706390817287 \n",
            "Epoch:  3\n",
            "59/64.0 loss: 0.5797019064426422 \n",
            "Epoch:  3\n",
            "60/64.0 loss: 0.5812726782970741 \n",
            "Epoch:  3\n",
            "61/64.0 loss: 0.5825217510423353 \n",
            "Epoch:  3\n",
            "62/64.0 loss: 0.5844369483372521 \n",
            "Epoch:  3\n",
            "63/64.0 loss: 0.5833361316472292 \n",
            "Epoch:  4\n",
            "0/64.0 loss: 0.7151591181755066 \n",
            "Epoch:  4\n",
            "1/64.0 loss: 0.7247749269008636 \n",
            "Epoch:  4\n",
            "2/64.0 loss: 0.6877675652503967 \n",
            "Epoch:  4\n",
            "3/64.0 loss: 0.6675962507724762 \n",
            "Epoch:  4\n",
            "4/64.0 loss: 0.5999937236309052 \n",
            "Epoch:  4\n",
            "5/64.0 loss: 0.6083280692497889 \n",
            "Epoch:  4\n",
            "6/64.0 loss: 0.5826782626765115 \n",
            "Epoch:  4\n",
            "7/64.0 loss: 0.5877350009977818 \n",
            "Epoch:  4\n",
            "8/64.0 loss: 0.5701316297054291 \n",
            "Epoch:  4\n",
            "9/64.0 loss: 0.5660275191068649 \n",
            "Epoch:  4\n",
            "10/64.0 loss: 0.5683788630095395 \n",
            "Epoch:  4\n",
            "11/64.0 loss: 0.5610452368855476 \n",
            "Epoch:  4\n",
            "12/64.0 loss: 0.5683866487099574 \n",
            "Epoch:  4\n",
            "13/64.0 loss: 0.5624143119369235 \n",
            "Epoch:  4\n",
            "14/64.0 loss: 0.5665488501389822 \n",
            "Epoch:  4\n",
            "15/64.0 loss: 0.5632928665727377 \n",
            "Epoch:  4\n",
            "16/64.0 loss: 0.565469470094232 \n",
            "Epoch:  4\n",
            "17/64.0 loss: 0.5663368221786287 \n",
            "Epoch:  4\n",
            "18/64.0 loss: 0.5687015825196317 \n",
            "Epoch:  4\n",
            "19/64.0 loss: 0.571706973016262 \n",
            "Epoch:  4\n",
            "20/64.0 loss: 0.5671261705103374 \n",
            "Epoch:  4\n",
            "21/64.0 loss: 0.569997333667495 \n",
            "Epoch:  4\n",
            "22/64.0 loss: 0.5734550240247146 \n",
            "Epoch:  4\n",
            "23/64.0 loss: 0.5748037186761698 \n",
            "Epoch:  4\n",
            "24/64.0 loss: 0.5774498760700226 \n",
            "Epoch:  4\n",
            "25/64.0 loss: 0.5752975058097106 \n",
            "Epoch:  4\n",
            "26/64.0 loss: 0.5709669214707834 \n",
            "Epoch:  4\n",
            "27/64.0 loss: 0.5648762956261635 \n",
            "Epoch:  4\n",
            "28/64.0 loss: 0.5647621987194851 \n",
            "Epoch:  4\n",
            "29/64.0 loss: 0.568086392680804 \n",
            "Epoch:  4\n",
            "30/64.0 loss: 0.569415089584166 \n",
            "Epoch:  4\n",
            "31/64.0 loss: 0.5715915551409125 \n",
            "Epoch:  4\n",
            "32/64.0 loss: 0.5710561392885266 \n",
            "Epoch:  4\n",
            "33/64.0 loss: 0.5730476353098365 \n",
            "Epoch:  4\n",
            "34/64.0 loss: 0.5734307859625135 \n",
            "Epoch:  4\n",
            "35/64.0 loss: 0.5709680765867233 \n",
            "Epoch:  4\n",
            "36/64.0 loss: 0.568680353261329 \n",
            "Epoch:  4\n",
            "37/64.0 loss: 0.5669148227101878 \n",
            "Epoch:  4\n",
            "38/64.0 loss: 0.5659407354318179 \n",
            "Epoch:  4\n",
            "39/64.0 loss: 0.5673843406140804 \n",
            "Epoch:  4\n",
            "40/64.0 loss: 0.5665656073791224 \n",
            "Epoch:  4\n",
            "41/64.0 loss: 0.5651365525665737 \n",
            "Epoch:  4\n",
            "42/64.0 loss: 0.5624288202718247 \n",
            "Epoch:  4\n",
            "43/64.0 loss: 0.5647822814908895 \n",
            "Epoch:  4\n",
            "44/64.0 loss: 0.5689037700494131 \n",
            "Epoch:  4\n",
            "45/64.0 loss: 0.5675031238276026 \n",
            "Epoch:  4\n",
            "46/64.0 loss: 0.5679425844486724 \n",
            "Epoch:  4\n",
            "47/64.0 loss: 0.5651942547410727 \n",
            "Epoch:  4\n",
            "48/64.0 loss: 0.5651363931140121 \n",
            "Epoch:  4\n",
            "49/64.0 loss: 0.5635714113712311 \n",
            "Epoch:  4\n",
            "50/64.0 loss: 0.5647492829491111 \n",
            "Epoch:  4\n",
            "51/64.0 loss: 0.5652067672747833 \n",
            "Epoch:  4\n",
            "52/64.0 loss: 0.5618010112699473 \n",
            "Epoch:  4\n",
            "53/64.0 loss: 0.5607978696072543 \n",
            "Epoch:  4\n",
            "54/64.0 loss: 0.5609629929065705 \n",
            "Epoch:  4\n",
            "55/64.0 loss: 0.5583026872149536 \n",
            "Epoch:  4\n",
            "56/64.0 loss: 0.5607888264614239 \n",
            "Epoch:  4\n",
            "57/64.0 loss: 0.5604026990717855 \n",
            "Epoch:  4\n",
            "58/64.0 loss: 0.5643668189897375 \n",
            "Epoch:  4\n",
            "59/64.0 loss: 0.5671034162243207 \n",
            "Epoch:  4\n",
            "60/64.0 loss: 0.5676622718084053 \n",
            "Epoch:  4\n",
            "61/64.0 loss: 0.5664581151739243 \n",
            "Epoch:  4\n",
            "62/64.0 loss: 0.567320661412345 \n",
            "Epoch:  4\n",
            "63/64.0 loss: 0.5679512512870133 \n",
            "Epoch:  5\n",
            "0/64.0 loss: 0.5381878018379211 \n",
            "Epoch:  5\n",
            "1/64.0 loss: 0.601237565279007 \n",
            "Epoch:  5\n",
            "2/64.0 loss: 0.5660819907983144 \n",
            "Epoch:  5\n",
            "3/64.0 loss: 0.564609982073307 \n",
            "Epoch:  5\n",
            "4/64.0 loss: 0.5606782615184784 \n",
            "Epoch:  5\n",
            "5/64.0 loss: 0.5491972515980402 \n",
            "Epoch:  5\n",
            "6/64.0 loss: 0.5619419770581382 \n",
            "Epoch:  5\n",
            "7/64.0 loss: 0.5703299529850483 \n",
            "Epoch:  5\n",
            "8/64.0 loss: 0.5667082766691843 \n",
            "Epoch:  5\n",
            "9/64.0 loss: 0.5606902748346329 \n",
            "Epoch:  5\n",
            "10/64.0 loss: 0.5716861080039631 \n",
            "Epoch:  5\n",
            "11/64.0 loss: 0.5939668541153272 \n",
            "Epoch:  5\n",
            "12/64.0 loss: 0.582336058983436 \n",
            "Epoch:  5\n",
            "13/64.0 loss: 0.5752435582024711 \n",
            "Epoch:  5\n",
            "14/64.0 loss: 0.5837868928909302 \n",
            "Epoch:  5\n",
            "15/64.0 loss: 0.5821013562381268 \n",
            "Epoch:  5\n",
            "16/64.0 loss: 0.5836586601593915 \n",
            "Epoch:  5\n",
            "17/64.0 loss: 0.5809799234072367 \n",
            "Epoch:  5\n",
            "18/64.0 loss: 0.5828777771247061 \n",
            "Epoch:  5\n",
            "19/64.0 loss: 0.5843460202217102 \n",
            "Epoch:  5\n",
            "20/64.0 loss: 0.583859384059906 \n",
            "Epoch:  5\n",
            "21/64.0 loss: 0.5813817246393724 \n",
            "Epoch:  5\n",
            "22/64.0 loss: 0.5813244322071904 \n",
            "Epoch:  5\n",
            "23/64.0 loss: 0.582276942829291 \n",
            "Epoch:  5\n",
            "24/64.0 loss: 0.5826077532768249 \n",
            "Epoch:  5\n",
            "25/64.0 loss: 0.5852367648711572 \n",
            "Epoch:  5\n",
            "26/64.0 loss: 0.5779066118929121 \n",
            "Epoch:  5\n",
            "27/64.0 loss: 0.577447489968368 \n",
            "Epoch:  5\n",
            "28/64.0 loss: 0.5709809229291719 \n",
            "Epoch:  5\n",
            "29/64.0 loss: 0.5695451915264129 \n",
            "Epoch:  5\n",
            "30/64.0 loss: 0.5652619657977935 \n",
            "Epoch:  5\n",
            "31/64.0 loss: 0.5608646394684911 \n",
            "Epoch:  5\n",
            "32/64.0 loss: 0.563017459529819 \n",
            "Epoch:  5\n",
            "33/64.0 loss: 0.5587394027148976 \n",
            "Epoch:  5\n",
            "34/64.0 loss: 0.5528108997004373 \n",
            "Epoch:  5\n",
            "35/64.0 loss: 0.5551551058888435 \n",
            "Epoch:  5\n",
            "36/64.0 loss: 0.5500206520428529 \n",
            "Epoch:  5\n",
            "37/64.0 loss: 0.5485924372547551 \n",
            "Epoch:  5\n",
            "38/64.0 loss: 0.5474687554897406 \n",
            "Epoch:  5\n",
            "39/64.0 loss: 0.5536963492631912 \n",
            "Epoch:  5\n",
            "40/64.0 loss: 0.5486105636852544 \n",
            "Epoch:  5\n",
            "41/64.0 loss: 0.5460795284736724 \n",
            "Epoch:  5\n",
            "42/64.0 loss: 0.548792595780173 \n",
            "Epoch:  5\n",
            "43/64.0 loss: 0.5481306632811372 \n",
            "Epoch:  5\n",
            "44/64.0 loss: 0.5509930855698055 \n",
            "Epoch:  5\n",
            "45/64.0 loss: 0.5559818543817686 \n",
            "Epoch:  5\n",
            "46/64.0 loss: 0.5582140842650799 \n",
            "Epoch:  5\n",
            "47/64.0 loss: 0.5577024438728889 \n",
            "Epoch:  5\n",
            "48/64.0 loss: 0.5526310539975459 \n",
            "Epoch:  5\n",
            "49/64.0 loss: 0.5561267024278641 \n",
            "Epoch:  5\n",
            "50/64.0 loss: 0.5560574595834694 \n",
            "Epoch:  5\n",
            "51/64.0 loss: 0.5556012145601786 \n",
            "Epoch:  5\n",
            "52/64.0 loss: 0.5543552282846199 \n",
            "Epoch:  5\n",
            "53/64.0 loss: 0.5547652437731072 \n",
            "Epoch:  5\n",
            "54/64.0 loss: 0.5563262088732286 \n",
            "Epoch:  5\n",
            "55/64.0 loss: 0.5573969978306975 \n",
            "Epoch:  5\n",
            "56/64.0 loss: 0.5563469252042603 \n",
            "Epoch:  5\n",
            "57/64.0 loss: 0.5568482408235813 \n",
            "Epoch:  5\n",
            "58/64.0 loss: 0.5566758356862149 \n",
            "Epoch:  5\n",
            "59/64.0 loss: 0.556960006058216 \n",
            "Epoch:  5\n",
            "60/64.0 loss: 0.5554592370009813 \n",
            "Epoch:  5\n",
            "61/64.0 loss: 0.5571154918401472 \n",
            "Epoch:  5\n",
            "62/64.0 loss: 0.558394046529891 \n",
            "Epoch:  5\n",
            "63/64.0 loss: 0.5590550149790943 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZffdI9Wn-dT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0891862a-eef7-43cf-c094-c93c27b0774f"
      },
      "source": [
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        # The new model has slightly different outputs. A sigmoid() is applied to logits to bound between 0 and 1\n",
        "        loss = loss_func(logits.sigmoid(), labels)\n",
        "        numpy_logits = logits.cpu().detach().numpy()\n",
        "        \n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "        all_logits += list(numpy_logits[:, 0])\n",
        "        \n",
        "print(classification_report(test_y, bert_predicted))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.54      1.00      0.70        85\n",
            "        True       1.00      0.03      0.05        75\n",
            "\n",
            "    accuracy                           0.54       160\n",
            "   macro avg       0.77      0.51      0.38       160\n",
            "weighted avg       0.75      0.54      0.40       160\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}