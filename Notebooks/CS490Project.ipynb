{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS490Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Johoodcoder/CS490Project/blob/whittington/Notebooks/CS490Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTZbJ1SJ53XO"
      },
      "source": [
        "Non-preinstalled module installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm5_ujD458U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576e62d8-c902-47f8-9aec-24a0f5aecbd5"
      },
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install pytorch-nlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 29.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 22.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 25.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 27.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 19.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 18.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 18.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 18.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 18.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.0+cu101)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/bd/3f9cc87a8faa561903644ec6ef7e7e408ca3640e77c5944124ad6adbaecd/boto3-1.17.39-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Collecting botocore<1.21.0,>=1.20.39\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/ad/abdc982cb695a20764df007a2d7cb0ac8964c9591fd014006e40334e4a74/botocore-1.20.39-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 52.8MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.39->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.39->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.39 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.17.39 botocore-1.20.39 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.6\n",
            "Collecting pytorch-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (1.19.5)\n",
            "Installing collected packages: pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw-rU23d7kyr"
      },
      "source": [
        "Import Dataset used in https://towardsdatascience.com/fake-news-classification-with-bert-afbeee601f41"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PN2iZvR7xuA"
      },
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vvyvLZY3Lnl"
      },
      "source": [
        "The base code from https://github.com/spierre91/medium_code/blob/master/fake_news_classifcation.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbufH4ZX8X5N"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import torch.nn as nn\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "import torch\n",
        "from torchnlp.datasets import imdb_dataset\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report\n",
        "import gc"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a-5pyC08dzO",
        "outputId": "3a7e42ea-42be-4e83-b0e7-e9451b2cf56e"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "train_data, test_data = imdb_dataset(train=True, test=True)\n",
        "#df = pd.read_csv(\"condensed_fake_real_news_SANITIZED.csv\")\n",
        "df = pd.read_csv(\"fnn_train.csv\")\n",
        "#df = df[['text', 'type']]\n",
        "df = df[['fullText_based_content', 'label_fnn']]\n",
        "print(len(df))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 84.1MB [00:03, 22.9MB/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7MfTPO18kw8",
        "outputId": "f103bf93-3f6b-4df7-bfea-9c14b84e9168"
      },
      "source": [
        "from collections import Counter \n",
        "\n",
        "#print(Counter(df['type'].values))\n",
        "print(Counter(df['label_fnn'].values))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'fake': 7621, 'real': 7591})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HHnQVRq8z0n",
        "outputId": "53fa2c3c-f890-44ee-aa1e-d59e2962cdd8"
      },
      "source": [
        "df = df[df['label_fnn'].isin(['fake', 'real'])]\n",
        "#df = df[df['type'].isin(['fake', 'real'])]\n",
        "df.dropna(inplace = True)\n",
        "df = df.sample(frac=1, random_state = 24).reset_index(drop=True)\n",
        "\n",
        "#print(Counter(df['type'].values))\n",
        "print(Counter(df['label_fnn'].values))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'fake': 7621, 'real': 7591})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wOwpOde8_WC",
        "outputId": "b8dbaf9e-81ee-4de6-fea3-185c47cdb687"
      },
      "source": [
        "train_data_df = df.head(3200)\n",
        "test_data_df = df.tail(800)\n",
        "print(train_data_df)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                 fullText_based_content label_fnn\n",
            "0     The country's economic woes were a somber topi...      real\n",
            "1     The book \"End of Days\" by Sylvia Browne explor...      fake\n",
            "2     Partisan narratives that emerged after the dea...      fake\n",
            "3     The president’s political opponents have \"been...      fake\n",
            "4     In the final days before the election, the Oba...      real\n",
            "...                                                 ...       ...\n",
            "3195  Rumors and news reports citing anonymous sourc...      fake\n",
            "3196  Democratic presidential candidate Hillary Clin...      real\n",
            "3197  Cormick Lynch, a Republican running to replace...      real\n",
            "3198  In a U.S. population of 329 million, is it pos...      fake\n",
            "3199  A political flier directed to voters in north ...      fake\n",
            "\n",
            "[3200 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bborPzYM9CUR"
      },
      "source": [
        "# train_data = []\n",
        "# for index, row in train_data_df.iterrows():\n",
        "#     train_data.append({'text': row['text'], 'type': row['type']})\n",
        "\n",
        "# test_data = []\n",
        "# for index, row in test_data_df.iterrows():\n",
        "#     test_data.append({'text': row['text'], 'type': row['type']})\n",
        "\n",
        "train_data = []\n",
        "for index, row in train_data_df.iterrows():\n",
        "    train_data.append({'fullText_based_content': row['fullText_based_content'], 'label_fnn': row['label_fnn']})\n",
        "\n",
        "test_data = []\n",
        "for index, row in test_data_df.iterrows():\n",
        "    test_data.append({'fullText_based_content': row['fullText_based_content'], 'label_fnn': row['label_fnn']})\n",
        "\n",
        "\n",
        "# train_data = [{'text': text, 'type': type_data } for text in list(train_data['text']) for type_data in list(train_data['type'])]\n",
        "# test_data = [{'text': text, 'type': type_data } for text in list(test_data['text']) for type_data in list(test_data['type'])]\n",
        "# #gc.collect()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G7zoLaF9gNf"
      },
      "source": [
        "# train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['type']), train_data)))\n",
        "# test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['type']), test_data)))\n",
        "\n",
        "train_texts, train_labels = list(zip(*map(lambda d: (d['fullText_based_content'], d['label_fnn']), train_data)))\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['fullText_based_content'], d['label_fnn']), test_data)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KNrngW99ooH",
        "outputId": "8a7d9742-5809-44e5-c85e-e414b9510157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\n",
        "\n",
        "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
        "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
        "\n",
        "\n",
        "\n",
        "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 16468207.70B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAKhviVo9ujZ"
      },
      "source": [
        "train_y = np.array(train_labels) == 'fake'\n",
        "test_y = np.array(test_labels) == 'fake'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWK3NwRHdFbi",
        "outputId": "0bd57ca8-f54a-4627-b08d-682029847258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(train_y)\n",
        "print(test_y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False  True  True ... False  True  True]\n",
            "[ True False False  True  True False False  True  True  True False False\n",
            "  True False False  True False  True False  True False False  True False\n",
            " False  True  True  True False False False  True False False False False\n",
            "  True  True False False  True False  True  True False False False False\n",
            "  True  True False  True  True  True  True False False False  True  True\n",
            " False  True  True False  True False  True False False False  True False\n",
            "  True  True  True False False  True False  True False False  True False\n",
            "  True False False  True False  True False False False False  True False\n",
            " False False False False  True  True False  True  True False  True False\n",
            "  True  True  True False False False False  True  True False False False\n",
            "  True False False False  True False False False  True  True False  True\n",
            " False  True  True False  True False  True  True False  True  True False\n",
            " False False False  True False False  True False False False False  True\n",
            "  True False  True False  True  True False False  True False  True False\n",
            " False False False  True False  True False False False False False False\n",
            "  True False False  True False  True  True  True False  True False False\n",
            " False  True False  True False  True  True False False  True False  True\n",
            " False  True False  True  True  True False  True False False  True  True\n",
            " False False False  True  True  True False  True  True False False False\n",
            "  True  True False False  True False False  True  True False False False\n",
            "  True  True False False  True  True  True  True  True False  True  True\n",
            " False  True  True  True  True False  True False False False  True False\n",
            "  True False  True False  True False False  True False False False False\n",
            "  True  True False False  True  True False False False False False False\n",
            " False False False  True False False False  True False  True  True  True\n",
            " False False  True  True  True  True  True  True  True  True False  True\n",
            "  True False  True False  True  True  True False  True  True  True  True\n",
            " False False False False False False False False False  True  True False\n",
            "  True False  True  True  True False False  True False False False  True\n",
            "  True False False  True False False False False  True False False False\n",
            "  True  True  True False  True False  True False  True  True  True False\n",
            " False  True False False  True False False False False False  True  True\n",
            "  True  True  True False False False False  True  True  True False  True\n",
            "  True  True  True  True  True  True  True False  True False  True  True\n",
            "  True  True False  True  True  True  True  True False  True  True  True\n",
            " False  True False  True False False False  True  True False  True False\n",
            "  True False  True  True  True  True False False  True False  True  True\n",
            " False False False  True False False False  True  True  True  True False\n",
            " False  True  True False  True  True  True False  True  True  True False\n",
            " False False False False  True False  True  True  True False False  True\n",
            " False  True False False  True False False False False  True False  True\n",
            " False False  True False False  True  True  True False False False  True\n",
            "  True False False  True False False  True  True  True False False False\n",
            "  True False False False False False  True  True  True  True False False\n",
            " False False  True  True  True False False  True False  True False False\n",
            "  True False False False  True False  True False  True  True False  True\n",
            " False False  True False  True  True  True False  True False  True  True\n",
            " False  True  True  True False False  True  True False  True False False\n",
            "  True False  True False False False False False False False False False\n",
            " False False False  True False False False  True  True False False  True\n",
            " False False False  True  True  True  True  True False  True  True False\n",
            " False False False False False False False False  True False  True  True\n",
            " False False  True False  True False  True False  True False  True  True\n",
            "  True False False False  True  True False False  True  True  True False\n",
            " False False  True False False  True False  True  True False  True False\n",
            "  True  True False  True False False False False False False False  True\n",
            " False  True False False False False  True  True  True False  True  True\n",
            " False False  True False False False False False  True  True False  True\n",
            "  True  True False  True False False False False  True  True  True  True\n",
            " False False  True False False False  True  True  True  True False  True\n",
            "  True False  True  True False  True False False  True False  True False\n",
            "  True  True False False False  True  True False  True  True False  True\n",
            " False False  True  True  True False  True False False False  True False\n",
            " False False  True False False False  True False False False  True  True\n",
            " False  True  True  True False False False  True False False  True False\n",
            "  True  True False False False  True  True False False False  True False\n",
            "  True  True  True False False False  True False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKVuGoMm9wXe"
      },
      "source": [
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens, masks=None):\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjP7PqON92FH"
      },
      "source": [
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1poQHOBe-DM6"
      },
      "source": [
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "train_masks_tensor = train_masks_tensor.to('cuda')\n",
        "test_masks_tensor = test_masks_tensor.to('cuda')\n",
        "\n",
        "train_tokens_tensor = train_tokens_tensor.to('cuda')\n",
        "test_tokens_tensor = test_tokens_tensor.to('cuda')\n",
        "train_y_tensor = train_y_tensor.to('cuda')\n",
        "test_y_tensor = test_y_tensor.to('cuda')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VROZ-KLF7oFn"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "EPOCHS = 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kHKfGwF-DZp"
      },
      "source": [
        "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
        "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
        "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kdg4sqN-DkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e1f44c-f610-4033-b24e-0a2ca299ce39"
      },
      "source": [
        "bert_clf = BertBinaryClassifier()\n",
        "bert_clf.to('cuda')\n",
        "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-5)\n",
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "        probas = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        batch_loss = loss_func(probas, labels)\n",
        "        train_loss += batch_loss.item()\n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:06<00:00, 63050072.23B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "0/400.0 loss: 0.642948567867279 \n",
            "Epoch:  1\n",
            "1/400.0 loss: 0.688322126865387 \n",
            "Epoch:  1\n",
            "2/400.0 loss: 0.6892523964246114 \n",
            "Epoch:  1\n",
            "3/400.0 loss: 0.6889333575963974 \n",
            "Epoch:  1\n",
            "4/400.0 loss: 0.6882847547531128 \n",
            "Epoch:  1\n",
            "5/400.0 loss: 0.6894421378771464 \n",
            "Epoch:  1\n",
            "6/400.0 loss: 0.6891220808029175 \n",
            "Epoch:  1\n",
            "7/400.0 loss: 0.6973537281155586 \n",
            "Epoch:  1\n",
            "8/400.0 loss: 0.7012863887680901 \n",
            "Epoch:  1\n",
            "9/400.0 loss: 0.6953596770763397 \n",
            "Epoch:  1\n",
            "10/400.0 loss: 0.6939443078908053 \n",
            "Epoch:  1\n",
            "11/400.0 loss: 0.6912013043959936 \n",
            "Epoch:  1\n",
            "12/400.0 loss: 0.6993320675996634 \n",
            "Epoch:  1\n",
            "13/400.0 loss: 0.7007723578384945 \n",
            "Epoch:  1\n",
            "14/400.0 loss: 0.7033030549685161 \n",
            "Epoch:  1\n",
            "15/400.0 loss: 0.7063280716538429 \n",
            "Epoch:  1\n",
            "16/400.0 loss: 0.7035118060953477 \n",
            "Epoch:  1\n",
            "17/400.0 loss: 0.7006418771213956 \n",
            "Epoch:  1\n",
            "18/400.0 loss: 0.6983312242909482 \n",
            "Epoch:  1\n",
            "19/400.0 loss: 0.6967587977647781 \n",
            "Epoch:  1\n",
            "20/400.0 loss: 0.6958661732219514 \n",
            "Epoch:  1\n",
            "21/400.0 loss: 0.6975276957858693 \n",
            "Epoch:  1\n",
            "22/400.0 loss: 0.6944373275922693 \n",
            "Epoch:  1\n",
            "23/400.0 loss: 0.6946969876686732 \n",
            "Epoch:  1\n",
            "24/400.0 loss: 0.6945375680923462 \n",
            "Epoch:  1\n",
            "25/400.0 loss: 0.6958902799166166 \n",
            "Epoch:  1\n",
            "26/400.0 loss: 0.6940524798852427 \n",
            "Epoch:  1\n",
            "27/400.0 loss: 0.69021894463471 \n",
            "Epoch:  1\n",
            "28/400.0 loss: 0.6951683780242657 \n",
            "Epoch:  1\n",
            "29/400.0 loss: 0.6968496362368266 \n",
            "Epoch:  1\n",
            "30/400.0 loss: 0.6964558516779253 \n",
            "Epoch:  1\n",
            "31/400.0 loss: 0.696048790588975 \n",
            "Epoch:  1\n",
            "32/400.0 loss: 0.6953923720302004 \n",
            "Epoch:  1\n",
            "33/400.0 loss: 0.696587753646514 \n",
            "Epoch:  1\n",
            "34/400.0 loss: 0.6951528481074742 \n",
            "Epoch:  1\n",
            "35/400.0 loss: 0.6967247923215231 \n",
            "Epoch:  1\n",
            "36/400.0 loss: 0.6952268916207391 \n",
            "Epoch:  1\n",
            "37/400.0 loss: 0.695561408996582 \n",
            "Epoch:  1\n",
            "38/400.0 loss: 0.6940459991112734 \n",
            "Epoch:  1\n",
            "39/400.0 loss: 0.6946406990289689 \n",
            "Epoch:  1\n",
            "40/400.0 loss: 0.6946715261878037 \n",
            "Epoch:  1\n",
            "41/400.0 loss: 0.6949847922438667 \n",
            "Epoch:  1\n",
            "42/400.0 loss: 0.6961454513461091 \n",
            "Epoch:  1\n",
            "43/400.0 loss: 0.6960134438493035 \n",
            "Epoch:  1\n",
            "44/400.0 loss: 0.6953890707757738 \n",
            "Epoch:  1\n",
            "45/400.0 loss: 0.6952284807744233 \n",
            "Epoch:  1\n",
            "46/400.0 loss: 0.6952261658425026 \n",
            "Epoch:  1\n",
            "47/400.0 loss: 0.6949468900760015 \n",
            "Epoch:  1\n",
            "48/400.0 loss: 0.6958174583863239 \n",
            "Epoch:  1\n",
            "49/400.0 loss: 0.6952191185951233 \n",
            "Epoch:  1\n",
            "50/400.0 loss: 0.6952644598250296 \n",
            "Epoch:  1\n",
            "51/400.0 loss: 0.6954379643385227 \n",
            "Epoch:  1\n",
            "52/400.0 loss: 0.6951961348641593 \n",
            "Epoch:  1\n",
            "53/400.0 loss: 0.6952566383061586 \n",
            "Epoch:  1\n",
            "54/400.0 loss: 0.6950570355762135 \n",
            "Epoch:  1\n",
            "55/400.0 loss: 0.6955451188342912 \n",
            "Epoch:  1\n",
            "56/400.0 loss: 0.6957836266149554 \n",
            "Epoch:  1\n",
            "57/400.0 loss: 0.695338675688053 \n",
            "Epoch:  1\n",
            "58/400.0 loss: 0.6954168960199518 \n",
            "Epoch:  1\n",
            "59/400.0 loss: 0.6952241152524948 \n",
            "Epoch:  1\n",
            "60/400.0 loss: 0.6954476168898286 \n",
            "Epoch:  1\n",
            "61/400.0 loss: 0.6963564743918758 \n",
            "Epoch:  1\n",
            "62/400.0 loss: 0.6963174087660653 \n",
            "Epoch:  1\n",
            "63/400.0 loss: 0.6958894738927484 \n",
            "Epoch:  1\n",
            "64/400.0 loss: 0.6955248695153456 \n",
            "Epoch:  1\n",
            "65/400.0 loss: 0.6950485001910817 \n",
            "Epoch:  1\n",
            "66/400.0 loss: 0.6952868006122646 \n",
            "Epoch:  1\n",
            "67/400.0 loss: 0.6955771560178083 \n",
            "Epoch:  1\n",
            "68/400.0 loss: 0.6954781171204387 \n",
            "Epoch:  1\n",
            "69/400.0 loss: 0.6956269732543401 \n",
            "Epoch:  1\n",
            "70/400.0 loss: 0.6956566672929576 \n",
            "Epoch:  1\n",
            "71/400.0 loss: 0.6953052348560758 \n",
            "Epoch:  1\n",
            "72/400.0 loss: 0.6950173549456139 \n",
            "Epoch:  1\n",
            "73/400.0 loss: 0.6948713716622945 \n",
            "Epoch:  1\n",
            "74/400.0 loss: 0.6946296906471252 \n",
            "Epoch:  1\n",
            "75/400.0 loss: 0.6950484835787824 \n",
            "Epoch:  1\n",
            "76/400.0 loss: 0.6947884087438707 \n",
            "Epoch:  1\n",
            "77/400.0 loss: 0.6945902712834187 \n",
            "Epoch:  1\n",
            "78/400.0 loss: 0.6948438837558408 \n",
            "Epoch:  1\n",
            "79/400.0 loss: 0.6950244694948197 \n",
            "Epoch:  1\n",
            "80/400.0 loss: 0.6947738609196227 \n",
            "Epoch:  1\n",
            "81/400.0 loss: 0.6946113574795607 \n",
            "Epoch:  1\n",
            "82/400.0 loss: 0.6943132547010858 \n",
            "Epoch:  1\n",
            "83/400.0 loss: 0.6940238858972277 \n",
            "Epoch:  1\n",
            "84/400.0 loss: 0.6942829952520483 \n",
            "Epoch:  1\n",
            "85/400.0 loss: 0.6937950101009634 \n",
            "Epoch:  1\n",
            "86/400.0 loss: 0.6943223914880862 \n",
            "Epoch:  1\n",
            "87/400.0 loss: 0.6944295032457872 \n",
            "Epoch:  1\n",
            "88/400.0 loss: 0.6947341882780697 \n",
            "Epoch:  1\n",
            "89/400.0 loss: 0.6948419749736786 \n",
            "Epoch:  1\n",
            "90/400.0 loss: 0.6944868571155673 \n",
            "Epoch:  1\n",
            "91/400.0 loss: 0.694634598882302 \n",
            "Epoch:  1\n",
            "92/400.0 loss: 0.6949707622169167 \n",
            "Epoch:  1\n",
            "93/400.0 loss: 0.6949109752127465 \n",
            "Epoch:  1\n",
            "94/400.0 loss: 0.6945351362228394 \n",
            "Epoch:  1\n",
            "95/400.0 loss: 0.6951304227113724 \n",
            "Epoch:  1\n",
            "96/400.0 loss: 0.6950604632957694 \n",
            "Epoch:  1\n",
            "97/400.0 loss: 0.6952213936922501 \n",
            "Epoch:  1\n",
            "98/400.0 loss: 0.6953036086727874 \n",
            "Epoch:  1\n",
            "99/400.0 loss: 0.6955150687694549 \n",
            "Epoch:  1\n",
            "100/400.0 loss: 0.6956488956319223 \n",
            "Epoch:  1\n",
            "101/400.0 loss: 0.6957630211231756 \n",
            "Epoch:  1\n",
            "102/400.0 loss: 0.6960150709429991 \n",
            "Epoch:  1\n",
            "103/400.0 loss: 0.6962392066533749 \n",
            "Epoch:  1\n",
            "104/400.0 loss: 0.696497566927047 \n",
            "Epoch:  1\n",
            "105/400.0 loss: 0.6966307551231025 \n",
            "Epoch:  1\n",
            "106/400.0 loss: 0.6967966138759506 \n",
            "Epoch:  1\n",
            "107/400.0 loss: 0.6966678064178538 \n",
            "Epoch:  1\n",
            "108/400.0 loss: 0.6966099132091628 \n",
            "Epoch:  1\n",
            "109/400.0 loss: 0.6964385363188657 \n",
            "Epoch:  1\n",
            "110/400.0 loss: 0.6964051224089958 \n",
            "Epoch:  1\n",
            "111/400.0 loss: 0.696355010249785 \n",
            "Epoch:  1\n",
            "112/400.0 loss: 0.6964719986493608 \n",
            "Epoch:  1\n",
            "113/400.0 loss: 0.6961447915487122 \n",
            "Epoch:  1\n",
            "114/400.0 loss: 0.6958678354387698 \n",
            "Epoch:  1\n",
            "115/400.0 loss: 0.6958166437930074 \n",
            "Epoch:  1\n",
            "116/400.0 loss: 0.6955672797993717 \n",
            "Epoch:  1\n",
            "117/400.0 loss: 0.6950664590981047 \n",
            "Epoch:  1\n",
            "118/400.0 loss: 0.6947842185236827 \n",
            "Epoch:  1\n",
            "119/400.0 loss: 0.6946245114008586 \n",
            "Epoch:  1\n",
            "120/400.0 loss: 0.6947532498146877 \n",
            "Epoch:  1\n",
            "121/400.0 loss: 0.694620032779506 \n",
            "Epoch:  1\n",
            "122/400.0 loss: 0.6946524381637573 \n",
            "Epoch:  1\n",
            "123/400.0 loss: 0.6952892287123588 \n",
            "Epoch:  1\n",
            "124/400.0 loss: 0.6959687614440918 \n",
            "Epoch:  1\n",
            "125/400.0 loss: 0.6963576078414917 \n",
            "Epoch:  1\n",
            "126/400.0 loss: 0.6964806158711591 \n",
            "Epoch:  1\n",
            "127/400.0 loss: 0.6961941905319691 \n",
            "Epoch:  1\n",
            "128/400.0 loss: 0.695880251799443 \n",
            "Epoch:  1\n",
            "129/400.0 loss: 0.6961374525840466 \n",
            "Epoch:  1\n",
            "130/400.0 loss: 0.6964138991960133 \n",
            "Epoch:  1\n",
            "131/400.0 loss: 0.696532026384816 \n",
            "Epoch:  1\n",
            "132/400.0 loss: 0.6964221663941118 \n",
            "Epoch:  1\n",
            "133/400.0 loss: 0.6963797967825363 \n",
            "Epoch:  1\n",
            "134/400.0 loss: 0.6962114665243361 \n",
            "Epoch:  1\n",
            "135/400.0 loss: 0.6962490901350975 \n",
            "Epoch:  1\n",
            "136/400.0 loss: 0.6962379821895683 \n",
            "Epoch:  1\n",
            "137/400.0 loss: 0.6962759887826615 \n",
            "Epoch:  1\n",
            "138/400.0 loss: 0.6962002640147861 \n",
            "Epoch:  1\n",
            "139/400.0 loss: 0.696311475123678 \n",
            "Epoch:  1\n",
            "140/400.0 loss: 0.6960821629416013 \n",
            "Epoch:  1\n",
            "141/400.0 loss: 0.6959494301970576 \n",
            "Epoch:  1\n",
            "142/400.0 loss: 0.6957707180009856 \n",
            "Epoch:  1\n",
            "143/400.0 loss: 0.6956780180335045 \n",
            "Epoch:  1\n",
            "144/400.0 loss: 0.6953951708201704 \n",
            "Epoch:  1\n",
            "145/400.0 loss: 0.6953347321242502 \n",
            "Epoch:  1\n",
            "146/400.0 loss: 0.6954756537262274 \n",
            "Epoch:  1\n",
            "147/400.0 loss: 0.6954297221995689 \n",
            "Epoch:  1\n",
            "148/400.0 loss: 0.6955447304968866 \n",
            "Epoch:  1\n",
            "149/400.0 loss: 0.6956302642822265 \n",
            "Epoch:  1\n",
            "150/400.0 loss: 0.6957797033107833 \n",
            "Epoch:  1\n",
            "151/400.0 loss: 0.695958928058022 \n",
            "Epoch:  1\n",
            "152/400.0 loss: 0.6960416315427793 \n",
            "Epoch:  1\n",
            "153/400.0 loss: 0.6961062527322149 \n",
            "Epoch:  1\n",
            "154/400.0 loss: 0.6960936823198872 \n",
            "Epoch:  1\n",
            "155/400.0 loss: 0.6957524346235471 \n",
            "Epoch:  1\n",
            "156/400.0 loss: 0.6958419767914304 \n",
            "Epoch:  1\n",
            "157/400.0 loss: 0.6954780915115453 \n",
            "Epoch:  1\n",
            "158/400.0 loss: 0.695738785671738 \n",
            "Epoch:  1\n",
            "159/400.0 loss: 0.6958577003329992 \n",
            "Epoch:  1\n",
            "160/400.0 loss: 0.695776026811659 \n",
            "Epoch:  1\n",
            "161/400.0 loss: 0.6954986096164326 \n",
            "Epoch:  1\n",
            "162/400.0 loss: 0.6955557641076164 \n",
            "Epoch:  1\n",
            "163/400.0 loss: 0.6953525823063966 \n",
            "Epoch:  1\n",
            "164/400.0 loss: 0.6949281053109603 \n",
            "Epoch:  1\n",
            "165/400.0 loss: 0.6952814876315105 \n",
            "Epoch:  1\n",
            "166/400.0 loss: 0.6956598280432695 \n",
            "Epoch:  1\n",
            "167/400.0 loss: 0.6950924964178176 \n",
            "Epoch:  1\n",
            "168/400.0 loss: 0.694667189784304 \n",
            "Epoch:  1\n",
            "169/400.0 loss: 0.694507089432548 \n",
            "Epoch:  1\n",
            "170/400.0 loss: 0.6948432437857689 \n",
            "Epoch:  1\n",
            "171/400.0 loss: 0.6952948736590009 \n",
            "Epoch:  1\n",
            "172/400.0 loss: 0.6954373803441924 \n",
            "Epoch:  1\n",
            "173/400.0 loss: 0.6951398746720676 \n",
            "Epoch:  1\n",
            "174/400.0 loss: 0.6951544683320182 \n",
            "Epoch:  1\n",
            "175/400.0 loss: 0.6952317590740594 \n",
            "Epoch:  1\n",
            "176/400.0 loss: 0.6954011004523369 \n",
            "Epoch:  1\n",
            "177/400.0 loss: 0.6951193464605996 \n",
            "Epoch:  1\n",
            "178/400.0 loss: 0.6949371519035468 \n",
            "Epoch:  1\n",
            "179/400.0 loss: 0.6949500666724311 \n",
            "Epoch:  1\n",
            "180/400.0 loss: 0.694962969142429 \n",
            "Epoch:  1\n",
            "181/400.0 loss: 0.6950997323780269 \n",
            "Epoch:  1\n",
            "182/400.0 loss: 0.6951683160385799 \n",
            "Epoch:  1\n",
            "183/400.0 loss: 0.6950038111080294 \n",
            "Epoch:  1\n",
            "184/400.0 loss: 0.6950270308030618 \n",
            "Epoch:  1\n",
            "185/400.0 loss: 0.6949513750050658 \n",
            "Epoch:  1\n",
            "186/400.0 loss: 0.6950254723987478 \n",
            "Epoch:  1\n",
            "187/400.0 loss: 0.6948914150608346 \n",
            "Epoch:  1\n",
            "188/400.0 loss: 0.6948347498500158 \n",
            "Epoch:  1\n",
            "189/400.0 loss: 0.6947316279536799 \n",
            "Epoch:  1\n",
            "190/400.0 loss: 0.6948740188988092 \n",
            "Epoch:  1\n",
            "191/400.0 loss: 0.6947926751648387 \n",
            "Epoch:  1\n",
            "192/400.0 loss: 0.694800834581642 \n",
            "Epoch:  1\n",
            "193/400.0 loss: 0.6947980627571184 \n",
            "Epoch:  1\n",
            "194/400.0 loss: 0.6948608878331307 \n",
            "Epoch:  1\n",
            "195/400.0 loss: 0.6940352959292275 \n",
            "Epoch:  1\n",
            "196/400.0 loss: 0.6939562090157252 \n",
            "Epoch:  1\n",
            "197/400.0 loss: 0.6936037115978472 \n",
            "Epoch:  1\n",
            "198/400.0 loss: 0.6933791598482947 \n",
            "Epoch:  1\n",
            "199/400.0 loss: 0.6935857364535332 \n",
            "Epoch:  1\n",
            "200/400.0 loss: 0.6928684752971972 \n",
            "Epoch:  1\n",
            "201/400.0 loss: 0.6925747881431391 \n",
            "Epoch:  1\n",
            "202/400.0 loss: 0.6923112913305536 \n",
            "Epoch:  1\n",
            "203/400.0 loss: 0.6920827983641157 \n",
            "Epoch:  1\n",
            "204/400.0 loss: 0.6919001567654494 \n",
            "Epoch:  1\n",
            "205/400.0 loss: 0.6918265209035966 \n",
            "Epoch:  1\n",
            "206/400.0 loss: 0.6921901354467236 \n",
            "Epoch:  1\n",
            "207/400.0 loss: 0.6912642233073711 \n",
            "Epoch:  1\n",
            "208/400.0 loss: 0.6912441758447857 \n",
            "Epoch:  1\n",
            "209/400.0 loss: 0.6904373549279712 \n",
            "Epoch:  1\n",
            "210/400.0 loss: 0.6898800767428502 \n",
            "Epoch:  1\n",
            "211/400.0 loss: 0.6896846030118331 \n",
            "Epoch:  1\n",
            "212/400.0 loss: 0.6890109061075488 \n",
            "Epoch:  1\n",
            "213/400.0 loss: 0.6889876968392702 \n",
            "Epoch:  1\n",
            "214/400.0 loss: 0.6891678108725436 \n",
            "Epoch:  1\n",
            "215/400.0 loss: 0.6892960684167014 \n",
            "Epoch:  1\n",
            "216/400.0 loss: 0.6892281040068595 \n",
            "Epoch:  1\n",
            "217/400.0 loss: 0.689916352066425 \n",
            "Epoch:  1\n",
            "218/400.0 loss: 0.6900941563523524 \n",
            "Epoch:  1\n",
            "219/400.0 loss: 0.6900175473906777 \n",
            "Epoch:  1\n",
            "220/400.0 loss: 0.6898585884279795 \n",
            "Epoch:  1\n",
            "221/400.0 loss: 0.6899372457383989 \n",
            "Epoch:  1\n",
            "222/400.0 loss: 0.6899025568513057 \n",
            "Epoch:  1\n",
            "223/400.0 loss: 0.6896710906709943 \n",
            "Epoch:  1\n",
            "224/400.0 loss: 0.689854425324334 \n",
            "Epoch:  1\n",
            "225/400.0 loss: 0.6898033376288625 \n",
            "Epoch:  1\n",
            "226/400.0 loss: 0.6897016929109716 \n",
            "Epoch:  1\n",
            "227/400.0 loss: 0.6896430253982544 \n",
            "Epoch:  1\n",
            "228/400.0 loss: 0.689460377505773 \n",
            "Epoch:  1\n",
            "229/400.0 loss: 0.6893882435301076 \n",
            "Epoch:  1\n",
            "230/400.0 loss: 0.6894103608606181 \n",
            "Epoch:  1\n",
            "231/400.0 loss: 0.6894539710262726 \n",
            "Epoch:  1\n",
            "232/400.0 loss: 0.6891847247729486 \n",
            "Epoch:  1\n",
            "233/400.0 loss: 0.6890887080604194 \n",
            "Epoch:  1\n",
            "234/400.0 loss: 0.6891023744928076 \n",
            "Epoch:  1\n",
            "235/400.0 loss: 0.6893037640947407 \n",
            "Epoch:  1\n",
            "236/400.0 loss: 0.6891509513311749 \n",
            "Epoch:  1\n",
            "237/400.0 loss: 0.6893736021358425 \n",
            "Epoch:  1\n",
            "238/400.0 loss: 0.6894362090022993 \n",
            "Epoch:  1\n",
            "239/400.0 loss: 0.6893756354848544 \n",
            "Epoch:  1\n",
            "240/400.0 loss: 0.6895362123908838 \n",
            "Epoch:  1\n",
            "241/400.0 loss: 0.6894865809393323 \n",
            "Epoch:  1\n",
            "242/400.0 loss: 0.6897697647412618 \n",
            "Epoch:  1\n",
            "243/400.0 loss: 0.690034813323959 \n",
            "Epoch:  1\n",
            "244/400.0 loss: 0.6896903278876324 \n",
            "Epoch:  1\n",
            "245/400.0 loss: 0.6896441445602635 \n",
            "Epoch:  1\n",
            "246/400.0 loss: 0.6892899443746096 \n",
            "Epoch:  1\n",
            "247/400.0 loss: 0.6890883133296044 \n",
            "Epoch:  1\n",
            "248/400.0 loss: 0.6886572052676037 \n",
            "Epoch:  1\n",
            "249/400.0 loss: 0.6882262451648712 \n",
            "Epoch:  1\n",
            "250/400.0 loss: 0.687802682359855 \n",
            "Epoch:  1\n",
            "251/400.0 loss: 0.6878786862842621 \n",
            "Epoch:  1\n",
            "252/400.0 loss: 0.6871994135408063 \n",
            "Epoch:  1\n",
            "253/400.0 loss: 0.6870514628455395 \n",
            "Epoch:  1\n",
            "254/400.0 loss: 0.6864726506027521 \n",
            "Epoch:  1\n",
            "255/400.0 loss: 0.6855057246284559 \n",
            "Epoch:  1\n",
            "256/400.0 loss: 0.6856646956404824 \n",
            "Epoch:  1\n",
            "257/400.0 loss: 0.6876463863507721 \n",
            "Epoch:  1\n",
            "258/400.0 loss: 0.6893868861741541 \n",
            "Epoch:  1\n",
            "259/400.0 loss: 0.6934223593427584 \n",
            "Epoch:  1\n",
            "260/400.0 loss: 0.6949650799405986 \n",
            "Epoch:  1\n",
            "261/400.0 loss: 0.6946034027647426 \n",
            "Epoch:  1\n",
            "262/400.0 loss: 0.695479378387502 \n",
            "Epoch:  1\n",
            "263/400.0 loss: 0.6956024938686327 \n",
            "Epoch:  1\n",
            "264/400.0 loss: 0.695392189948064 \n",
            "Epoch:  1\n",
            "265/400.0 loss: 0.6954180646435659 \n",
            "Epoch:  1\n",
            "266/400.0 loss: 0.6952763812595539 \n",
            "Epoch:  1\n",
            "267/400.0 loss: 0.695194620361079 \n",
            "Epoch:  1\n",
            "268/400.0 loss: 0.6952005692352593 \n",
            "Epoch:  1\n",
            "269/400.0 loss: 0.6950597313819108 \n",
            "Epoch:  1\n",
            "270/400.0 loss: 0.6949019331993652 \n",
            "Epoch:  1\n",
            "271/400.0 loss: 0.6947888143579749 \n",
            "Epoch:  1\n",
            "272/400.0 loss: 0.6949617232813503 \n",
            "Epoch:  1\n",
            "273/400.0 loss: 0.6947883423009928 \n",
            "Epoch:  1\n",
            "274/400.0 loss: 0.6939595031738282 \n",
            "Epoch:  1\n",
            "275/400.0 loss: 0.6941616466079933 \n",
            "Epoch:  1\n",
            "276/400.0 loss: 0.6944632994999524 \n",
            "Epoch:  1\n",
            "277/400.0 loss: 0.6941969105236822 \n",
            "Epoch:  1\n",
            "278/400.0 loss: 0.6936421853667092 \n",
            "Epoch:  1\n",
            "279/400.0 loss: 0.6939091169408389 \n",
            "Epoch:  1\n",
            "280/400.0 loss: 0.6943042047083166 \n",
            "Epoch:  1\n",
            "281/400.0 loss: 0.6940490015855072 \n",
            "Epoch:  1\n",
            "282/400.0 loss: 0.6939901069280536 \n",
            "Epoch:  1\n",
            "283/400.0 loss: 0.6941683940904241 \n",
            "Epoch:  1\n",
            "284/400.0 loss: 0.6940746991257919 \n",
            "Epoch:  1\n",
            "285/400.0 loss: 0.6939541147305415 \n",
            "Epoch:  1\n",
            "286/400.0 loss: 0.693515663778325 \n",
            "Epoch:  1\n",
            "287/400.0 loss: 0.6932953438825078 \n",
            "Epoch:  1\n",
            "288/400.0 loss: 0.6933094749813674 \n",
            "Epoch:  1\n",
            "289/400.0 loss: 0.6932391773010123 \n",
            "Epoch:  1\n",
            "290/400.0 loss: 0.6929082821324929 \n",
            "Epoch:  1\n",
            "291/400.0 loss: 0.6925959770810114 \n",
            "Epoch:  1\n",
            "292/400.0 loss: 0.6923520554861518 \n",
            "Epoch:  1\n",
            "293/400.0 loss: 0.6920361599954618 \n",
            "Epoch:  1\n",
            "294/400.0 loss: 0.691549536737345 \n",
            "Epoch:  1\n",
            "295/400.0 loss: 0.6909978635407783 \n",
            "Epoch:  1\n",
            "296/400.0 loss: 0.6902645120917748 \n",
            "Epoch:  1\n",
            "297/400.0 loss: 0.6904323970111425 \n",
            "Epoch:  1\n",
            "298/400.0 loss: 0.6905319926930111 \n",
            "Epoch:  1\n",
            "299/400.0 loss: 0.6905057888229688 \n",
            "Epoch:  1\n",
            "300/400.0 loss: 0.6908734012085734 \n",
            "Epoch:  1\n",
            "301/400.0 loss: 0.690660321653284 \n",
            "Epoch:  1\n",
            "302/400.0 loss: 0.6910094228318029 \n",
            "Epoch:  1\n",
            "303/400.0 loss: 0.6913031203378188 \n",
            "Epoch:  1\n",
            "304/400.0 loss: 0.6922559889613605 \n",
            "Epoch:  1\n",
            "305/400.0 loss: 0.6922411134624793 \n",
            "Epoch:  1\n",
            "306/400.0 loss: 0.6925584577776321 \n",
            "Epoch:  1\n",
            "307/400.0 loss: 0.6929254389621995 \n",
            "Epoch:  1\n",
            "308/400.0 loss: 0.6924630086205924 \n",
            "Epoch:  1\n",
            "309/400.0 loss: 0.6923373555944812 \n",
            "Epoch:  1\n",
            "310/400.0 loss: 0.6921465384423541 \n",
            "Epoch:  1\n",
            "311/400.0 loss: 0.6923196339645447 \n",
            "Epoch:  1\n",
            "312/400.0 loss: 0.6922293651027801 \n",
            "Epoch:  1\n",
            "313/400.0 loss: 0.6920847529248827 \n",
            "Epoch:  1\n",
            "314/400.0 loss: 0.6919223820406293 \n",
            "Epoch:  1\n",
            "315/400.0 loss: 0.6917054058064388 \n",
            "Epoch:  1\n",
            "316/400.0 loss: 0.691366578220193 \n",
            "Epoch:  1\n",
            "317/400.0 loss: 0.6915024111293396 \n",
            "Epoch:  1\n",
            "318/400.0 loss: 0.6913574419238351 \n",
            "Epoch:  1\n",
            "319/400.0 loss: 0.6911734947003424 \n",
            "Epoch:  1\n",
            "320/400.0 loss: 0.6909920234360799 \n",
            "Epoch:  1\n",
            "321/400.0 loss: 0.6912368511005959 \n",
            "Epoch:  1\n",
            "322/400.0 loss: 0.6913284942646145 \n",
            "Epoch:  1\n",
            "323/400.0 loss: 0.6907185653661504 \n",
            "Epoch:  1\n",
            "324/400.0 loss: 0.6905032164316911 \n",
            "Epoch:  1\n",
            "325/400.0 loss: 0.6906607230565299 \n",
            "Epoch:  1\n",
            "326/400.0 loss: 0.690389173384471 \n",
            "Epoch:  1\n",
            "327/400.0 loss: 0.6900553408010703 \n",
            "Epoch:  1\n",
            "328/400.0 loss: 0.6900664596028603 \n",
            "Epoch:  1\n",
            "329/400.0 loss: 0.6896142487273071 \n",
            "Epoch:  1\n",
            "330/400.0 loss: 0.6893350156952607 \n",
            "Epoch:  1\n",
            "331/400.0 loss: 0.6887529572270003 \n",
            "Epoch:  1\n",
            "332/400.0 loss: 0.6888687028004242 \n",
            "Epoch:  1\n",
            "333/400.0 loss: 0.6885012446227902 \n",
            "Epoch:  1\n",
            "334/400.0 loss: 0.6888756249377976 \n",
            "Epoch:  1\n",
            "335/400.0 loss: 0.6890825397734132 \n",
            "Epoch:  1\n",
            "336/400.0 loss: 0.6882072991362665 \n",
            "Epoch:  1\n",
            "337/400.0 loss: 0.6879992552057526 \n",
            "Epoch:  1\n",
            "338/400.0 loss: 0.687966945600369 \n",
            "Epoch:  1\n",
            "339/400.0 loss: 0.6878508394255357 \n",
            "Epoch:  1\n",
            "340/400.0 loss: 0.6878741593304967 \n",
            "Epoch:  1\n",
            "341/400.0 loss: 0.6878144697487703 \n",
            "Epoch:  1\n",
            "342/400.0 loss: 0.6881143859107015 \n",
            "Epoch:  1\n",
            "343/400.0 loss: 0.6881271867558013 \n",
            "Epoch:  1\n",
            "344/400.0 loss: 0.6883164727169534 \n",
            "Epoch:  1\n",
            "345/400.0 loss: 0.6886500065037281 \n",
            "Epoch:  1\n",
            "346/400.0 loss: 0.6887899147674055 \n",
            "Epoch:  1\n",
            "347/400.0 loss: 0.689156062301548 \n",
            "Epoch:  1\n",
            "348/400.0 loss: 0.6898219435126187 \n",
            "Epoch:  1\n",
            "349/400.0 loss: 0.690031030518668 \n",
            "Epoch:  1\n",
            "350/400.0 loss: 0.6900006002850003 \n",
            "Epoch:  1\n",
            "351/400.0 loss: 0.689920764755119 \n",
            "Epoch:  1\n",
            "352/400.0 loss: 0.6902019893143737 \n",
            "Epoch:  1\n",
            "353/400.0 loss: 0.6898078438589128 \n",
            "Epoch:  1\n",
            "354/400.0 loss: 0.6896628032267933 \n",
            "Epoch:  1\n",
            "355/400.0 loss: 0.690120272924391 \n",
            "Epoch:  1\n",
            "356/400.0 loss: 0.6898715920141097 \n",
            "Epoch:  1\n",
            "357/400.0 loss: 0.6899857257997524 \n",
            "Epoch:  1\n",
            "358/400.0 loss: 0.6898361823020871 \n",
            "Epoch:  1\n",
            "359/400.0 loss: 0.6897292425235112 \n",
            "Epoch:  1\n",
            "360/400.0 loss: 0.6897749002620454 \n",
            "Epoch:  1\n",
            "361/400.0 loss: 0.6897380470570939 \n",
            "Epoch:  1\n",
            "362/400.0 loss: 0.6898400781896817 \n",
            "Epoch:  1\n",
            "363/400.0 loss: 0.6902036062636219 \n",
            "Epoch:  1\n",
            "364/400.0 loss: 0.6900903411107521 \n",
            "Epoch:  1\n",
            "365/400.0 loss: 0.6899640133472088 \n",
            "Epoch:  1\n",
            "366/400.0 loss: 0.6899831148843999 \n",
            "Epoch:  1\n",
            "367/400.0 loss: 0.6897005019304545 \n",
            "Epoch:  1\n",
            "368/400.0 loss: 0.6896021891092543 \n",
            "Epoch:  1\n",
            "369/400.0 loss: 0.689639304940765 \n",
            "Epoch:  1\n",
            "370/400.0 loss: 0.6896384212205995 \n",
            "Epoch:  1\n",
            "371/400.0 loss: 0.6895221489411528 \n",
            "Epoch:  1\n",
            "372/400.0 loss: 0.6896604336298823 \n",
            "Epoch:  1\n",
            "373/400.0 loss: 0.6896028789606962 \n",
            "Epoch:  1\n",
            "374/400.0 loss: 0.6894649877548218 \n",
            "Epoch:  1\n",
            "375/400.0 loss: 0.6893290056193129 \n",
            "Epoch:  1\n",
            "376/400.0 loss: 0.6894034393902483 \n",
            "Epoch:  1\n",
            "377/400.0 loss: 0.6893175707923042 \n",
            "Epoch:  1\n",
            "378/400.0 loss: 0.6896103287120607 \n",
            "Epoch:  1\n",
            "379/400.0 loss: 0.6899303749987954 \n",
            "Epoch:  1\n",
            "380/400.0 loss: 0.6899538690962503 \n",
            "Epoch:  1\n",
            "381/400.0 loss: 0.6900260061493719 \n",
            "Epoch:  1\n",
            "382/400.0 loss: 0.689859076517369 \n",
            "Epoch:  1\n",
            "383/400.0 loss: 0.6900182400519649 \n",
            "Epoch:  1\n",
            "384/400.0 loss: 0.6899610872392531 \n",
            "Epoch:  1\n",
            "385/400.0 loss: 0.6898084520675976 \n",
            "Epoch:  1\n",
            "386/400.0 loss: 0.6898711938266606 \n",
            "Epoch:  1\n",
            "387/400.0 loss: 0.6901580327257668 \n",
            "Epoch:  1\n",
            "388/400.0 loss: 0.6901585487289723 \n",
            "Epoch:  1\n",
            "389/400.0 loss: 0.69008509516716 \n",
            "Epoch:  1\n",
            "390/400.0 loss: 0.6899675010415294 \n",
            "Epoch:  1\n",
            "391/400.0 loss: 0.690056041947433 \n",
            "Epoch:  1\n",
            "392/400.0 loss: 0.6898961958994392 \n",
            "Epoch:  1\n",
            "393/400.0 loss: 0.68974177244351 \n",
            "Epoch:  1\n",
            "394/400.0 loss: 0.6901168240776545 \n",
            "Epoch:  1\n",
            "395/400.0 loss: 0.6902052582514406 \n",
            "Epoch:  1\n",
            "396/400.0 loss: 0.6900773081431161 \n",
            "Epoch:  1\n",
            "397/400.0 loss: 0.6898523554131014 \n",
            "Epoch:  1\n",
            "398/400.0 loss: 0.6896786196787554 \n",
            "Epoch:  1\n",
            "399/400.0 loss: 0.6896378158032894 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZffdI9Wn-dT2",
        "outputId": "f5166b82-7786-4c23-d978-4899f49f0513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        loss = loss_func(logits, labels)\n",
        "        numpy_logits = logits.cpu().detach().numpy()\n",
        "        \n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "        all_logits += list(numpy_logits[:, 0])\n",
        "        \n",
        "print(classification_report(test_y, bert_predicted))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.63      0.76      0.69       429\n",
            "        True       0.63      0.47      0.54       371\n",
            "\n",
            "    accuracy                           0.63       800\n",
            "   macro avg       0.63      0.62      0.62       800\n",
            "weighted avg       0.63      0.63      0.62       800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtaBqUbVAIeS",
        "outputId": "d92b2ad6-5f68-48a8-bce4-dd6e9201381a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(test_y)\n",
        "print(bert_predicted)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True False False  True  True False False  True  True  True False False\n",
            "  True False False  True False  True False  True False False  True False\n",
            " False  True  True  True False False False  True False False False False\n",
            "  True  True False False  True False  True  True False False False False\n",
            "  True  True False  True  True  True  True False False False  True  True\n",
            " False  True  True False  True False  True False False False  True False\n",
            "  True  True  True False False  True False  True False False  True False\n",
            "  True False False  True False  True False False False False  True False\n",
            " False False False False  True  True False  True  True False  True False\n",
            "  True  True  True False False False False  True  True False False False\n",
            "  True False False False  True False False False  True  True False  True\n",
            " False  True  True False  True False  True  True False  True  True False\n",
            " False False False  True False False  True False False False False  True\n",
            "  True False  True False  True  True False False  True False  True False\n",
            " False False False  True False  True False False False False False False\n",
            "  True False False  True False  True  True  True False  True False False\n",
            " False  True False  True False  True  True False False  True False  True\n",
            " False  True False  True  True  True False  True False False  True  True\n",
            " False False False  True  True  True False  True  True False False False\n",
            "  True  True False False  True False False  True  True False False False\n",
            "  True  True False False  True  True  True  True  True False  True  True\n",
            " False  True  True  True  True False  True False False False  True False\n",
            "  True False  True False  True False False  True False False False False\n",
            "  True  True False False  True  True False False False False False False\n",
            " False False False  True False False False  True False  True  True  True\n",
            " False False  True  True  True  True  True  True  True  True False  True\n",
            "  True False  True False  True  True  True False  True  True  True  True\n",
            " False False False False False False False False False  True  True False\n",
            "  True False  True  True  True False False  True False False False  True\n",
            "  True False False  True False False False False  True False False False\n",
            "  True  True  True False  True False  True False  True  True  True False\n",
            " False  True False False  True False False False False False  True  True\n",
            "  True  True  True False False False False  True  True  True False  True\n",
            "  True  True  True  True  True  True  True False  True False  True  True\n",
            "  True  True False  True  True  True  True  True False  True  True  True\n",
            " False  True False  True False False False  True  True False  True False\n",
            "  True False  True  True  True  True False False  True False  True  True\n",
            " False False False  True False False False  True  True  True  True False\n",
            " False  True  True False  True  True  True False  True  True  True False\n",
            " False False False False  True False  True  True  True False False  True\n",
            " False  True False False  True False False False False  True False  True\n",
            " False False  True False False  True  True  True False False False  True\n",
            "  True False False  True False False  True  True  True False False False\n",
            "  True False False False False False  True  True  True  True False False\n",
            " False False  True  True  True False False  True False  True False False\n",
            "  True False False False  True False  True False  True  True False  True\n",
            " False False  True False  True  True  True False  True False  True  True\n",
            " False  True  True  True False False  True  True False  True False False\n",
            "  True False  True False False False False False False False False False\n",
            " False False False  True False False False  True  True False False  True\n",
            " False False False  True  True  True  True  True False  True  True False\n",
            " False False False False False False False False  True False  True  True\n",
            " False False  True False  True False  True False  True False  True  True\n",
            "  True False False False  True  True False False  True  True  True False\n",
            " False False  True False False  True False  True  True False  True False\n",
            "  True  True False  True False False False False False False False  True\n",
            " False  True False False False False  True  True  True False  True  True\n",
            " False False  True False False False False False  True  True False  True\n",
            "  True  True False  True False False False False  True  True  True  True\n",
            " False False  True False False False  True  True  True  True False  True\n",
            "  True False  True  True False  True False False  True False  True False\n",
            "  True  True False False False  True  True False  True  True False  True\n",
            " False False  True  True  True False  True False False False  True False\n",
            " False False  True False False False  True False False False  True  True\n",
            " False  True  True  True False False False  True False False  True False\n",
            "  True  True False False False  True  True False False False  True False\n",
            "  True  True  True False False False  True False]\n",
            "[False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, True, False, False, False, False, False, False, True, False, True, False, False, True, True, False, False, True, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, True, False, False, True, True, True, True, False, True, True, False, False, False, False, True, True, False, False, True, False, True, False, False, False, False, False, True, False, True, True, False, False, False, False, False, True, False, False, False, False, True, True, True, False, False, True, False, False, False, True, False, True, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, True, False, False, False, False, True, False, False, True, False, True, False, False, False, False, False, True, False, False, False, True, True, False, True, False, False, True, True, True, False, False, False, False, False, False, False, False, True, False, False, True, False, False, True, False, False, True, False, False, False, False, False, True, False, False, False, True, False, False, False, False, True, False, True, False, False, True, False, False, False, False, False, False, True, False, True, False, False, False, False, True, True, False, True, False, False, False, True, False, False, False, True, True, True, False, False, True, False, True, False, False, True, False, False, False, False, True, True, False, False, False, True, True, True, False, True, True, True, False, True, True, True, False, True, False, True, True, True, True, False, False, False, True, True, False, False, True, False, False, False, False, True, True, False, False, False, True, False, False, False, True, False, False, True, False, True, True, False, False, False, False, False, False, False, True, True, False, False, False, False, True, True, False, False, True, True, False, False, True, False, True, False, True, True, False, False, True, False, True, True, False, True, False, False, False, True, False, False, False, False, False, False, True, False, True, True, False, False, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, True, False, True, True, True, True, False, False, False, False, False, False, False, False, False, False, True, True, True, False, False, True, True, False, False, True, False, True, True, True, False, True, False, True, True, True, True, False, False, False, True, False, True, True, False, True, True, True, False, False, True, True, True, False, True, False, False, True, False, False, False, True, True, False, False, False, True, False, False, True, False, False, False, False, False, True, False, False, True, True, False, False, False, False, False, True, False, True, True, True, False, True, False, False, True, True, False, False, True, False, False, False, False, True, True, False, False, True, True, False, True, False, False, False, False, False, True, False, True, False, False, False, True, True, True, False, True, False, False, False, True, True, False, False, True, False, True, False, False, False, True, True, False, False, False, False, True, False, False, False, False, False, False, False, True, True, False, False, False, False, False, False, True, False, False, False, False, False, True, False, True, False, False, False, False, False, False, False, True, False, False, False, False, True, False, True, False, False, False, False, False, False, True, False, True, False, False, False, False, True, True, True, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, True, True, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, True, True, True, True, False, False, True, True, False, False, False, True, True, False, False, False, True, False, False, False, False, True, True, True, False, False, False, True, False, True, True, True, False, True, False, True, True, True, False, True, False, False, False, True, True, False, False, False, True, True, False, False, True, False, False, False, False, True, True, False, True, False, False, False, True, False, False, False, True, False, False, True, True, True, False, False, True, False, True, True, True, False, False, False, True, True, True, False, True, True, False, False, False, False, False, False, True, False, False, False, True, False, True, True, False, False, False, False, False, False, False, True, True, True, False, False, False, True, True, True, False, True, False, True, False, False, True, True, False, False, False, False, False, False, False, True, False, True, True, True, False, True, True, False, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, True, True, True, False, False, True, False, False, True, True, False, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, True, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}